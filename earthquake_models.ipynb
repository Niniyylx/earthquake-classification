{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdc41a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=25.5.1\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge obspy -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94d4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n",
      "Current time is 2025-08-04 02:33:03.076428\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, world!\")\n",
    "from datetime import datetime\n",
    "print(\"Current time is\", datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460a7af",
   "metadata": {},
   "source": [
    "### Earthquake Catalog and Feature Extraction\n",
    "\n",
    "This script retrieves the earthquake catalog from IRIS FDSN for the period **February 25 ‚Äì March 25, 2011** in the Tohoku region (Mw ‚â• 3.0), and downloads corresponding waveforms from station **IU.MAJO.00.BHZ**.  \n",
    "For each event, short ([-30s, +60s]) and long windows (5‚Äì30 minutes depending on magnitude) are extracted. The waveforms are preprocessed by instrument response removal, detrending, demeaning, and multi-band filtering.  \n",
    "Energy features (sum of squared amplitudes) are then calculated across multiple frequency bands, together with event time, magnitude, and magnitude category.  \n",
    "The final dataset is saved as:  \n",
    "\n",
    "- `tohoku_earthquake_features_2011.csv`  \n",
    "- `tohoku_earthquake_features_2011.pkl`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a412d134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching earthquake catalog...\n",
      "2011-02-25: retrieved 8 events\n",
      "2011-02-26: retrieved 31 events\n",
      "2011-02-27: retrieved 13 events\n",
      "2011-02-28: retrieved 18 events\n",
      "2011-03-01: retrieved 10 events\n",
      "2011-03-02: retrieved 5 events\n",
      "2011-03-03: retrieved 8 events\n",
      "2011-03-04: retrieved 9 events\n",
      "2011-03-05: retrieved 10 events\n",
      "2011-03-06: retrieved 11 events\n",
      "2011-03-07: retrieved 12 events\n",
      "2011-03-08: retrieved 8 events\n",
      "2011-03-09: retrieved 138 events\n",
      "2011-03-10: retrieved 61 events\n",
      "2011-03-11: retrieved 1320 events\n",
      "2011-03-12: retrieved 1082 events\n",
      "2011-03-13: retrieved 803 events\n",
      "2011-03-14: retrieved 661 events\n",
      "2011-03-15: retrieved 613 events\n",
      "2011-03-16: retrieved 533 events\n",
      "2011-03-17: retrieved 522 events\n",
      "2011-03-18: retrieved 398 events\n",
      "2011-03-19: retrieved 394 events\n",
      "2011-03-20: retrieved 362 events\n",
      "2011-03-21: retrieved 329 events\n",
      "2011-03-22: retrieved 404 events\n",
      "2011-03-23: retrieved 282 events\n",
      "2011-03-24: retrieved 256 events\n",
      "\n",
      "Total events ‚â•Mw3.0: 8301\n",
      "\n",
      "Starting parallel feature extraction...\n",
      "\n",
      "Successfully extracted features for 8301 events\n",
      "\n",
      "Sample of extracted features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>magnitude_category</th>\n",
       "      <th>short_0.1-1.0Hz_energy</th>\n",
       "      <th>long_0.1-1.0Hz_energy</th>\n",
       "      <th>short_1.0-5.0Hz_energy</th>\n",
       "      <th>long_1.0-5.0Hz_energy</th>\n",
       "      <th>short_5.0-8.0Hz_energy</th>\n",
       "      <th>long_5.0-8.0Hz_energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-02-25T18:59:17.500000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Minor+Light</td>\n",
       "      <td>4.211656e-10</td>\n",
       "      <td>1.293103e-09</td>\n",
       "      <td>1.323571e-13</td>\n",
       "      <td>1.429869e-11</td>\n",
       "      <td>1.784218e-14</td>\n",
       "      <td>1.845573e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-02-25T06:44:47.500000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Minor+Light</td>\n",
       "      <td>1.270409e-10</td>\n",
       "      <td>4.205405e-10</td>\n",
       "      <td>4.182927e-12</td>\n",
       "      <td>6.218598e-12</td>\n",
       "      <td>4.735624e-13</td>\n",
       "      <td>7.924541e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-02-25T15:48:16.400000</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Minor+Light</td>\n",
       "      <td>3.568408e-10</td>\n",
       "      <td>1.265034e-09</td>\n",
       "      <td>6.429292e-13</td>\n",
       "      <td>1.996687e-12</td>\n",
       "      <td>5.364888e-14</td>\n",
       "      <td>1.680307e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-25T17:00:17.550000</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Minor+Light</td>\n",
       "      <td>3.079306e-10</td>\n",
       "      <td>1.316121e-09</td>\n",
       "      <td>1.174070e-13</td>\n",
       "      <td>6.546385e-13</td>\n",
       "      <td>1.144721e-14</td>\n",
       "      <td>4.531881e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-25T05:55:54.600000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Minor+Light</td>\n",
       "      <td>7.106407e-11</td>\n",
       "      <td>3.524731e-10</td>\n",
       "      <td>8.031155e-13</td>\n",
       "      <td>2.933601e-12</td>\n",
       "      <td>5.826152e-14</td>\n",
       "      <td>1.220384e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   event_time  magnitude magnitude_category  \\\n",
       "0  2011-02-25T18:59:17.500000        3.1        Minor+Light   \n",
       "1  2011-02-25T06:44:47.500000        3.1        Minor+Light   \n",
       "2  2011-02-25T15:48:16.400000        3.3        Minor+Light   \n",
       "3  2011-02-25T17:00:17.550000        3.7        Minor+Light   \n",
       "4  2011-02-25T05:55:54.600000        3.1        Minor+Light   \n",
       "\n",
       "   short_0.1-1.0Hz_energy  long_0.1-1.0Hz_energy  short_1.0-5.0Hz_energy  \\\n",
       "0            4.211656e-10           1.293103e-09            1.323571e-13   \n",
       "1            1.270409e-10           4.205405e-10            4.182927e-12   \n",
       "2            3.568408e-10           1.265034e-09            6.429292e-13   \n",
       "3            3.079306e-10           1.316121e-09            1.174070e-13   \n",
       "4            7.106407e-11           3.524731e-10            8.031155e-13   \n",
       "\n",
       "   long_1.0-5.0Hz_energy  short_5.0-8.0Hz_energy  long_5.0-8.0Hz_energy  \n",
       "0           1.429869e-11            1.784218e-14           1.845573e-12  \n",
       "1           6.218598e-12            4.735624e-13           7.924541e-13  \n",
       "2           1.996687e-12            5.364888e-14           1.680307e-13  \n",
       "3           6.546385e-13            1.144721e-14           4.531881e-14  \n",
       "4           2.933601e-12            5.826152e-14           1.220384e-13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Magnitude category distribution:\n",
      "Minor+Light        7685\n",
      "Moderate+Strong     612\n",
      "Major+Great           4\n",
      "Name: magnitude_category, dtype: int64\n",
      "\n",
      "Saving dataset to: /Users/donghui\n",
      "‚úÖ Dataset saved successfully!\n",
      "üìä Total records: 8301\n",
      "üìÅ Files created:\n",
      "   - tohoku_earthquake_features_2011.csv\n",
      "   - tohoku_earthquake_features_2011.pkl\n",
      "\n",
      "üìÇ Files in directory /Users/donghui:\n",
      "   - tohoku_earthquake_features_2011.csv\n",
      "   - earthquake_features_2011.csv\n",
      "   - earthquake_features_2011.pkl\n",
      "   - tohoku_earthquake_features_2011.pkl\n",
      "\n",
      "üîç Verifying saved files...\n",
      "‚úÖ CSV file verified: 8301 rows loaded\n",
      "üìã Columns: ['event_time', 'magnitude', 'magnitude_category', 'short_0.1-1.0Hz_energy', 'long_0.1-1.0Hz_energy', 'short_1.0-5.0Hz_energy', 'long_1.0-5.0Hz_energy', 'short_5.0-8.0Hz_energy', 'long_5.0-8.0Hz_energy']\n",
      "\n",
      "üìà Dataset Statistics:\n",
      "   - Time range: 2011-02-25T05:55:54.600000 to 2011-03-24T23:56:51.260000\n",
      "   - Magnitude range: 3.0 to 9.1\n",
      "   - Feature columns: 9\n",
      "\n",
      "üéâ Earthquake feature extraction and dataset creation completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.clients.fdsn.client import FDSNNoDataException\n",
    "from obspy.core import UTCDateTime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from http.client import IncompleteRead, HTTPException\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 1. Configuration ‚Äî‚Äî‚Äî\n",
    "client        = Client(\"IRIS\", timeout=120)\n",
    "network       = \"IU\"\n",
    "station       = \"MAJO\"\n",
    "location      = \"00\"\n",
    "channel       = \"BHZ\"\n",
    "\n",
    "# Time range: 15 days before and after the Tohoku earthquake\n",
    "start_date    = UTCDateTime(\"2011-02-25\")\n",
    "end_date      = UTCDateTime(\"2011-03-25\")\n",
    "\n",
    "# Only fetch events with Mw ‚â• 3.0\n",
    "min_magnitude = 3.0\n",
    "max_magnitude = 9.5\n",
    "\n",
    "# Time windows and frequency bands\n",
    "pre_window_short  = 30   # 30 seconds before event for short window\n",
    "post_window_short = 60   # 60 seconds after event for short window\n",
    "frequency_bands   = [(0.1, 1.0), (1.0, 5.0), (5.0, 8.0)]\n",
    "\n",
    "def choose_post_window_duration(magnitude):\n",
    "    \"\"\"\n",
    "    Choose the duration of post-event long window based on magnitude\n",
    "    \n",
    "    Args:\n",
    "        magnitude (float): Event magnitude\n",
    "        \n",
    "    Returns:\n",
    "        int: Duration in seconds\n",
    "    \"\"\"\n",
    "    if magnitude < 4.0:   \n",
    "        return 300   # 5 minutes for small events\n",
    "    elif magnitude < 6.0:   \n",
    "        return 600   # 10 minutes for moderate events\n",
    "    else:\n",
    "        return 1800  # 30 minutes for large events\n",
    "\n",
    "def safe_event_query(**kwargs):\n",
    "    \"\"\"\n",
    "    Robust event catalog query with retry mechanism\n",
    "    \n",
    "    Args:\n",
    "        **kwargs: Parameters for get_events()\n",
    "        \n",
    "    Returns:\n",
    "        obspy.Catalog: Event catalog or empty list if failed\n",
    "    \"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return client.get_events(**kwargs)\n",
    "        except (IncompleteRead, HTTPException):\n",
    "            continue\n",
    "    return []\n",
    "\n",
    "def fetch_earthquake_catalog(start_time, end_time):\n",
    "    \"\"\"\n",
    "    Fetch earthquake catalog day by day from ISC, Mw ‚â• 3.0\n",
    "    \n",
    "    Args:\n",
    "        start_time (UTCDateTime): Start time\n",
    "        end_time (UTCDateTime): End time\n",
    "        \n",
    "    Returns:\n",
    "        list: List of unique earthquake events\n",
    "    \"\"\"\n",
    "    events_list = []\n",
    "    current_time = start_time.datetime\n",
    "    end_datetime = end_time.datetime\n",
    "    \n",
    "    while current_time < end_datetime:\n",
    "        day_start = UTCDateTime(current_time)\n",
    "        day_end = UTCDateTime(current_time + relativedelta(days=1))\n",
    "        \n",
    "        # Query events for one day\n",
    "        catalog = safe_event_query(\n",
    "            starttime=day_start, \n",
    "            endtime=day_end,\n",
    "            minmagnitude=min_magnitude, \n",
    "            maxmagnitude=max_magnitude,\n",
    "            catalog=\"ISC\",\n",
    "            # Regional search around Japan (¬±10 degrees from Tohoku epicenter)\n",
    "            minlatitude=37.2247-10, \n",
    "            maxlatitude=37.2247+10,\n",
    "            minlongitude=140.8777-10, \n",
    "            maxlongitude=140.8777+10\n",
    "        )\n",
    "        \n",
    "        if catalog:\n",
    "            events_list.extend(catalog)\n",
    "            print(f\"{day_start.date}: retrieved {len(catalog)} events\")\n",
    "            \n",
    "        current_time += relativedelta(days=1)\n",
    "    \n",
    "    # Remove duplicates based on resource ID\n",
    "    unique_events = list({event.resource_id.id: event for event in events_list}.values())\n",
    "    return unique_events\n",
    "\n",
    "def assign_magnitude_category(magnitude):\n",
    "    \"\"\"\n",
    "    Assign magnitude category based on earthquake magnitude\n",
    "    \n",
    "    Args:\n",
    "        magnitude (float): Earthquake magnitude\n",
    "        \n",
    "    Returns:\n",
    "        str: Magnitude category\n",
    "    \"\"\"\n",
    "    if 3.0 <= magnitude < 5.0:\n",
    "        return \"Minor+Light\"      # M3.0-4.9\n",
    "    elif 5.0 <= magnitude < 7.0:\n",
    "        return \"Moderate+Strong\"  # M5.0-6.9\n",
    "    else:\n",
    "        return \"Major+Great\"      # M7.0+\n",
    "\n",
    "def extract_event_features(earthquake_event):\n",
    "    \"\"\"\n",
    "    Download waveform and extract features for a single earthquake event\n",
    "    \n",
    "    Args:\n",
    "        earthquake_event: ObsPy Event object\n",
    "        \n",
    "    Returns:\n",
    "        dict: Feature dictionary or None if processing failed\n",
    "    \"\"\"\n",
    "    origin_time = earthquake_event.origins[0].time\n",
    "    magnitude = earthquake_event.magnitudes[0].mag\n",
    "    post_window_long = choose_post_window_duration(magnitude)\n",
    "    \n",
    "    try:\n",
    "        # Download waveform data\n",
    "        waveform_stream = client.get_waveforms(\n",
    "            network, station, location, channel,\n",
    "            origin_time - pre_window_short, \n",
    "            origin_time + post_window_long,\n",
    "            attach_response=True,\n",
    "            longestonly=True\n",
    "        )\n",
    "        \n",
    "        if len(waveform_stream) == 0:\n",
    "            return None\n",
    "            \n",
    "        # Remove instrument response and preprocess\n",
    "        waveform_stream.remove_response(output=\"VEL\")\n",
    "        waveform_stream.detrend(\"linear\")\n",
    "        waveform_stream.detrend(\"demean\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    # Extract time windows\n",
    "    short_window_stream = waveform_stream.slice(\n",
    "        origin_time - pre_window_short, \n",
    "        origin_time + post_window_short\n",
    "    )\n",
    "    long_window_stream = waveform_stream.slice(\n",
    "        origin_time, \n",
    "        origin_time + post_window_long\n",
    "    )\n",
    "    \n",
    "    # Initialize feature dictionary\n",
    "    feature_dict = {\n",
    "        \"event_time\": origin_time.isoformat(),\n",
    "        \"magnitude\": magnitude,\n",
    "        \"magnitude_category\": assign_magnitude_category(magnitude)\n",
    "    }\n",
    "    \n",
    "    # Extract energy features for different frequency bands and time windows\n",
    "    for freq_min, freq_max in frequency_bands:\n",
    "        for window_name, stream_segment in [(\"short\", short_window_stream), \n",
    "                                          (\"long\", long_window_stream)]:\n",
    "            # Copy trace for filtering\n",
    "            trace_copy = stream_segment[0].copy()\n",
    "            \n",
    "            # Apply bandpass filter\n",
    "            trace_copy.filter(\"bandpass\",\n",
    "                            freqmin=freq_min, \n",
    "                            freqmax=freq_max,\n",
    "                            corners=4, \n",
    "                            zerophase=True)\n",
    "            \n",
    "            # Calculate energy (sum of squared amplitudes)\n",
    "            energy_feature_name = f\"{window_name}_{freq_min:.1f}-{freq_max:.1f}Hz_energy\"\n",
    "            feature_dict[energy_feature_name] = np.sum(trace_copy.data**2)\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 2. Fetch earthquake catalog ‚Äî‚Äî‚Äî\n",
    "print(\"Fetching earthquake catalog...\")\n",
    "earthquake_catalog = fetch_earthquake_catalog(start_date, end_date)\n",
    "print(f\"\\nTotal events ‚â•Mw{min_magnitude}: {len(earthquake_catalog)}\\n\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 3. Parallel waveform download and feature extraction ‚Äî‚Äî‚Äî\n",
    "print(\"Starting parallel feature extraction...\")\n",
    "extracted_features = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    # Submit all tasks\n",
    "    future_dict = {executor.submit(extract_event_features, event): event \n",
    "                   for event in earthquake_catalog}\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(future_dict):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            extracted_features.append(result)\n",
    "\n",
    "print(f\"\\nSuccessfully extracted features for {len(extracted_features)} events\\n\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 4. Create DataFrame and display results ‚Äî‚Äî‚Äî\n",
    "earthquake_dataframe = pd.DataFrame(extracted_features)\n",
    "\n",
    "# Ensure magnitude category column exists\n",
    "if 'magnitude_category' not in earthquake_dataframe.columns:\n",
    "    earthquake_dataframe['magnitude_category'] = earthquake_dataframe['magnitude'].apply(assign_magnitude_category)\n",
    "\n",
    "# Display results\n",
    "from IPython.display import display\n",
    "print(\"Sample of extracted features:\")\n",
    "display(earthquake_dataframe.head())\n",
    "\n",
    "print(\"\\nMagnitude category distribution:\")\n",
    "print(earthquake_dataframe['magnitude_category'].value_counts())\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 5. Save dataset to files ‚Äî‚Äî‚Äî\n",
    "import os\n",
    "\n",
    "# Display current working directory\n",
    "print(f\"\\nSaving dataset to: {os.getcwd()}\")\n",
    "\n",
    "# Save as CSV file (universal format)\n",
    "csv_filename = 'tohoku_earthquake_features_2011.csv'\n",
    "earthquake_dataframe.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "\n",
    "# Save as pickle file (preserves data types)\n",
    "pickle_filename = 'tohoku_earthquake_features_2011.pkl'\n",
    "earthquake_dataframe.to_pickle(pickle_filename)\n",
    "\n",
    "print(f\"‚úÖ Dataset saved successfully!\")\n",
    "print(f\"üìä Total records: {len(earthquake_dataframe)}\")\n",
    "print(f\"üìÅ Files created:\")\n",
    "print(f\"   - {csv_filename}\")\n",
    "print(f\"   - {pickle_filename}\")\n",
    "\n",
    "# Display files in current directory\n",
    "print(f\"\\nüìÇ Files in directory {os.getcwd()}:\")\n",
    "data_files = [f for f in os.listdir('.') if f.endswith(('.csv', '.pkl', '.xlsx'))]\n",
    "for filename in data_files:\n",
    "    print(f\"   - {filename}\")\n",
    "\n",
    "# ‚Äî‚Äî‚Äî 6. Verify file integrity ‚Äî‚Äî‚Äî\n",
    "print(\"\\nüîç Verifying saved files...\")\n",
    "try:\n",
    "    # Test CSV file loading\n",
    "    loaded_dataframe = pd.read_csv(csv_filename)\n",
    "    print(f\"‚úÖ CSV file verified: {len(loaded_dataframe)} rows loaded\")\n",
    "    print(f\"üìã Columns: {list(loaded_dataframe.columns)}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nüìà Dataset Statistics:\")\n",
    "    print(f\"   - Time range: {loaded_dataframe['event_time'].min()} to {loaded_dataframe['event_time'].max()}\")\n",
    "    print(f\"   - Magnitude range: {loaded_dataframe['magnitude'].min():.1f} to {loaded_dataframe['magnitude'].max():.1f}\")\n",
    "    print(f\"   - Feature columns: {len(loaded_dataframe.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå File verification failed: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Earthquake feature extraction and dataset creation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f4ae8",
   "metadata": {},
   "source": [
    "### Continuous Waveform Download\n",
    "\n",
    "This script downloads **continuous seismic waveform data** from IRIS FDSN for station **IU.MAJO.00.BHZ**, covering the period **March 1 ‚Äì March 21, 2011**.  \n",
    "The waveforms are retrieved on a daily basis and saved in MiniSEED format within the directory `waveforms/`.  \n",
    "Each file is named according to the station and date, e.g.,  \n",
    "\n",
    "- `waveforms/MAJO_2011-03-01.mseed`  \n",
    "- `waveforms/MAJO_2011-03-02.mseed`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbb4ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2011-03-01 ...\n",
      "Downloading 2011-03-02 ...\n",
      "Downloading 2011-03-03 ...\n",
      "Downloading 2011-03-04 ...\n",
      "Downloading 2011-03-05 ...\n",
      "Downloading 2011-03-06 ...\n",
      "Downloading 2011-03-07 ...\n",
      "Downloading 2011-03-08 ...\n",
      "Downloading 2011-03-09 ...\n",
      "Downloading 2011-03-10 ...\n",
      "Downloading 2011-03-11 ...\n",
      "Downloading 2011-03-12 ...\n",
      "Downloading 2011-03-13 ...\n",
      "Downloading 2011-03-14 ...\n",
      "Downloading 2011-03-15 ...\n",
      "Downloading 2011-03-16 ...\n",
      "Downloading 2011-03-17 ...\n",
      "Downloading 2011-03-18 ...\n",
      "Downloading 2011-03-19 ...\n",
      "Downloading 2011-03-20 ...\n"
     ]
    }
   ],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import os\n",
    "\n",
    "client = Client(\"IRIS\")\n",
    "network = \"IU\"\n",
    "station = \"MAJO\"\n",
    "location = \"00\"\n",
    "channel = \"BHZ\"\n",
    "\n",
    "output_dir = \"waveforms\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Êó∂Èó¥ÊÆµËÆæÂÆö\n",
    "start_date = UTCDateTime(\"2011-03-01\")\n",
    "end_date = UTCDateTime(\"2011-03-21\")\n",
    "\n",
    "# ÊØèÂ§©‰∏ãËΩΩ\n",
    "current_date = start_date\n",
    "while current_date < end_date:\n",
    "    try:\n",
    "        print(f\"Downloading {current_date.date} ...\")\n",
    "        st = client.get_waveforms(network, station, location, channel,\n",
    "                                  current_date, current_date + 86400)\n",
    "        st.write(os.path.join(output_dir, f\"{station}_{current_date.date}.mseed\"), format=\"MSEED\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {current_date.date} ‚Üí {e}\")\n",
    "    current_date += 86400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90224244",
   "metadata": {},
   "source": [
    "### Dataset Construction\n",
    "\n",
    "This script integrates the earthquake catalog with the continuous waveform data from station **IU.MAJO.00.BHZ** to construct the final training dataset.  \n",
    "\n",
    "- **Positive samples (earthquake windows):**  \n",
    "  For each catalogued event, a 90-second window was extracted (20s before and 70s after the origin time). Each sample is labeled as *small* (Mw < 5.0), *moderate* (5.0 ‚â§ Mw < 6.0), or *large* (Mw ‚â• 6.0).  \n",
    "\n",
    "- **Negative samples (noise windows):**  \n",
    "  Sliding windows of 90s were extracted from non-event periods (step = 45s), excluding a ¬±120s margin around known events to avoid contamination. These were labeled as *noise*.  \n",
    "\n",
    "- **Preprocessing:**  \n",
    "  Waveforms were detrended, bandpass-filtered (0.1‚Äì8 Hz), and standardized.  \n",
    "\n",
    "- **Features:**  \n",
    "  For each window, frequency band energy features (0.5‚Äì2 Hz, 2‚Äì5 Hz, 5‚Äì8 Hz) were computed, along with metadata such as sample ID, time window, and magnitude category.  \n",
    "\n",
    "- **Output files:**  \n",
    "  - `seismic_dataset.npz` ‚Üí contains waveform arrays, class labels, and sample IDs.  \n",
    "  - `seismic_features.csv` ‚Üí contains extracted features and metadata for all samples.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d556d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing MAJO_2011-03-01.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-02.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-03.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-04.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-05.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-06.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-07.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-08.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-09.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-10.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-11.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-12.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-13.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-14.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-15.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-16.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-17.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-18.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-19.mseed: 'datetime.date' object is not callable\n",
      "Error processing MAJO_2011-03-20.mseed: 'datetime.date' object is not callable\n",
      "‚úÖ Dataset saved: seismic_dataset.npz and seismic_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy import UTCDateTime, read\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATIONS ---\n",
    "sampling_rate = 20\n",
    "window_duration_sec = 90\n",
    "window_length = sampling_rate * window_duration_sec\n",
    "pre_event_sec = 20\n",
    "post_event_sec = 70\n",
    "exclude_margin_sec = 120\n",
    "sliding_step_sec = 45\n",
    "\n",
    "# --- PATHS ---\n",
    "event_file = \"tohoku_earthquake_features_2011.csv\"\n",
    "waveform_dir = \"waveforms\"\n",
    "output_npz_path = \"seismic_dataset.npz\"\n",
    "output_csv_path = \"seismic_features.csv\"\n",
    "\n",
    "# --- LOAD EVENT DATA ---\n",
    "events_df = pd.read_csv(event_file)\n",
    "events_df[\"event_time\"] = pd.to_datetime(events_df[\"event_time\"])\n",
    "event_times = [UTCDateTime(t) for t in events_df[\"event_time\"]]\n",
    "\n",
    "# --- HELPERS ---\n",
    "def classify_magnitude(mag):\n",
    "    if mag < 5.0:\n",
    "        return \"small\"\n",
    "    elif 5.0 <= mag < 6.0:\n",
    "        return \"moderate\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def extract_band_energy_features(signal, fs=20):\n",
    "    from scipy.signal import butter, sosfilt\n",
    "    def band_energy(x, fmin, fmax):\n",
    "        sos = butter(4, [fmin, fmax], btype='band', fs=fs, output='sos')\n",
    "        filtered = sosfilt(sos, x)\n",
    "        return np.sum(filtered ** 2)\n",
    "    return {\n",
    "        \"energy_0.5_2Hz\": band_energy(signal, 0.5, 2.0),\n",
    "        \"energy_2_5Hz\": band_energy(signal, 2.0, 5.0),\n",
    "        \"energy_5_8Hz\": band_energy(signal, 5.0, 8.0)\n",
    "    }\n",
    "\n",
    "def overlaps_with_events(event_times, start, end, margin):\n",
    "    for event_time in event_times:\n",
    "        if (start - margin) <= event_time <= (end + margin):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# --- PROCESSING ---\n",
    "waveform_samples = []\n",
    "class_labels = []\n",
    "binary_labels = []\n",
    "sample_ids = []\n",
    "metadata_records = []\n",
    "\n",
    "mseed_files = sorted(glob(os.path.join(waveform_dir, \"*.mseed\")))\n",
    "pos_count, neg_count = 0, 0\n",
    "\n",
    "for file_path in mseed_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        st = read(file_path)\n",
    "        st.detrend(\"demean\")\n",
    "        st.filter(\"bandpass\", freqmin=0.1, freqmax=8.0)\n",
    "        tr = st[0]\n",
    "\n",
    "        t_start = tr.stats.starttime\n",
    "        t_end = tr.stats.endtime\n",
    "\n",
    "        # --- POSITIVE SAMPLES ---\n",
    "        day_events = events_df[events_df[\"event_time\"].dt.date == t_start.date()]\n",
    "        for _, row in day_events.iterrows():\n",
    "            t0 = UTCDateTime(row[\"event_time\"]) - pre_event_sec\n",
    "            t1 = t0 + window_duration_sec\n",
    "            if t0 < t_start or t1 > t_end:\n",
    "                continue\n",
    "            data = tr.slice(starttime=t0, endtime=t1).data[:window_length]\n",
    "            if len(data) == window_length:\n",
    "                mag_category = classify_magnitude(row[\"magnitude\"])\n",
    "                waveform_samples.append(data)\n",
    "                class_labels.append({\"small\": 1, \"moderate\": 2, \"large\": 3}[mag_category])\n",
    "                binary_labels.append(1)\n",
    "                sid = f\"pos_{pos_count:05d}\"\n",
    "                sample_ids.append(sid)\n",
    "                feats = extract_band_energy_features(data)\n",
    "                metadata_records.append({\n",
    "                    \"sample_id\": sid,\n",
    "                    \"is_earthquake\": 1,\n",
    "                    \"label\": {\"small\": 1, \"moderate\": 2, \"large\": 3}[mag_category],\n",
    "                    \"magnitude\": row[\"magnitude\"],\n",
    "                    \"magnitude_category\": mag_category,\n",
    "                    \"window_start\": str(t0),\n",
    "                    \"window_end\": str(t1),\n",
    "                    **feats\n",
    "                })\n",
    "                pos_count += 1\n",
    "\n",
    "        # --- NEGATIVE SAMPLES ---\n",
    "        win_start = t_start\n",
    "        while win_start + window_duration_sec <= t_end:\n",
    "            win_end = win_start + window_duration_sec\n",
    "            if not overlaps_with_events(event_times, win_start, win_end, exclude_margin_sec):\n",
    "                data = tr.slice(starttime=win_start, endtime=win_end).data[:window_length]\n",
    "                if len(data) == window_length:\n",
    "                    waveform_samples.append(data)\n",
    "                    class_labels.append(0)\n",
    "                    binary_labels.append(0)\n",
    "                    sid = f\"neg_{neg_count:05d}\"\n",
    "                    sample_ids.append(sid)\n",
    "                    feats = extract_band_energy_features(data)\n",
    "                    metadata_records.append({\n",
    "                        \"sample_id\": sid,\n",
    "                        \"is_earthquake\": 0,\n",
    "                        \"label\": 0,\n",
    "                        \"magnitude\": np.nan,\n",
    "                        \"magnitude_category\": \"noise\",\n",
    "                        \"window_start\": str(win_start),\n",
    "                        \"window_end\": str(win_end),\n",
    "                        **feats\n",
    "                    })\n",
    "                    neg_count += 1\n",
    "            win_start += sliding_step_sec\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- SAVE OUTPUT FILES ---\n",
    "np.savez_compressed(output_npz_path,\n",
    "                    waveforms=np.array(waveform_samples),\n",
    "                    labels=np.array(class_labels),\n",
    "                    sample_types=np.array(binary_labels),\n",
    "                    sample_ids=np.array(sample_ids))\n",
    "\n",
    "csv_df = pd.DataFrame(metadata_records)\n",
    "csv_df.to_csv(output_csv_path, index=False)\n",
    "print(\"‚úÖ Dataset saved: seismic_dataset.npz and seismic_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03819fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset saved: seismic_dataset.npz and seismic_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy import UTCDateTime, read\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATIONS ---\n",
    "sampling_rate = 20\n",
    "window_duration_sec = 90\n",
    "window_length = sampling_rate * window_duration_sec\n",
    "pre_event_sec = 20\n",
    "post_event_sec = 70\n",
    "exclude_margin_sec = 120\n",
    "sliding_step_sec = 45\n",
    "\n",
    "# --- PATHS ---\n",
    "event_file = \"tohoku_earthquake_features_2011.csv\"\n",
    "waveform_dir = \"waveforms\"\n",
    "output_npz_path = \"seismic_dataset.npz\"\n",
    "output_csv_path = \"seismic_features.csv\"\n",
    "\n",
    "# --- LOAD EVENT DATA ---\n",
    "events_df = pd.read_csv(event_file)\n",
    "events_df[\"event_time\"] = pd.to_datetime(events_df[\"event_time\"])\n",
    "event_times = [UTCDateTime(t) for t in events_df[\"event_time\"]]\n",
    "\n",
    "# --- HELPERS ---\n",
    "def classify_magnitude(mag):\n",
    "    if mag < 5.0:\n",
    "        return \"small\"\n",
    "    elif 5.0 <= mag < 6.0:\n",
    "        return \"moderate\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def extract_band_energy_features(signal, fs=20):\n",
    "    from scipy.signal import butter, sosfilt\n",
    "    def band_energy(x, fmin, fmax):\n",
    "        sos = butter(4, [fmin, fmax], btype='band', fs=fs, output='sos')\n",
    "        filtered = sosfilt(sos, x)\n",
    "        return np.sum(filtered ** 2)\n",
    "    return {\n",
    "        \"energy_0.5_2Hz\": band_energy(signal, 0.5, 2.0),\n",
    "        \"energy_2_5Hz\": band_energy(signal, 2.0, 5.0),\n",
    "        \"energy_5_8Hz\": band_energy(signal, 5.0, 8.0)\n",
    "    }\n",
    "\n",
    "def overlaps_with_events(event_times, start, end, margin):\n",
    "    for event_time in event_times:\n",
    "        if (start - margin) <= event_time <= (end + margin):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# --- PROCESSING ---\n",
    "waveform_samples = []\n",
    "class_labels = []\n",
    "binary_labels = []\n",
    "sample_ids = []\n",
    "metadata_records = []\n",
    "\n",
    "mseed_files = sorted(glob(os.path.join(waveform_dir, \"*.mseed\")))\n",
    "pos_count, neg_count = 0, 0\n",
    "\n",
    "for file_path in mseed_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        st = read(file_path)\n",
    "        st.detrend(\"demean\")\n",
    "        st.filter(\"bandpass\", freqmin=0.1, freqmax=8.0)\n",
    "        tr = st[0]\n",
    "\n",
    "        t_start = tr.stats.starttime\n",
    "        t_end = tr.stats.endtime\n",
    "\n",
    "        # --- POSITIVE SAMPLES ---\n",
    "        day_events = events_df[events_df[\"event_time\"].dt.date == t_start.date]\n",
    "        for _, row in day_events.iterrows():\n",
    "            t0 = UTCDateTime(row[\"event_time\"]) - pre_event_sec\n",
    "            t1 = t0 + window_duration_sec\n",
    "            if t0 < t_start or t1 > t_end:\n",
    "                continue\n",
    "            data = tr.slice(starttime=t0, endtime=t1).data[:window_length]\n",
    "            if len(data) == window_length:\n",
    "                mag_category = classify_magnitude(row[\"magnitude\"])\n",
    "                waveform_samples.append(data)\n",
    "                class_labels.append({\"small\": 1, \"moderate\": 2, \"large\": 3}[mag_category])\n",
    "                binary_labels.append(1)\n",
    "                sid = f\"pos_{pos_count:05d}\"\n",
    "                sample_ids.append(sid)\n",
    "                feats = extract_band_energy_features(data)\n",
    "                metadata_records.append({\n",
    "                    \"sample_id\": sid,\n",
    "                    \"is_earthquake\": 1,\n",
    "                    \"label\": {\"small\": 1, \"moderate\": 2, \"large\": 3}[mag_category],\n",
    "                    \"magnitude\": row[\"magnitude\"],\n",
    "                    \"magnitude_category\": mag_category,\n",
    "                    \"window_start\": str(t0),\n",
    "                    \"window_end\": str(t1),\n",
    "                    **feats\n",
    "                })\n",
    "                pos_count += 1\n",
    "\n",
    "        # --- NEGATIVE SAMPLES ---\n",
    "        win_start = t_start\n",
    "        while win_start + window_duration_sec <= t_end:\n",
    "            win_end = win_start + window_duration_sec\n",
    "            if not overlaps_with_events(event_times, win_start, win_end, exclude_margin_sec):\n",
    "                data = tr.slice(starttime=win_start, endtime=win_end).data[:window_length]\n",
    "                if len(data) == window_length:\n",
    "                    waveform_samples.append(data)\n",
    "                    class_labels.append(0)\n",
    "                    binary_labels.append(0)\n",
    "                    sid = f\"neg_{neg_count:05d}\"\n",
    "                    sample_ids.append(sid)\n",
    "                    feats = extract_band_energy_features(data)\n",
    "                    metadata_records.append({\n",
    "                        \"sample_id\": sid,\n",
    "                        \"is_earthquake\": 0,\n",
    "                        \"label\": 0,\n",
    "                        \"magnitude\": np.nan,\n",
    "                        \"magnitude_category\": \"noise\",\n",
    "                        \"window_start\": str(win_start),\n",
    "                        \"window_end\": str(win_end),\n",
    "                        **feats\n",
    "                    })\n",
    "                    neg_count += 1\n",
    "            win_start += sliding_step_sec\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- SAVE OUTPUT FILES ---\n",
    "np.savez_compressed(output_npz_path,\n",
    "                    waveforms=np.array(waveform_samples),\n",
    "                    labels=np.array(class_labels),\n",
    "                    sample_types=np.array(binary_labels),\n",
    "                    sample_ids=np.array(sample_ids))\n",
    "\n",
    "csv_df = pd.DataFrame(metadata_records)\n",
    "csv_df.to_csv(output_csv_path, index=False)\n",
    "print(\"‚úÖ Dataset saved: seismic_dataset.npz and seismic_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fddd3a9",
   "metadata": {},
   "source": [
    "### Appendix Fix-U0: Unifying Dataset for Models\n",
    "\n",
    "This utility script converts your generated outputs into a single, standardized NPZ file that both the CNN model and the XGBoost baseline can consume.\n",
    "\n",
    "**Inputs**\n",
    "- `seismic_dataset.npz` ‚Äî arrays: `waveforms`, `labels` (pos: 1/2/3; neg: 0), `sample_types` (binary 0/1), `sample_ids`\n",
    "- `seismic_features.csv` ‚Äî metadata table with `sample_id`, `window_start`, `window_end`, ‚Ä¶\n",
    "\n",
    "**Processing**\n",
    "- Ensures files exist and loads arrays.\n",
    "- Derives **detection label** `detect_label` from `sample_types` (fallback: `labels > 0`).\n",
    "- Builds **magnitude class (int)** `mag_class` as `0/1/2` for S/M/L and `-1` for noise.\n",
    "- Builds **magnitude class (str)** `mag_cls` as `\"S\"|\"M\"|\"L\"|\"noise\"`.\n",
    "- Aligns `window_start`/`window_end` from the CSV to the NPZ order via `sample_id` (raises error if mismatched).\n",
    "- Stores sampling metadata: `fs=20`, `win_sec=90`.\n",
    "\n",
    "**Output**\n",
    "- `data/wave_mag_dataset.npz` containing:\n",
    "  - `waveforms`, `detect_label`, `mag_class`, `mag_cls`, `sample_ids`\n",
    "  - `window_start`, `window_end`, `fs`, `win_sec`\n",
    "\n",
    "This creates a **unified contract** for downstream training and evaluation while preserving sample ordering and window metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1861c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote data/wave_mag_dataset.npz\n",
      "waveforms shape: (26540, 1800) fs: 20 win_sec: 90\n"
     ]
    }
   ],
   "source": [
    "# Appendix Fix-U0 ‚Äî Build data/wave_mag_dataset.npz from your existing outputs\n",
    "# Inputs required (from your generator script):\n",
    "#   - seismic_dataset.npz    (waveforms, labels, sample_types, sample_ids)\n",
    "#   - seismic_features.csv   (sample_id, window_start, window_end, magnitude_category, ...)\n",
    "# Output:\n",
    "#   - data/wave_mag_dataset.npz (unified contract for CNN & baseline)\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "SRC_NPZ = \"seismic_dataset.npz\"\n",
    "SRC_CSV = \"seismic_features.csv\"\n",
    "DST_NPZ = \"data/wave_mag_dataset.npz\"\n",
    "\n",
    "assert os.path.exists(SRC_NPZ), f\"Missing {SRC_NPZ}\"\n",
    "assert os.path.exists(SRC_CSV), f\"Missing {SRC_CSV}\"\n",
    "\n",
    "d = np.load(SRC_NPZ, allow_pickle=True)\n",
    "waves = d[\"waveforms\"].astype(np.float32)\n",
    "sids  = d[\"sample_ids\"].astype(str)\n",
    "\n",
    "# detect_label: prefer your binary channel 'sample_types' (0/1). Fallback to labels>0.\n",
    "if \"sample_types\" in d.files:\n",
    "    detect = d[\"sample_types\"].astype(np.int8)\n",
    "else:\n",
    "    detect = (d[\"labels\"].astype(np.int32) > 0).astype(np.int8)\n",
    "\n",
    "# mag_class (int): 0/1/2 for S/M/L, and -1 for noise (negatives)\n",
    "labs = d[\"labels\"].astype(np.int32)  # your positives: 1/2/3 ; negatives: 0\n",
    "mag_class = np.where(detect==1, labs-1, -1).astype(np.int8)\n",
    "\n",
    "# mag_cls (str): \"S\"/\"M\"/\"L\"/\"noise\"\n",
    "mag_cls = np.array([\"noise\"] * len(mag_class), dtype=object)\n",
    "map_int2str = {0:\"S\", 1:\"M\", 2:\"L\"}\n",
    "pos_idx = np.where(mag_class >= 0)[0]\n",
    "for i in pos_idx:\n",
    "    mag_cls[i] = map_int2str[int(mag_class[i])]\n",
    "\n",
    "# attach window_start / window_end from your CSV metadata\n",
    "meta = pd.read_csv(SRC_CSV)\n",
    "need_cols = {\"sample_id\", \"window_start\", \"window_end\"}\n",
    "missing = need_cols - set(meta.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"{SRC_CSV} missing columns: {missing}\")\n",
    "\n",
    "meta = meta.set_index(\"sample_id\")[[\"window_start\",\"window_end\"]]\n",
    "# align rows to NPZ sample order\n",
    "try:\n",
    "    win_start = meta.loc[sids, \"window_start\"].astype(str).values\n",
    "    win_end   = meta.loc[sids, \"window_end\"].astype(str).values\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"sample_id mismatch between NPZ and CSV: {e}\")\n",
    "\n",
    "# sampling config (from your generator)\n",
    "fs = np.int32(20)       # sampling_rate\n",
    "win_sec = np.int32(90)  # window_duration_sec\n",
    "\n",
    "np.savez_compressed(\n",
    "    DST_NPZ,\n",
    "    waveforms=waves,\n",
    "    detect_label=detect,\n",
    "    mag_class=mag_class,\n",
    "    mag_cls=mag_cls,\n",
    "    sample_ids=sids,\n",
    "    window_start=win_start,\n",
    "    window_end=win_end,\n",
    "    fs=fs,\n",
    "    win_sec=win_sec,\n",
    ")\n",
    "print(\"[OK] wrote\", DST_NPZ)\n",
    "print(\"waveforms shape:\", waves.shape, \"fs:\", int(fs), \"win_sec:\", int(win_sec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf3931",
   "metadata": {},
   "source": [
    "### Appendix CNN-1: Unified NPZ Construction (Final)\n",
    "\n",
    "This script builds a unified dataset (`data/wave_mag_dataset.npz`) by combining the **Tohoku earthquake catalog** with the **continuous MiniSEED waveforms** from station *IU.MAJO.00.BHZ*.  \n",
    "The resulting NPZ is the **standardized input** for both the CNN+BiLSTM+Attention model and the XGBoost baseline.\n",
    "\n",
    "**Main steps:**\n",
    "- **Catalog normalization:**  \n",
    "  Convert event times to UTC-naive timestamps; map magnitudes to three categories (*S, M, L*).  \n",
    "- **Positive sample extraction:**  \n",
    "  For each catalogued event, extract a 90s window (20s pre + 70s post).  \n",
    "- **Negative sample extraction:**  \n",
    "  Sliding 90s windows (step = 45s) from non-event periods, excluding ¬±120s around events.  \n",
    "  A per-file cap limits negatives to ‚â§3√ó positives.  \n",
    "- **Waveform preprocessing:**  \n",
    "  Detrend, bandpass filter (0.1‚Äì8 Hz), and resample to 20 Hz (final length = 1800 samples).  \n",
    "\n",
    "**Output file:**\n",
    "- `data/wave_mag_dataset.npz`  \n",
    "  - `waveforms`: (N, T) float32 array of windows  \n",
    "  - `detect_label`: binary 0/1 (noise vs earthquake)  \n",
    "  - `mag_class`: int {0=S, 1=M, 2=L, -1=noise}  \n",
    "  - `mag_cls`: str {\"S\",\"M\",\"L\",\"noise\"}  \n",
    "  - `sample_ids`: unique identifiers  \n",
    "  - `window_start`, `window_end`: ISO timestamps  \n",
    "  - `fs`: sampling rate (20 Hz)  \n",
    "  - `win_sec`: window duration (90 s)  \n",
    "\n",
    "**Summary:**  \n",
    "This NPZ provides a consistent **contract** for training and evaluating both baseline and deep learning models, with aligned labels, metadata, and waveform arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d3f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAJO_2011-03-01.mseed] +pos=10, +neg=30, totals pos=10, neg=30\n",
      "[MAJO_2011-03-02.mseed] +pos=5, +neg=15, totals pos=15, neg=45\n",
      "[MAJO_2011-03-03.mseed] +pos=8, +neg=24, totals pos=23, neg=69\n",
      "[MAJO_2011-03-04.mseed] +pos=9, +neg=27, totals pos=32, neg=96\n",
      "[MAJO_2011-03-05.mseed] +pos=10, +neg=30, totals pos=42, neg=126\n",
      "[MAJO_2011-03-06.mseed] +pos=11, +neg=33, totals pos=53, neg=159\n",
      "[MAJO_2011-03-07.mseed] +pos=12, +neg=36, totals pos=65, neg=195\n",
      "[MAJO_2011-03-08.mseed] +pos=8, +neg=24, totals pos=73, neg=219\n",
      "[MAJO_2011-03-09.mseed] +pos=138, +neg=414, totals pos=211, neg=633\n",
      "[MAJO_2011-03-10.mseed] +pos=61, +neg=183, totals pos=272, neg=816\n",
      "[MAJO_2011-03-11.mseed] +pos=1319, +neg=406, totals pos=1591, neg=1222\n",
      "[MAJO_2011-03-12.mseed] +pos=1082, +neg=4, totals pos=2673, neg=1226\n",
      "[MAJO_2011-03-13.mseed] +pos=803, +neg=58, totals pos=3476, neg=1284\n",
      "[MAJO_2011-03-14.mseed] +pos=660, +neg=67, totals pos=4136, neg=1351\n",
      "[MAJO_2011-03-15.mseed] +pos=613, +neg=89, totals pos=4749, neg=1440\n",
      "[MAJO_2011-03-16.mseed] +pos=532, +neg=192, totals pos=5281, neg=1632\n",
      "[MAJO_2011-03-17.mseed] +pos=522, +neg=164, totals pos=5803, neg=1796\n",
      "[MAJO_2011-03-18.mseed] +pos=397, +neg=365, totals pos=6200, neg=2161\n",
      "[MAJO_2011-03-19.mseed] +pos=394, +neg=380, totals pos=6594, neg=2541\n",
      "[MAJO_2011-03-20.mseed] +pos=362, +neg=361, totals pos=6956, neg=2902\n",
      "[OK] Saved -> data/wave_mag_dataset.npz\n",
      "Shapes: (9858, 1800) | detect_label counts: {0: 2902, 1: 6956}\n",
      "mag_class counts (incl. -1 noise): {-1: 2902, 1: 6956}\n"
     ]
    }
   ],
   "source": [
    "# Appendix CNN-1 (final fixed) ‚Äî Build a unified NPZ from Tohoku catalog + MiniSEED waveforms\n",
    "# Output: data/wave_mag_dataset.npz\n",
    "# This NPZ is shared by both the main model (CNN+BiLSTM+Attention) and the XGBoost baseline.\n",
    "\n",
    "import os, numpy as np, pandas as pd\n",
    "from glob import glob\n",
    "from obspy import read, UTCDateTime\n",
    "\n",
    "# ----------------- Configs -----------------\n",
    "waveform_dir        = \"waveforms\"                          # where your *.mseed files live\n",
    "tohoku_csv          = \"tohoku_earthquake_features_2011.csv\"\n",
    "npz_out             = \"data/wave_mag_dataset.npz\"\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "target_fs           = 20        # resample/ensure 20 Hz so each 90 s window has T = 1800 samples\n",
    "win_sec             = 90        # 20 s pre + 70 s post\n",
    "pre_event_sec       = 20\n",
    "post_event_sec      = 70\n",
    "exclude_margin_sec  = 120       # negatives avoid events by ¬±120 s\n",
    "slide_step_sec      = 45        # negative sliding step (seconds)\n",
    "max_neg_ratio       = 3.0       # per-file cap: negatives <= 3x positives (avoid huge imbalance)\n",
    "bandpass_min, bandpass_max = 0.1, 8.0   # preprocessing band\n",
    "\n",
    "# ----------------- Load & normalize catalog (Tohoku) -----------------\n",
    "cat = pd.read_csv(tohoku_csv)\n",
    "\n",
    "# KEY FIX 1: unify to UTC-naive pandas Timestamps to avoid tz comparison errors.\n",
    "# If CSV is in UTC (e.g., ends with 'Z'), utc=True parses as tz-aware; tz_localize(None) drops tz info but keeps UTC clock time.\n",
    "cat[\"event_time\"] = pd.to_datetime(cat[\"event_time\"], utc=True).dt.tz_localize(None)\n",
    "\n",
    "# Normalize magnitude categories to S/M/L (strings). If absent, bin numeric magnitude.\n",
    "def to_SML(x: str) -> str:\n",
    "    x = str(x).lower()\n",
    "    if x.startswith(\"s\"): return \"S\"\n",
    "    if x.startswith(\"m\"): return \"M\"  # moderate / medium\n",
    "    if x.startswith(\"l\"): return \"L\"\n",
    "    # fallback if unknown\n",
    "    return \"S\"\n",
    "\n",
    "if \"magnitude_category\" in cat.columns:\n",
    "    cat[\"mag_cls\"] = cat[\"magnitude_category\"].map(to_SML)\n",
    "elif \"magnitude\" in cat.columns:\n",
    "    bins   = [-np.inf, 5.0, 6.0, np.inf]  # S: <5.0, M: [5.0,6.0), L: ‚â•6.0\n",
    "    labels = [\"S\",\"M\",\"L\"]\n",
    "    cat[\"mag_cls\"] = pd.cut(cat[\"magnitude\"].astype(float), bins=bins, labels=labels, right=False)\n",
    "else:\n",
    "    raise ValueError(\"Tohoku CSV must have 'magnitude_category' or 'magnitude' to form S/M/L.\")\n",
    "\n",
    "# For efficient negative masking in pandas space (stay tz-naive everywhere)\n",
    "event_times_pd = cat[\"event_time\"]\n",
    "\n",
    "def overlaps_with_events_pd(event_times: pd.Series, start_ts: pd.Timestamp, end_ts: pd.Timestamp, margin_sec: int) -> bool:\n",
    "    \"\"\"Return True if any event_time falls within [start_ts - margin, end_ts + margin].\"\"\"\n",
    "    margin = pd.to_timedelta(margin_sec, unit=\"s\")\n",
    "    window_start = start_ts - margin\n",
    "    window_end   = end_ts + margin\n",
    "    # Vectorized check using boolean mask\n",
    "    return ((event_times >= window_start) & (event_times <= window_end)).any()\n",
    "\n",
    "# ----------------- Containers -----------------\n",
    "waves, y_detect, y_mag_int, y_mag_str = [], [], [], []\n",
    "sample_ids, t_start_list, t_end_list  = [], [], []\n",
    "\n",
    "pos_total = 0\n",
    "neg_total = 0\n",
    "\n",
    "# ----------------- Iterate over MiniSEED files -----------------\n",
    "mseed_files = sorted(glob(os.path.join(waveform_dir, \"*.mseed\")))\n",
    "if not mseed_files:\n",
    "    raise FileNotFoundError(f\"No .mseed files found under: {waveform_dir}\")\n",
    "\n",
    "for fp in mseed_files:\n",
    "    added_pos, added_neg = 0, 0\n",
    "    try:\n",
    "        st = read(fp)\n",
    "        st.detrend(\"demean\")\n",
    "        st.filter(\"bandpass\", freqmin=bandpass_min, freqmax=bandpass_max)\n",
    "        tr = st[0]\n",
    "\n",
    "        # Resample to target_fs if needed\n",
    "        if abs(tr.stats.sampling_rate - target_fs) > 1e-6:\n",
    "            tr.resample(sampling_rate=target_fs)\n",
    "\n",
    "        fs = int(round(tr.stats.sampling_rate))\n",
    "        assert fs == target_fs, f\"Unexpected fs={fs} after resample.\"\n",
    "        win_len = fs * win_sec\n",
    "\n",
    "        t0_file, t1_file = tr.stats.starttime, tr.stats.endtime\n",
    "\n",
    "        # KEY FIX 2: build tz-naive pandas Timestamps directly from UTCDateTime.datetime (no string round-trip).\n",
    "        safe_start = pd.Timestamp((t0_file + pre_event_sec).datetime)  # tz-naive\n",
    "        safe_end   = pd.Timestamp((t1_file - post_event_sec).datetime) # tz-naive\n",
    "\n",
    "        # Events whose full [t-20s, t+70s] window fits inside this file coverage\n",
    "        try:\n",
    "            evs = cat[(cat[\"event_time\"] >= safe_start) & (cat[\"event_time\"] <= safe_end)]\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {os.path.basename(fp)}: event filter error: {e}\")\n",
    "            evs = cat.iloc[0:0]  # empty to keep running\n",
    "\n",
    "        # ---- Positives: event-driven windows (Tohoku -> MiniSEED) ----\n",
    "        for _, row in evs.iterrows():\n",
    "            ev_ts = row[\"event_time\"]                               # pandas TS (tz-naive)\n",
    "            ev_utc = UTCDateTime(ev_ts.to_pydatetime())             # KEY FIX 4: avoid string; build UTCDateTime safely\n",
    "            t0 = ev_utc - pre_event_sec\n",
    "            t1 = ev_utc + post_event_sec\n",
    "\n",
    "            x = tr.slice(starttime=t0, endtime=t1).data\n",
    "            if len(x) < win_len:      # guard against gaps\n",
    "                continue\n",
    "            x = x[:win_len].astype(np.float32)\n",
    "\n",
    "            waves.append(x)\n",
    "            y_detect.append(1)\n",
    "            mag_map = {\"S\":0, \"M\":1, \"L\":2}\n",
    "            y_mag_int.append(mag_map[str(row[\"mag_cls\"])])\n",
    "            y_mag_str.append(str(row[\"mag_cls\"]))\n",
    "            sid = f\"pos_{pos_total:06d}\"\n",
    "            sample_ids.append(sid)\n",
    "            t_start_list.append(str(t0))\n",
    "            t_end_list.append(str(t1))\n",
    "            pos_total += 1\n",
    "            added_pos += 1\n",
    "\n",
    "        # ---- Negatives: sliding windows avoiding events ¬± margin ----\n",
    "        neg_limit = int(max_neg_ratio * max(1, added_pos))  # per-file cap\n",
    "        win_start = t0_file\n",
    "        while (win_start + win_sec) <= t1_file and added_neg < neg_limit:\n",
    "            win_end = win_start + win_sec\n",
    "\n",
    "            # KEY FIX 3: negative overlap check in tz-naive pandas space\n",
    "            start_ts = pd.Timestamp(win_start.datetime)\n",
    "            end_ts   = pd.Timestamp(win_end.datetime)\n",
    "\n",
    "            if not overlaps_with_events_pd(event_times_pd, start_ts, end_ts, exclude_margin_sec):\n",
    "                x = tr.slice(starttime=win_start, endtime=win_end).data\n",
    "                if len(x) >= win_len:\n",
    "                    x = x[:win_len].astype(np.float32)\n",
    "                    waves.append(x)\n",
    "                    y_detect.append(0)\n",
    "                    y_mag_int.append(-1)          # -1 for noise\n",
    "                    y_mag_str.append(\"noise\")\n",
    "                    sid = f\"neg_{neg_total:06d}\"\n",
    "                    sample_ids.append(sid)\n",
    "                    t_start_list.append(str(win_start))\n",
    "                    t_end_list.append(str(win_end))\n",
    "                    neg_total += 1\n",
    "                    added_neg += 1\n",
    "\n",
    "            win_start += slide_step_sec\n",
    "\n",
    "        print(f\"[{os.path.basename(fp)}] +pos={added_pos}, +neg={added_neg}, totals pos={pos_total}, neg={neg_total}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {os.path.basename(fp)}: {e}\")\n",
    "\n",
    "# ----------------- Save NPZ (unified contract) -----------------\n",
    "waves = np.asarray(waves, dtype=np.float32)\n",
    "detect = np.asarray(y_detect, dtype=np.int8)\n",
    "mag_i  = np.asarray(y_mag_int, dtype=np.int8)\n",
    "mag_s  = np.asarray(y_mag_str, dtype=object)\n",
    "sids   = np.asarray(sample_ids, dtype=object)\n",
    "wst    = np.asarray(t_start_list, dtype=object)\n",
    "wed    = np.asarray(t_end_list, dtype=object)\n",
    "\n",
    "np.savez_compressed(\n",
    "    npz_out,\n",
    "    waveforms=waves,               # (N, T)\n",
    "    detect_label=detect,           # 0/1\n",
    "    mag_class=mag_i,               # 0/1/2 for S/M/L; -1 for noise\n",
    "    mag_cls=mag_s,                 # \"S\"/\"M\"/\"L\"/\"noise\"\n",
    "    sample_ids=sids,\n",
    "    window_start=wst,\n",
    "    window_end=wed,\n",
    "    fs=np.int32(target_fs),\n",
    "    win_sec=np.int32(win_sec),\n",
    ")\n",
    "\n",
    "# Quick summary\n",
    "def counts(arr):\n",
    "    if len(arr) == 0: return {}\n",
    "    keys, vals = np.unique(arr, return_counts=True)\n",
    "    return {int(k) if isinstance(k, (np.integer,)) else str(k): int(v) for k, v in zip(keys, vals)}\n",
    "\n",
    "print(f\"[OK] Saved -> {npz_out}\")\n",
    "print(\"Shapes:\", waves.shape, \"| detect_label counts:\", counts(detect))\n",
    "print(\"mag_class counts (incl. -1 noise):\", counts(mag_i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64e0bd",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "\n",
    "**Provenance**\n",
    "\n",
    "* **Continuous waveforms (MiniSEED):** IU.MAJO.00.BHZ, time span **2011-03-01 ‚Üí 2011-03-20**.\n",
    "* **Event catalog:** `tohoku_earthquake_features_2011.csv` (contains `event_time` and magnitude info).\n",
    "* **Tasks:** (i) 3-class magnitude classification **S/M/L** (Small/Moderate/Large), (ii) fair comparison to a feature-engineered baseline (XGBoost + band-energy features).\n",
    "\n",
    "**Construction Pipeline**\n",
    "\n",
    "1. Parse catalog `event_time` as **UTC-naive** timestamps to avoid timezone comparison issues.\n",
    "2. Detrend and band-pass filter waveforms to **0.1‚Äì8 Hz**, resample to **20 Hz**.\n",
    "3. **Positives:** for each catalog event, extract a **90 s** window **\\[‚Äì20 s, +70 s]** around the event time.\n",
    "4. **Negatives:** slide a 90 s window every **45 s**, but **exclude** any window overlapping **¬±120 s** around **any** catalog event; per file, cap negatives to **‚â§ 3√ó** that file‚Äôs positives (mitigates extreme imbalance).\n",
    "5. Package everything into `data/wave_mag_dataset.npz`. Both the deep model and the baseline **share exactly the same windows** and **the same time split**.\n",
    "\n",
    "**Labels**\n",
    "\n",
    "* `detect_label`: **0/1** = noise/earthquake.\n",
    "* `mag_cls`: **\"S\"/\"M\"/\"L\"/\"noise\"**.\n",
    "* `mag_class`: **0/1/2/-1** = **S/M/L/noise** (used for training).\n",
    "\n",
    "**Windowing & Preprocessing**\n",
    "\n",
    "* Sampling rate (`fs`): **20 Hz**.\n",
    "* Window length (`win_sec`): **90 s** ‚Üí **T = 90 √ó 20 = 1800** samples.\n",
    "* Preprocessing: detrend (demean) + **0.1‚Äì8 Hz** band-pass.\n",
    "* Negative exclusion: no overlap with any catalog event within **¬±120 s**.\n",
    "\n",
    "**Files & Schemas**\n",
    "\n",
    "* **Unified dataset (for both CNN and Baseline):** `data/wave_mag_dataset.npz`\n",
    "\n",
    "  * `waveforms`: shape `(N, 1800)`, `float32`\n",
    "  * `detect_label`: `(N,)`, `int8` (0/1)\n",
    "  * `mag_class`: `(N,)`, `int8` (0/1/2/-1)\n",
    "  * `mag_cls`: `(N,)`, `object` (\"S\"/\"M\"/\"L\"/\"noise\")\n",
    "  * `sample_ids`, `window_start`, `window_end`, `fs`=20, `win_sec`=90\n",
    "* **Derived features (for XGBoost baseline):**\n",
    "\n",
    "  * `data/features_from_npz_detect.csv` (all windows, incl. 0/1)\n",
    "  * `data/features_from_npz_mag.csv` (positives only, S/M/L)\n",
    "  * Columns: `sample_id, window_start, detect_label, mag_cls, energy_0.5_2Hz, energy_2_5Hz, energy_5_8Hz`\n",
    "* **Frozen split (shared by both models):** `runs/frozen_splits.json` (time-based 80/20, stored by `sample_id`).\n",
    "\n",
    "**Current Build Stats**\n",
    "\n",
    "* Total windows **N = 9,858**; each window length **T = 1,800**.\n",
    "* `detect_label` counts: **1 ‚Üí 6,956** (earthquake), **0 ‚Üí 2,902** (noise).\n",
    "* `mag_class` counts: **-1 ‚Üí 2,902** (noise); S/M/L distribution follows the catalog and is validated in code (QC cell).\n",
    "\n",
    "**Fair Comparison**\n",
    "\n",
    "* The deep model (CNN + BiLSTM + Attention) and the XGBoost baseline **strictly share**:\n",
    "\n",
    "  1. **The same set of waveform windows** (from the NPZ), and\n",
    "  2. **The same time-based split** (frozen by `sample_id` in `runs/frozen_splits.json`).\n",
    "* This removes selection bias and ensures a true **apples-to-apples** evaluation.\n",
    "\n",
    "**Reproducibility Notes**\n",
    "\n",
    "* Key parameters: `fs=20 Hz`, `win_sec=90 s`, band-pass `0.1‚Äì8 Hz`, negative exclusion `¬±120 s`, negative cap `‚â§3√ó`, slide step `45 s`, time split `80/20`.\n",
    "* Rebuilding the dataset may change counts, but file structure and training pipeline remain identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc703c3",
   "metadata": {},
   "source": [
    "### QC: Validate and Hotfix Magnitude Labels\n",
    "\n",
    "This script performs a quick quality check on `data/wave_mag_dataset.npz` and applies a minimal hotfix if label collapse is detected.\n",
    "\n",
    "- **Check:**  \n",
    "  Load `detect_label` and `mag_class` and print class counts (note: `-1` denotes noise).  \n",
    "  If the **positive subset** (`detect_label == 1`) of `mag_class` has **fewer than 3 unique values**, it indicates label collapse.\n",
    "\n",
    "- **Hotfix:**  \n",
    "  Rebuild integer magnitude labels from the string labels `mag_cls` using:  \n",
    "  `{\"S\": 0, \"M\": 1, \"L\": 2, \"noise\": -1}`.  \n",
    "  Save back to the same NPZ, preserving all other arrays and metadata.\n",
    "\n",
    "- **Outcome:**  \n",
    "  Ensures S/M/L classes are correctly encoded for training while keeping dataset structure intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c65fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect_label counts: {1: 6956, 0: 2902}\n",
      "mag_class counts: {1: 6956, -1: 2902}\n",
      "[OK] mag_class rebuilt from mag_cls.\n"
     ]
    }
   ],
   "source": [
    "# QC ‚Äî check mag labels; hotfix if needed\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "NPZ = \"data/wave_mag_dataset.npz\"\n",
    "d = np.load(NPZ, allow_pickle=True)\n",
    "\n",
    "det = d[\"detect_label\"].astype(int)\n",
    "mag_i = d[\"mag_class\"].astype(int)\n",
    "print(\"detect_label counts:\", pd.Series(det).value_counts().to_dict())\n",
    "print(\"mag_class counts:\", pd.Series(mag_i).value_counts().to_dict())  # -1 ÊòØ noise\n",
    "\n",
    "# Â¶ÇÊûú‰Ω†ÂèëÁé∞Ê≠£Ê†∑Êú¨ÁöÑ mag_class Âè™Êúâ‰∏Ä‰∏™ÂÄºÔºà‰æãÂ¶ÇÂÖ®ÊòØ 1ÔºâÔºåÁî®Â≠óÁ¨¶‰∏≤Ê†áÁ≠æÈáçÂª∫Ôºö\n",
    "if pd.Series(mag_i[det==1]).nunique() < 3:\n",
    "    map2int = {\"S\":0, \"M\":1, \"L\":2, \"noise\":-1}\n",
    "    mag_cls = d[\"mag_cls\"].astype(str)\n",
    "    mag_new = np.array([map2int.get(s, -1) for s in mag_cls], dtype=np.int8)\n",
    "    np.savez_compressed(\n",
    "        NPZ,\n",
    "        waveforms=d[\"waveforms\"], detect_label=det,\n",
    "        mag_class=mag_new, mag_cls=d[\"mag_cls\"],\n",
    "        sample_ids=d[\"sample_ids\"], window_start=d[\"window_start\"], window_end=d[\"window_end\"],\n",
    "        fs=d[\"fs\"], win_sec=d[\"win_sec\"],\n",
    "    )\n",
    "    print(\"[OK] mag_class rebuilt from mag_cls.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27038422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mag_cls (strings) among positives:\n",
      " M    6956\n",
      "dtype: int64\n",
      "\n",
      "mag_class (ints) among positives:\n",
      " 1    6956\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "d = np.load(\"data/wave_mag_dataset.npz\", allow_pickle=True)\n",
    "\n",
    "det = d[\"detect_label\"].astype(int)\n",
    "mag_i = d[\"mag_class\"].astype(int)      # 0:S, 1:M, 2:L, -1:noise\n",
    "mag_s = d[\"mag_cls\"].astype(str)\n",
    "\n",
    "print(\"mag_cls (strings) among positives:\\n\", pd.Series(mag_s[det==1]).value_counts())\n",
    "print(\"\\nmag_class (ints) among positives:\\n\", pd.Series(mag_i[det==1]).value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b57207",
   "metadata": {},
   "source": [
    "### QC Repair: Align Positive Labels to Catalog (Timezone-Safe)\n",
    "\n",
    "**Goal:**  \n",
    "Timezone-safe realignment of **positive windows** in `data/wave_mag_dataset.npz` to the Tohoku catalog (`tohoku_earthquake_features_2011.csv`) and **rebuilding S/M/L labels** from catalog magnitudes.\n",
    "\n",
    "**Inputs**\n",
    "- `data/wave_mag_dataset.npz`: contains `waveforms`, `detect_label`, `mag_class`, `mag_cls`, `window_start`, `window_end`, `fs`, `win_sec`, ‚Ä¶\n",
    "- `tohoku_earthquake_features_2011.csv`: contains `event_time`, `magnitude`, and optional `magnitude_category`.\n",
    "\n",
    "**Method**\n",
    "1. **Select positives**: `detect_label == 1`.  \n",
    "2. **Reconstruct event time** per window: `evt_from_window = window_start + 20 s`.  \n",
    "   - Parse to **UTC-naive** timestamps to avoid timezone comparison errors.  \n",
    "3. **Nearest-time join** to catalog `event_time` with a tolerance of **5 s**, and fallback to **10 s** if needed.  \n",
    "4. **Derive S/M/L** from catalog:  \n",
    "   - Prefer numeric `magnitude` thresholds: S (<5.0), M ([5.0,6.0)), L (‚â•6.0).  \n",
    "   - Fallback to `magnitude_category` text if numeric magnitude is missing.  \n",
    "5. **Write back to NPZ**:  \n",
    "   - Update matched **positives**: `mag_class` (0/1/2 for S/M/L), `mag_cls` (\"S\"/\"M\"/\"L\").  \n",
    "   - Force all **negatives** to `\"noise\"` / `-1`.  \n",
    "   - Preserve all other arrays and metadata.\n",
    "\n",
    "**Outputs & Checks**\n",
    "- Overwrites `data/wave_mag_dataset.npz` in place (schema unchanged).  \n",
    "- Logs **matched / unmatched** counts.  \n",
    "- Prints **label distributions** among positives for sanity check.\n",
    "\n",
    "**Why this matters**\n",
    "- Ensures positive labels (S/M/L) are **catalog-consistent** and **timezone-robust**, preventing silent label drift or class collapse before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cad7f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] positives=6956, matched=6956, unmatched=0\n",
      "[OK] Saved fixed mag_class/mag_cls to data/wave_mag_dataset.npz\n",
      "mag_cls among positives:\n",
      " S    6382\n",
      "M     515\n",
      "L      59\n",
      "dtype: int64\n",
      "mag_class among positives:\n",
      " 0    6382\n",
      "1     515\n",
      "2      59\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Repair mag_class/mag_cls by aligning positives to the Tohoku catalog (timezone-safe)\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "NPZ = \"data/wave_mag_dataset.npz\"\n",
    "CSV = \"tohoku_earthquake_features_2011.csv\"\n",
    "\n",
    "d = np.load(NPZ, allow_pickle=True)\n",
    "\n",
    "# --- 1) Select positive windows and compute event times (UTC-naive) ---\n",
    "mask_pos = d[\"detect_label\"].astype(int) == 1\n",
    "\n",
    "# NOTE: to_datetime returns a DatetimeIndex here; use .tz_localize(None) directly (no .dt)\n",
    "wstart_idx = pd.to_datetime(d[\"window_start\"][mask_pos], utc=True).tz_localize(None)\n",
    "evt_from_window = wstart_idx + pd.to_timedelta(20, \"s\")   # event time = window_start + 20s\n",
    "\n",
    "pos_df = pd.DataFrame({\n",
    "    \"idx\": np.arange(len(d[\"sample_ids\"]))[mask_pos],\n",
    "    \"evt_from_window\": evt_from_window\n",
    "}).sort_values(\"evt_from_window\").reset_index(drop=True)\n",
    "\n",
    "# --- 2) Load catalog and normalize to UTC-naive ---\n",
    "cat = pd.read_csv(CSV)\n",
    "if \"event_time\" not in cat.columns:\n",
    "    raise ValueError(\"Catalog must contain 'event_time'.\")\n",
    "cat[\"event_time\"] = pd.to_datetime(cat[\"event_time\"], utc=True).dt.tz_localize(None)\n",
    "cat = cat.sort_values(\"event_time\").reset_index(drop=True)\n",
    "\n",
    "# --- 3) Nearest-time join (try 5s tolerance, fallback to 10s) ---\n",
    "def nearest_join(tol_seconds: int):\n",
    "    return pd.merge_asof(\n",
    "        pos_df,\n",
    "        cat.rename(columns={\"event_time\":\"evt_cat\"})[\n",
    "            [\"evt_cat\"] + [c for c in cat.columns if c != \"event_time\"]\n",
    "        ],\n",
    "        left_on=\"evt_from_window\",\n",
    "        right_on=\"evt_cat\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(seconds=tol_seconds)\n",
    "    )\n",
    "\n",
    "m = nearest_join(5)\n",
    "if m[\"evt_cat\"].isna().any():\n",
    "    m = nearest_join(10)\n",
    "\n",
    "unmatched = int(m[\"evt_cat\"].isna().sum())\n",
    "total_pos = len(m)\n",
    "print(f\"[Info] positives={total_pos}, matched={total_pos - unmatched}, unmatched={unmatched}\")\n",
    "\n",
    "# --- 4) Derive S/M/L from catalog (prefer numeric magnitude; fallback to text category) ---\n",
    "def to_SML_from_row(row):\n",
    "    if \"magnitude\" in m.columns and pd.notna(row.get(\"magnitude\", np.nan)):\n",
    "        mag = float(row[\"magnitude\"])\n",
    "        if mag < 5.0: return \"S\"\n",
    "        elif mag < 6.0: return \"M\"\n",
    "        else: return \"L\"\n",
    "    # Fallback to text category if magnitude missing\n",
    "    catg = str(row.get(\"magnitude_category\", \"\")).lower()\n",
    "    if catg.startswith(\"s\"): return \"S\"\n",
    "    if catg.startswith(\"m\"): return \"M\"   # moderate / medium\n",
    "    if catg.startswith(\"l\"): return \"L\"\n",
    "    return \"M\"  # conservative fallback\n",
    "\n",
    "m[\"mag_cls_new\"] = m.apply(to_SML_from_row, axis=1)\n",
    "map2int = {\"S\":0, \"M\":1, \"L\":2}\n",
    "\n",
    "# --- 5) Write back to NPZ (update positives that matched; keep negatives as noise) ---\n",
    "mag_class = d[\"mag_class\"].astype(int).copy()\n",
    "mag_cls   = d[\"mag_cls\"].astype(object).copy()\n",
    "\n",
    "matched_mask = m[\"evt_cat\"].notna()\n",
    "mag_class[m.loc[matched_mask, \"idx\"].values] = m.loc[matched_mask, \"mag_cls_new\"].map(map2int).astype(np.int8).values\n",
    "mag_cls[m.loc[matched_mask, \"idx\"].values]   = m.loc[matched_mask, \"mag_cls_new\"].values\n",
    "\n",
    "# Ensure negatives are noise\n",
    "neg_mask = d[\"detect_label\"].astype(int) == 0\n",
    "mag_class[neg_mask] = -1\n",
    "mag_cls[neg_mask]   = \"noise\"\n",
    "\n",
    "np.savez_compressed(\n",
    "    NPZ,\n",
    "    waveforms=d[\"waveforms\"],\n",
    "    detect_label=d[\"detect_label\"],\n",
    "    mag_class=mag_class,\n",
    "    mag_cls=mag_cls,\n",
    "    sample_ids=d[\"sample_ids\"],\n",
    "    window_start=d[\"window_start\"],\n",
    "    window_end=d[\"window_end\"],\n",
    "    fs=d[\"fs\"],\n",
    "    win_sec=d[\"win_sec\"],\n",
    ")\n",
    "\n",
    "# --- 6) Quick report ---\n",
    "print(\"[OK] Saved fixed mag_class/mag_cls to\", NPZ)\n",
    "print(\"mag_cls among positives:\\n\", pd.Series(mag_cls[d[\"detect_label\"].astype(int)==1]).value_counts())\n",
    "print(\"mag_class among positives:\\n\", pd.Series(mag_class[d[\"detect_label\"].astype(int)==1]).value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb45490",
   "metadata": {},
   "source": [
    "### Frozen Time-Based Split (`runs/frozen_splits.json`)\n",
    "\n",
    "This script creates **reproducible, time-ordered 80/20 splits** by `sample_id` for two tasks:\n",
    "\n",
    "- **Detection (`detect`)**: uses **all windows** (earthquake + noise).  \n",
    "  Sort by `window_start`; take the **earliest 80%** as train and the **latest 20%** as test.\n",
    "\n",
    "- **Magnitude classification (`magcls`)**: uses **positives only** (`detect_label==1` and `mag_class‚â•0`).  \n",
    "  Apply the same **time-based 80/20** split on this subset.\n",
    "\n",
    "**Output file**\n",
    "- `runs/frozen_splits.json` with:\n",
    "  - `detect.train_ids`, `detect.test_ids`\n",
    "  - `magcls.train_ids`, `magcls.test_ids`\n",
    "\n",
    "Using a single frozen split prevents temporal leakage and ensures **apples-to-apples** comparisons across CNN and XGBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a183a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote runs/frozen_splits.json\n"
     ]
    }
   ],
   "source": [
    "# Freeze a single time-based split, saved by sample_id\n",
    "import numpy as np, pandas as pd, json\n",
    "\n",
    "d = np.load(\"data/wave_mag_dataset.npz\", allow_pickle=True)\n",
    "sid = d[\"sample_ids\"].astype(str)\n",
    "ts  = pd.to_datetime(d[\"window_start\"])\n",
    "det = d[\"detect_label\"].astype(int)\n",
    "mag = d[\"mag_class\"].astype(int)  # 0/1/2; -1 for noise\n",
    "\n",
    "# ÂÖ®‰ΩìÔºàÂê´Âô™Â£∞ÔºâÊåâÊó∂Èó¥ 80/20 ÂàáÔºåÁî®‰∫é‚ÄúÊ£ÄÊµã‰ªªÂä°‚ÄùÔºõ‰∏âÂàÜÁ±ªÂè™Áî® S/M/L Â≠êÈõÜÂÜçÊåâÊó∂Èó¥ 80/20 Âàá\n",
    "order_all = np.argsort(ts.values)\n",
    "cut_all = int(0.8 * len(order_all))\n",
    "split = {\n",
    "    \"detect\": {\n",
    "        \"train_ids\": sid[order_all[:cut_all]].tolist(),\n",
    "        \"test_ids\":  sid[order_all[cut_all:]].tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "mask_pos = (det==1) & (mag>=0)\n",
    "sid_pos = sid[mask_pos]; ts_pos = ts[mask_pos]\n",
    "order_pos = np.argsort(ts_pos.values)\n",
    "cut_pos = int(0.8 * len(order_pos))\n",
    "split[\"magcls\"] = {\n",
    "    \"train_ids\": sid_pos[order_pos[:cut_pos]].tolist(),\n",
    "    \"test_ids\":  sid_pos[order_pos[cut_pos:]].tolist()\n",
    "}\n",
    "\n",
    "import os; os.makedirs(\"runs\", exist_ok=True)\n",
    "json.dump(split, open(\"runs/frozen_splits.json\",\"w\"))\n",
    "print(\"[OK] wrote runs/frozen_splits.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c491a38a",
   "metadata": {},
   "source": [
    "### Band-Energy Features from NPZ Windows\n",
    "\n",
    "This script derives **band-energy features** from the unified waveform windows in `data/wave_mag_dataset.npz` for use by the XGBoost baseline and for exploratory analysis.\n",
    "\n",
    "**Inputs**\n",
    "- `data/wave_mag_dataset.npz`  \n",
    "  Uses: `waveforms`, `fs`, `sample_ids`, `window_start`, `detect_label`, `mag_cls`.\n",
    "\n",
    "**Method**\n",
    "- For each window, z-score normalize (mean/std) and compute energy (sum of squared amplitudes) in three bands using 4th-order Butterworth SOS filters:\n",
    "  - 0.5‚Äì2 Hz, 2‚Äì5 Hz, 5‚Äì8 Hz.\n",
    "- Preserve metadata: `sample_id`, `window_start`, `detect_label`, `mag_cls`.\n",
    "- Sort rows by `window_start`.\n",
    "\n",
    "**Outputs**\n",
    "- `data/features_from_npz_detect.csv` ‚Äî **all windows** (earthquake + noise) with:\n",
    "  - `sample_id`, `window_start`, `detect_label`, `mag_cls`,\n",
    "  - `energy_0.5_2Hz`, `energy_2_5Hz`, `energy_5_8Hz`.\n",
    "- `data/features_from_npz_mag.csv` ‚Äî **positives only** (`detect_label==1`, `mag_cls ‚àà {S,M,L}`) with the same columns.\n",
    "\n",
    "**Purpose**\n",
    "- Provides a compact, interpretable feature set for the **XGBoost baseline** and quick QC/analysis while staying aligned with the NPZ windows and time order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac08152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] to data/features_from_npz_detect.csv: (9858, 7)  | data/features_from_npz_mag.csv: (6956, 7)\n"
     ]
    }
   ],
   "source": [
    "# Derive band-energy features (0.5‚Äì2 / 2‚Äì5 / 5‚Äì8 Hz) from the NPZ windows\n",
    "import numpy as np, pandas as pd, os\n",
    "from scipy.signal import butter, sosfilt\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "d = np.load(\"data/wave_mag_dataset.npz\", allow_pickle=True)\n",
    "\n",
    "X   = d[\"waveforms\"]\n",
    "fs  = int(d[\"fs\"])\n",
    "sid = d[\"sample_ids\"].astype(str)\n",
    "wst = pd.to_datetime(d[\"window_start\"])\n",
    "det = d[\"detect_label\"].astype(int)\n",
    "mcs = d[\"mag_cls\"].astype(str)\n",
    "\n",
    "def band_energy(x, fmin, fmax, fs):\n",
    "    sos = butter(4, [fmin, fmax], btype=\"band\", fs=fs, output=\"sos\")\n",
    "    return float(np.sum(sosfilt(sos, x)**2))\n",
    "\n",
    "rows = []\n",
    "for i, x in enumerate(X):\n",
    "    m, s = x.mean(), x.std()\n",
    "    if s > 1e-8: x = (x - m) / s\n",
    "    rows.append({\n",
    "        \"sample_id\": sid[i],\n",
    "        \"window_start\": wst[i],\n",
    "        \"detect_label\": int(det[i]),\n",
    "        \"mag_cls\": mcs[i],\n",
    "        \"energy_0.5_2Hz\": band_energy(x, 0.5, 2.0, fs),\n",
    "        \"energy_2_5Hz\":   band_energy(x, 2.0, 5.0, fs),\n",
    "        \"energy_5_8Hz\":   band_energy(x, 5.0, 8.0, fs),\n",
    "    })\n",
    "\n",
    "feat = pd.DataFrame(rows).sort_values(\"window_start\").reset_index(drop=True)\n",
    "feat.to_csv(\"data/features_from_npz_detect.csv\", index=False)\n",
    "feat_pos = feat[(feat[\"detect_label\"]==1) & (feat[\"mag_cls\"].isin([\"S\",\"M\",\"L\"]))].copy()\n",
    "feat_pos.to_csv(\"data/features_from_npz_mag.csv\", index=False)\n",
    "print(\"[OK] to data/features_from_npz_detect.csv:\", feat.shape, \n",
    "      \" | data/features_from_npz_mag.csv:\", feat_pos.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6b93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== XGBoost baseline (S/M/L) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9854    0.9897    0.9875      1361\n",
      "           1     0.4400    0.3548    0.3929        31\n",
      "\n",
      "    accuracy                         0.9756      1392\n",
      "   macro avg     0.7127    0.6723    0.6902      1392\n",
      "weighted avg     0.9732    0.9756    0.9743      1392\n",
      "\n",
      "Confusion matrix:\n",
      " [[1347   14]\n",
      " [  20   11]]\n"
     ]
    }
   ],
   "source": [
    "# XGBoost baseline on the same split\n",
    "import json, pandas as pd, numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "splits = json.load(open(\"runs/frozen_splits.json\"))\n",
    "train_ids = set(splits[\"magcls\"][\"train_ids\"])\n",
    "test_ids  = set(splits[\"magcls\"][\"test_ids\"])\n",
    "\n",
    "df = pd.read_csv(\"data/features_from_npz_mag.csv\", parse_dates=[\"window_start\"])\n",
    "df[\"y\"] = df[\"mag_cls\"].map({\"S\":0,\"M\":1,\"L\":2})\n",
    "\n",
    "Xcols = [\"energy_0.5_2Hz\",\"energy_2_5Hz\",\"energy_5_8Hz\"]\n",
    "X_tr = df[df[\"sample_id\"].isin(train_ids)][Xcols].values\n",
    "y_tr = df[df[\"sample_id\"].isin(train_ids)][\"y\"].values\n",
    "X_te = df[df[\"sample_id\"].isin(test_ids)][Xcols].values\n",
    "y_te = df[df[\"sample_id\"].isin(test_ids)][\"y\"].values\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"xgb\", XGBClassifier(\n",
    "            n_estimators=400, max_depth=4, learning_rate=0.05,\n",
    "            subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "            objective=\"multi:softprob\", num_class=3, random_state=42,\n",
    "            eval_metric=\"mlogloss\"\n",
    "        ))\n",
    "    ])\n",
    "except Exception:\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"gbdt\", GradientBoostingClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "clf.fit(X_tr, y_tr)\n",
    "y_hat = clf.predict(X_te)\n",
    "print(\"== XGBoost baseline (S/M/L) ==\")\n",
    "print(classification_report(y_te, y_hat, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_te, y_hat).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24801154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donghui/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 done.\n",
      "epoch 2 done.\n",
      "epoch 3 done.\n",
      "epoch 4 done.\n",
      "epoch 5 done.\n",
      "epoch 6 done.\n",
      "epoch 7 done.\n",
      "epoch 8 done.\n",
      "epoch 9 done.\n",
      "epoch 10 done.\n",
      "== CNN+BiLSTM+Attn (S/M/L) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9953    0.9375    0.9656      1361\n",
      "           1     0.2170    0.7419    0.3358        31\n",
      "           2     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9332      1392\n",
      "   macro avg     0.4041    0.5598    0.4338      1392\n",
      "weighted avg     0.9780    0.9332    0.9515      1392\n",
      "\n",
      "Confusion matrix:\n",
      " [[1276   83    2]\n",
      " [   6   23    2]\n",
      " [   0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donghui/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/donghui/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/donghui/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CNN+BiLSTM+Attention for S/M/L classification (same split)\n",
    "import json, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# load NPZ\n",
    "d = np.load(\"data/wave_mag_dataset.npz\", allow_pickle=True)\n",
    "X   = d[\"waveforms\"]\n",
    "det = d[\"detect_label\"].astype(int)\n",
    "mag = d[\"mag_class\"].astype(int)\n",
    "sid = d[\"sample_ids\"].astype(str)\n",
    "\n",
    "# keep only S/M/L positives\n",
    "mask = (det==1) & (mag>=0)\n",
    "X, y, sid = X[mask], mag[mask], sid[mask]\n",
    "\n",
    "# map sample_id -> index for split\n",
    "id2idx = {s:i for i,s in enumerate(sid)}\n",
    "splits = json.load(open(\"runs/frozen_splits.json\"))\n",
    "tr_ids = [s for s in splits[\"magcls\"][\"train_ids\"] if s in id2idx]\n",
    "te_ids = [s for s in splits[\"magcls\"][\"test_ids\"]  if s in id2idx]\n",
    "tr_idx = np.array([id2idx[s] for s in tr_ids], dtype=int)\n",
    "te_idx = np.array([id2idx[s] for s in te_ids], dtype=int)\n",
    "\n",
    "class WaveDS(Dataset):\n",
    "    def __init__(self, waves, labels):\n",
    "        self.w = waves; self.y = labels.astype(np.int64)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.w[i]\n",
    "        m, s = x.mean(), x.std()\n",
    "        if s > 1e-8: x = (x - m) / s\n",
    "        return torch.from_numpy(x[None,:]), torch.tensor(self.y[i])\n",
    "\n",
    "dl_tr = DataLoader(WaveDS(X[tr_idx], y[tr_idx]), batch_size=64, shuffle=True)\n",
    "dl_te = DataLoader(WaveDS(X[te_idx], y[te_idx]), batch_size=128, shuffle=False)\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, n_classes=3, hidden=64):\n",
    "        super().__init__()\n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Conv1d(1,16,7,2,3), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16,32,5,2,2), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32,64,3,1,1), nn.BatchNorm1d(64), nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(64, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.attn_W = nn.Linear(2*hidden, 128)\n",
    "        self.attn_v = nn.Linear(128, 1, bias=False)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(2*hidden, n_classes)\n",
    "    def forward(self, x):\n",
    "        h = self.fe(x)              # (B,64,T')\n",
    "        h = h.transpose(1,2)        # (B,T',64)\n",
    "        h, _ = self.lstm(h)         # (B,T',2H)\n",
    "        a = torch.tanh(self.attn_W(h))         # (B,T',128)\n",
    "        a = self.attn_v(a).squeeze(-1)         # (B,T')\n",
    "        w = torch.softmax(a, dim=1)            # (B,T')\n",
    "        ctx = (h * w.unsqueeze(-1)).sum(1)     # (B,2H)\n",
    "        z = self.drop(ctx)\n",
    "        return self.fc(z)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNNBiLSTMAttn().to(device)\n",
    "# class weightsÔºàÊåâËÆ≠ÁªÉÈõÜÈ¢ëÊ¨°ÂèçÊØîÔºâ\n",
    "counts = np.bincount(y[tr_idx], minlength=3)\n",
    "cw = counts.sum() / np.maximum(1, counts)\n",
    "cw = (cw / cw.mean()).astype(np.float32)\n",
    "crit = nn.CrossEntropyLoss(weight=torch.tensor(cw, device=device))\n",
    "opt  = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for ep in range(10):\n",
    "    model.train()\n",
    "    for xb, yb in dl_tr:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        loss = crit(model(xb), yb)\n",
    "        opt.zero_grad(); loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        opt.step()\n",
    "    print(f\"epoch {ep+1} done.\")\n",
    "\n",
    "# eval\n",
    "model.eval()\n",
    "preds, gts = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl_te:\n",
    "        p = model(xb.to(device)).argmax(1).cpu().numpy()\n",
    "        preds.extend(p); gts.extend(yb.numpy())\n",
    "print(\"== CNN+BiLSTM+Attn (S/M/L) ==\")\n",
    "print(classification_report(gts, preds, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(gts, preds).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ae1ad",
   "metadata": {},
   "source": [
    "### QC Repair: Align Positive Labels to Catalog (Timezone-Safe)\n",
    "\n",
    "**Purpose**  \n",
    "Safely realign **positive windows** in `data/wave_mag_dataset.npz` to the Tohoku catalog and **rebuild S/M/L labels** from catalog magnitudes, avoiding timezone pitfalls.\n",
    "\n",
    "**Inputs**  \n",
    "- `data/wave_mag_dataset.npz` ‚Äî expects keys: `waveforms`, `detect_label`, `mag_class`, `mag_cls`, `sample_ids`, `window_start`, `window_end`, `fs`, `win_sec`  \n",
    "- `tohoku_earthquake_features_2011.csv` ‚Äî must include `event_time` (and ideally `magnitude` / `magnitude_category`)\n",
    "\n",
    "**Method (summary)**  \n",
    "1. **Select positives**: `detect_label == 1`.  \n",
    "2. **Reconstruct event time** for each window as `window_start + 20s` (UTC-naive).  \n",
    "3. **Normalize catalog times** to UTC-naive and sort.  \n",
    "4. **Nearest-neighbor time join** (`merge_asof`) using progressive tolerances **5s ‚Üí 10s**.  \n",
    "5. **Derive S/M/L**:  \n",
    "   - Prefer numeric `magnitude`: S (<5.0), M ([5.0,6.0)), L (‚â•6.0).  \n",
    "   - Fallback to `magnitude_category` text; final fallback = `M`.  \n",
    "6. **Write back**: update matched positives‚Äô `mag_class` (0/1/2) and `mag_cls` (‚ÄúS/M/L‚Äù); force all negatives to `-1 / \"noise\"`.  \n",
    "7. **Report**: print matched/unmatched counts and positive-class distributions (string & int).\n",
    "\n",
    "**Output**  \n",
    "- Overwrites `data/wave_mag_dataset.npz` in place (schema unchanged), with **catalog-consistent S/M/L labels** for matched positives.\n",
    "\n",
    "**Why this matters**  \n",
    "Prevents label drift/class collapse by anchoring positive labels to the authoritative catalog while being robust to timezone handling and small timing offsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e598ce96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Try tol=5s] matched=6956, unmatched=0\n",
      "[Info] positives=6956, matched=6956, unmatched=0\n",
      "[OK] Saved fixed mag_class/mag_cls to data/wave_mag_dataset.npz\n",
      "mag_cls among positives:\n",
      " S    6382\n",
      "M     515\n",
      "L      59\n",
      "dtype: int64\n",
      "mag_class among positives:\n",
      " 0    6382\n",
      "1     515\n",
      "2      59\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Repair mag_class/mag_cls by aligning positive windows to the Tohoku catalog.\n",
    "\n",
    "What this script does:\n",
    "1) Load your NPZ dataset and pick positive windows (detect_label == 1).\n",
    "2) Convert window_start to timezone-naive UTC and define the event time as window_start + EVENT_OFFSET_SEC.\n",
    "3) Load the external catalog CSV (must contain 'event_time'), convert to timezone-naive UTC.\n",
    "4) Nearest-neighbor time join (merge_asof) with progressively relaxed tolerances.\n",
    "5) Derive S/M/L class from numeric magnitude if available; otherwise fall back to text category.\n",
    "6) Write updated labels back to the NPZ (only for matched positives); set negatives as noise.\n",
    "7) Print a quick summary of matches and class distribution among positives.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====== CONFIG ======\n",
    "NPZ = \"data/wave_mag_dataset.npz\"           # Input/Output NPZ\n",
    "CSV = \"tohoku_earthquake_features_2011.csv\" # Catalog CSV; must include 'event_time'\n",
    "EVENT_OFFSET_SEC = 20                       # Event time = window_start + 20s\n",
    "TOLERANCES = [5, 10]                        # Merge tolerances in seconds (progressively relaxed)\n",
    "# ====================\n",
    "\n",
    "# --- 0) Load NPZ and verify required keys ---\n",
    "d = np.load(NPZ, allow_pickle=True)\n",
    "\n",
    "required_keys = [\n",
    "    \"waveforms\", \"detect_label\", \"mag_class\", \"mag_cls\",\n",
    "    \"sample_ids\", \"window_start\", \"window_end\", \"fs\", \"win_sec\"\n",
    "]\n",
    "missing = [k for k in required_keys if k not in d.files]\n",
    "if missing:\n",
    "    raise KeyError(f\"NPZ is missing required fields: {missing}\")\n",
    "\n",
    "# --- 1) Select positive windows and compute event times (UTC-naive) ---\n",
    "detect_label = d[\"detect_label\"].astype(int)\n",
    "mask_pos = detect_label == 1\n",
    "\n",
    "# NOTE:\n",
    "# - For ndarray/Datetime-like arrays, pd.to_datetime returns a DatetimeIndex.\n",
    "# - You should NOT use `.dt` on a DatetimeIndex. Call `.tz_localize(None)` directly.\n",
    "wstart_idx = pd.to_datetime(d[\"window_start\"][mask_pos], utc=True).tz_localize(None)\n",
    "evt_from_window = wstart_idx + pd.to_timedelta(EVENT_OFFSET_SEC, \"s\")\n",
    "\n",
    "pos_df = (\n",
    "    pd.DataFrame({\n",
    "        \"idx\": np.arange(len(d[\"sample_ids\"]))[mask_pos],\n",
    "        \"evt_from_window\": evt_from_window\n",
    "    })\n",
    "    .sort_values(\"evt_from_window\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --- 2) Load catalog and normalize to UTC-naive ---\n",
    "cat = pd.read_csv(CSV)\n",
    "if \"event_time\" not in cat.columns:\n",
    "    raise ValueError(\"Catalog CSV must contain a column named 'event_time'.\")\n",
    "\n",
    "# Here 'event_time' is a Series; using `.dt.tz_localize(None)` is correct.\n",
    "cat[\"event_time\"] = pd.to_datetime(cat[\"event_time\"], utc=True).dt.tz_localize(None)\n",
    "cat = cat.sort_values(\"event_time\").reset_index(drop=True)\n",
    "\n",
    "# Prepare the right-hand table for merge_asof\n",
    "cat_renamed = cat.rename(columns={\"event_time\": \"evt_cat\"})\n",
    "right_cols = [\"evt_cat\"] + [c for c in cat_renamed.columns if c != \"evt_cat\"]\n",
    "\n",
    "# --- 3) Nearest-time join with tolerance fallback ---\n",
    "def nearest_join(tol_seconds: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform a nearest-neighbor as-of merge between positive windows and the catalog\n",
    "    with a given tolerance (in seconds). Both sides must be sorted by their join keys.\n",
    "    \"\"\"\n",
    "    return pd.merge_asof(\n",
    "        pos_df,\n",
    "        cat_renamed[right_cols],\n",
    "        left_on=\"evt_from_window\",\n",
    "        right_on=\"evt_cat\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(seconds=tol_seconds),\n",
    "    )\n",
    "\n",
    "m = None\n",
    "for tol in TOLERANCES:\n",
    "    m = nearest_join(tol)\n",
    "    unmatched = int(m[\"evt_cat\"].isna().sum())\n",
    "    total_pos = len(m)\n",
    "    print(f\"[Try tol={tol}s] matched={total_pos - unmatched}, unmatched={unmatched}\")\n",
    "    if unmatched == 0:\n",
    "        break\n",
    "\n",
    "if m is None:\n",
    "    raise RuntimeError(\"merge_asof produced no result; please check your inputs.\")\n",
    "\n",
    "unmatched = int(m[\"evt_cat\"].isna().sum())\n",
    "total_pos = len(m)\n",
    "print(f\"[Info] positives={total_pos}, matched={total_pos - unmatched}, unmatched={unmatched}\")\n",
    "\n",
    "# --- 4) Map magnitude to S/M/L (prefer numeric magnitude; fall back to text category) ---\n",
    "def to_SML_from_row(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Priority:\n",
    "      1) If numeric 'magnitude' present and not NaN: map by thresholds (<5: S, <6: M, else L).\n",
    "      2) Else, use 'magnitude_category' prefix (s/m/l).\n",
    "      3) Else, return 'M' as a conservative default.\n",
    "    \"\"\"\n",
    "    # 1) Numeric magnitude\n",
    "    if \"magnitude\" in row.index:\n",
    "        val = row.get(\"magnitude\", np.nan)\n",
    "        if pd.notna(val):\n",
    "            mag = float(val)\n",
    "            if mag < 5.0:\n",
    "                return \"S\"\n",
    "            elif mag < 6.0:\n",
    "                return \"M\"\n",
    "            else:\n",
    "                return \"L\"\n",
    "\n",
    "    # 2) Text fallback\n",
    "    catg = str(row.get(\"magnitude_category\", \"\")).strip().lower()\n",
    "    if catg.startswith(\"s\"):\n",
    "        return \"S\"\n",
    "    if catg.startswith(\"m\"):\n",
    "        return \"M\"\n",
    "    if catg.startswith(\"l\"):\n",
    "        return \"L\"\n",
    "\n",
    "    # 3) Conservative default\n",
    "    return \"M\"\n",
    "\n",
    "m[\"mag_cls_new\"] = m.apply(to_SML_from_row, axis=1)\n",
    "map2int = {\"S\": 0, \"M\": 1, \"L\": 2}\n",
    "\n",
    "# --- 5) Write back: only update matched positives; force negatives to noise ---\n",
    "mag_class = d[\"mag_class\"].astype(int).copy()\n",
    "mag_cls   = d[\"mag_cls\"].astype(object).copy()\n",
    "\n",
    "matched_mask = m[\"evt_cat\"].notna()\n",
    "matched_idx = m.loc[matched_mask, \"idx\"].to_numpy()\n",
    "\n",
    "# Update matched positive samples\n",
    "mag_class[matched_idx] = (\n",
    "    m.loc[matched_mask, \"mag_cls_new\"].map(map2int).astype(np.int8).to_numpy()\n",
    ")\n",
    "mag_cls[matched_idx] = m.loc[matched_mask, \"mag_cls_new\"].to_numpy()\n",
    "\n",
    "# Explicitly set negatives to noise\n",
    "neg_mask = detect_label == 0\n",
    "mag_class[neg_mask] = -1\n",
    "mag_cls[neg_mask] = \"noise\"\n",
    "\n",
    "# --- 6) Save compressed NPZ with all required fields for downstream compatibility ---\n",
    "np.savez_compressed(\n",
    "    NPZ,\n",
    "    waveforms=d[\"waveforms\"],\n",
    "    detect_label=d[\"detect_label\"],\n",
    "    mag_class=mag_class,\n",
    "    mag_cls=mag_cls,\n",
    "    sample_ids=d[\"sample_ids\"],\n",
    "    window_start=d[\"window_start\"],\n",
    "    window_end=d[\"window_end\"],\n",
    "    fs=d[\"fs\"],\n",
    "    win_sec=d[\"win_sec\"],\n",
    ")\n",
    "\n",
    "# --- 7) Quick report (only among positives/events) ---\n",
    "print(\"[OK] Saved fixed mag_class/mag_cls to\", NPZ)\n",
    "\n",
    "pos_mask_after = (detect_label == 1)\n",
    "print(\n",
    "    \"mag_cls among positives:\\n\",\n",
    "    pd.Series(mag_cls[pos_mask_after]).value_counts(),\n",
    ")\n",
    "print(\n",
    "    \"mag_class among positives:\\n\",\n",
    "    pd.Series(mag_class[pos_mask_after]).value_counts().sort_index(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111147ee",
   "metadata": {},
   "source": [
    "### Stratified Split with Minimum L in Test (`runs/frozen_splits.json`)\n",
    "\n",
    "**Purpose**  \n",
    "Create a **stratified 80/20 split** for the **S/M/L** task that **guarantees at least K ‚ÄúL‚Äù samples** in the test set (to stabilize evaluation on rare large events).\n",
    "\n",
    "**Inputs**\n",
    "- `data/features_from_npz_mag.csv` with columns:\n",
    "  - `sample_id`, `window_start`, `mag_cls` ‚àà {S,M,L},\n",
    "  - `energy_0.5_2Hz`, `energy_2_5Hz`, `energy_5_8Hz`.\n",
    "\n",
    "**Method**\n",
    "- Map labels: `{\"S\":0,\"M\":1,\"L\":2}` and build features `X = [energy_0.5_2Hz, energy_2_5Hz, energy_5_8Hz]`.\n",
    "- Use `StratifiedShuffleSplit(test_size=0.2)` with **up to 200 seeds**; return the first split where **test set contains ‚â• K L-samples** (default `K=10`).\n",
    "\n",
    "**Output**\n",
    "- Writes `runs/frozen_splits.json` with:\n",
    "  ```json\n",
    "  {\n",
    "    \"magcls\": {\n",
    "      \"train_ids\": [\"...\"],\n",
    "      \"test_ids\":  [\"...\"]\n",
    "    }\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d359cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0\n",
      "train counts: {0: 5105, 1: 412, 2: 47}\n",
      "test  counts: {0: 1277, 1: 103, 2: 12}\n",
      "[OK] saved runs/frozen_splits.json\n"
     ]
    }
   ],
   "source": [
    "# Build a stratified split that guarantees at least K L-samples in test\n",
    "import json, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "DF = pd.read_csv(\"data/features_from_npz_mag.csv\", parse_dates=[\"window_start\"])\n",
    "LABEL_MAP = {\"S\":0, \"M\":1, \"L\":2}\n",
    "DF[\"y\"] = DF[\"mag_cls\"].map(LABEL_MAP).astype(int)\n",
    "\n",
    "Xcols = [\"energy_0.5_2Hz\",\"energy_2_5Hz\",\"energy_5_8Hz\"]\n",
    "X = DF[Xcols].values\n",
    "y = DF[\"y\"].values\n",
    "\n",
    "def stratified_split_with_min_L(X, y, df, K=10, test_size=0.2, max_tries=200):\n",
    "    for seed in range(max_tries):\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "        idx_tr, idx_te = next(sss.split(X, y))\n",
    "        if (y[idx_te] == 2).sum() >= K:  # ensure at least K L's in test\n",
    "            return idx_tr, idx_te, seed\n",
    "    raise RuntimeError(\"Failed to secure enough L in test; consider lowering K.\")\n",
    "\n",
    "idx_tr, idx_te, used_seed = stratified_split_with_min_L(X, y, DF, K=10, test_size=0.2)\n",
    "train_ids = DF.iloc[idx_tr][\"sample_id\"].tolist()\n",
    "test_ids  = DF.iloc[idx_te][\"sample_id\"].tolist()\n",
    "\n",
    "print(\"seed:\", used_seed)\n",
    "print(\"train counts:\", pd.Series(y[idx_tr]).value_counts().sort_index().to_dict())\n",
    "print(\"test  counts:\", pd.Series(y[idx_te]).value_counts().sort_index().to_dict())\n",
    "\n",
    "# Save to your existing split file for downstream reuse\n",
    "splits = {\"magcls\": {\"train_ids\": train_ids, \"test_ids\": test_ids}}\n",
    "with open(\"runs/frozen_splits.json\",\"w\") as f:\n",
    "    json.dump(splits, f, indent=2)\n",
    "print(\"[OK] saved runs/frozen_splits.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57e65e",
   "metadata": {},
   "source": [
    "## Baseline Model ‚Äî XGBoost (S/M/L)\n",
    "\n",
    "### Code Summary\n",
    "- **Data**: `data/features_from_npz_mag.csv` (positives only: S/M/L), split IDs from `runs/frozen_splits.json` (`magcls`).\n",
    "- **Features**: Band-energy `[0.5‚Äì2 Hz, 2‚Äì5 Hz, 5‚Äì8 Hz]`.\n",
    "- **Pipeline**: `StandardScaler` ‚Üí `XGBClassifier` (`multi:softprob`, `num_class=3`).\n",
    "- **Class Weights**: computed on train set (balanced) ‚Üí **[S, M, L] = [0.3633, 4.5016, 39.4610]**.\n",
    "- **Key Params**:  \n",
    "  `n_estimators=600`, `max_depth=5`, `learning_rate=0.035`,  \n",
    "  `subsample=0.9`, `colsample_bytree=0.9`, `reg_lambda=1.0`.\n",
    "\n",
    "### Baseline Results (Test Set, N = 1392)\n",
    "\n",
    "- **Accuracy**: **0.8096**  \n",
    "- **Macro F1**: **0.4092**  \n",
    "- **Weighted F1**: **0.8397**\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|:-----:|:---------:|:------:|:--------:|:-------:|\n",
    "| **S** | 0.9450 | 0.8481 | 0.8939 | 1277 |\n",
    "| **M** | 0.1853 | 0.4175 | 0.2567 | 103 |\n",
    "| **L** | 0.0714 | 0.0833 | 0.0769 | 12 |\n",
    "\n",
    "**Confusion Matrix** (`rows = true, cols = predicted`):\n",
    "[[1083 184 10]\n",
    "[ 57 43 3]\n",
    "[ 6 5 1]]\n",
    "\n",
    "## Summary and Interpretation\n",
    "\n",
    "- Performance is strong on **S (small events)**, reflecting their dominance in the dataset.  \n",
    "- **M (moderate events)** gain some recall from class weighting, but precision remains low, indicating frequent false positives.  \n",
    "- **L (large events)** remain extremely underrepresented; the model struggles to classify them reliably.  \n",
    "\n",
    "### Conclusion\n",
    "This XGBoost baseline shows that simple band-energy features, even with class weighting, are insufficient for reliable recognition of moderate and large earthquakes under class imbalance.  \n",
    "It establishes a **fair reference point** for evaluating improvements achieved by deep learning models (CNN+BiLSTM+Attention) on the exact same dataset and splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93534f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights [S,M,L]: [ 0.36330395  4.50161812 39.46099291]\n",
      "== XGBoost (S/M/L, stratified) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9450    0.8481    0.8939      1277\n",
      "           M     0.1853    0.4175    0.2567       103\n",
      "           L     0.0714    0.0833    0.0769        12\n",
      "\n",
      "    accuracy                         0.8096      1392\n",
      "   macro avg     0.4006    0.4496    0.4092      1392\n",
      "weighted avg     0.8813    0.8096    0.8397      1392\n",
      "\n",
      "Confusion matrix:\n",
      " [[1083  184   10]\n",
      " [  57   43    3]\n",
      " [   6    5    1]]\n"
     ]
    }
   ],
   "source": [
    "import json, pandas as pd, numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "splits = json.load(open(\"runs/frozen_splits.json\"))\n",
    "train_ids = set(splits[\"magcls\"][\"train_ids\"])\n",
    "test_ids  = set(splits[\"magcls\"][\"test_ids\"])\n",
    "\n",
    "df = pd.read_csv(\"data/features_from_npz_mag.csv\", parse_dates=[\"window_start\"])\n",
    "df[\"y\"] = df[\"mag_cls\"].map({\"S\":0,\"M\":1,\"L\":2}).astype(int)\n",
    "\n",
    "Xcols = [\"energy_0.5_2Hz\",\"energy_2_5Hz\",\"energy_5_8Hz\"]\n",
    "X_tr = df[df[\"sample_id\"].isin(train_ids)][Xcols].values\n",
    "y_tr = df[df[\"sample_id\"].isin(train_ids)][\"y\"].values\n",
    "X_te = df[df[\"sample_id\"].isin(test_ids)][Xcols].values\n",
    "y_te = df[df[\"sample_id\"].isin(test_ids)][\"y\"].values\n",
    "\n",
    "# class weights (balanced)\n",
    "classes = np.array([0,1,2])\n",
    "cls_w = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "cw_map = {c:w for c,w in zip(classes, cls_w)}\n",
    "sw_tr = np.array([cw_map[int(t)] for t in y_tr])\n",
    "print(\"class weights [S,M,L]:\", cls_w)\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        objective=\"multi:softprob\", num_class=3, eval_metric=\"mlogloss\",\n",
    "        n_estimators=600, max_depth=5, learning_rate=0.035,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf.fit(X_tr, y_tr, xgb__sample_weight=sw_tr)\n",
    "y_hat = clf.predict(X_te)\n",
    "\n",
    "print(\"== XGBoost (S/M/L, stratified) ==\")\n",
    "print(classification_report(\n",
    "    y_te, y_hat, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0\n",
    "))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_te, y_hat, labels=[0,1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fd54e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "class weights [S,M,L] = [ 0.36330396  4.501618   39.460995  ]\n",
      "log-prior init: [-0.086 -2.603 -4.774]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train 0.1500 | val 0.1270 | ~284.0s/epoch | lr 1.00e-03\n",
      "[saved] best checkpoint -> runs/enhanced_cnn_bilstm_attn/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 02 | train 0.1220 | val 0.1411 | ~335.7s/epoch | lr 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 03 | train 0.1198 | val 0.1145 | ~278.7s/epoch | lr 1.00e-03\n",
      "[saved] best checkpoint -> runs/enhanced_cnn_bilstm_attn/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 04 | train 0.1137 | val 0.1106 | ~292.9s/epoch | lr 1.00e-03\n",
      "[saved] best checkpoint -> runs/enhanced_cnn_bilstm_attn/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 05 | train 0.1105 | val 0.1223 | ~315.6s/epoch | lr 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 06 | train 0.1083 | val 0.1073 | ~335.4s/epoch | lr 1.00e-03\n",
      "[saved] best checkpoint -> runs/enhanced_cnn_bilstm_attn/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 07 | train 0.1094 | val 0.1166 | ~281.2s/epoch | lr 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 08 | train 0.1057 | val 0.1126 | ~301.2s/epoch | lr 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 09 | train 0.1077 | val 0.1045 | ~277.8s/epoch | lr 1.00e-03\n",
      "[saved] best checkpoint -> runs/enhanced_cnn_bilstm_attn/best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | train 0.1033 | val 0.1079 | ~300.0s/epoch | lr 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 | train 0.1036 | val 0.1077 | ~266.4s/epoch | lr 1.00e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | train 0.1019 | val 0.1123 | ~267.2s/epoch | lr 5.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Enhanced CNN+BiLSTM+Attn (S/M/L) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9368    0.9859    0.9607      1277\n",
      "           M     0.5833    0.2718    0.3709       103\n",
      "           L     0.0000    0.0000    0.0000        12\n",
      "\n",
      "    accuracy                         0.9246      1392\n",
      "   macro avg     0.5067    0.4192    0.4439      1392\n",
      "weighted avg     0.9025    0.9246    0.9088      1392\n",
      "\n",
      "Confusion matrix:\n",
      "[[1259   18    0]\n",
      " [  75   28    0]\n",
      " [  10    2    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced CNN + BiLSTM + Attention for seismic magnitude classification\n",
    "Comparable with the XGBoost baseline (English-commented + stability fixes)\n",
    "\n",
    "Key principles:\n",
    "1) Use the same frozen_splits.json for data splits\n",
    "2) Keep the same evaluation metrics and reporting\n",
    "3) Only improve the CNN model internals and training strategy\n",
    "4) Keep the same class-weight computation\n",
    "\n",
    "Stability fixes (important):\n",
    "- DO NOT stack class weights + label smoothing with Focal (use Focal alone, gamma=1.5)\n",
    "- Initialize the final classifier bias with log-priors (class frequencies from training set)\n",
    "- Slightly milder augmentation (keeps comparability but stabilizes early epochs)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========================= CONFIG (kept comparable) ========================= #\n",
    "@dataclass\n",
    "class Config:\n",
    "    npz_path: str = \"data/wave_mag_dataset.npz\"\n",
    "    frozen_split_path: str = \"runs/frozen_splits.json\"\n",
    "    use_frozen_split: bool = True\n",
    "\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 12\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "\n",
    "    # Key improvements\n",
    "    use_focal_loss: bool = True            # Focal for imbalance (alone, no class weights here)\n",
    "    focal_gamma: float = 1.5               # was 2.0\n",
    "    use_label_smoothing: bool = True       # only used in CE branch\n",
    "    label_smoothing: float = 0.1\n",
    "\n",
    "    # Lightweight data augmentation (keeps comparability; slightly milder)\n",
    "    augment_prob: float = 0.25             # was 0.30\n",
    "    noise_std: float = 0.008               # was 0.01\n",
    "\n",
    "    # Other settings\n",
    "    num_workers: int = 0\n",
    "    seed: int = 42\n",
    "    patience: int = 3\n",
    "    grad_clip: float = 1.0\n",
    "    min_L_in_val: int = 8\n",
    "\n",
    "    amp: bool = True\n",
    "    save_dir: str = \"runs/enhanced_cnn_bilstm_attn\"\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# ========================= DATASET (lightweight augmentation) ========================= #\n",
    "class EnhancedWaveformDataset(Dataset):\n",
    "    \"\"\"Waveform dataset with lightweight augmentation; same preprocessing as original.\"\"\"\n",
    "\n",
    "    def __init__(self, waves: np.ndarray, labels: np.ndarray,\n",
    "                 per_sample_norm: bool = True, augment: bool = False,\n",
    "                 augment_prob: float = 0.25, noise_std: float = 0.008):\n",
    "        # Ensure float32 upfront to avoid dtype mismatches later\n",
    "        self.waves = waves.astype(np.float32, copy=False)\n",
    "        self.labels = labels.astype(np.int64, copy=False)\n",
    "        self.per_sample_norm = per_sample_norm\n",
    "        self.augment = augment\n",
    "        self.augment_prob = float(augment_prob)\n",
    "        self.noise_std = float(noise_std)\n",
    "\n",
    "        # Input shape handling\n",
    "        if self.waves.ndim == 2:\n",
    "            self.waves = self.waves[:, None, :]\n",
    "        elif self.waves.ndim != 3:\n",
    "            raise ValueError(f\"Unexpected waveform shape {self.waves.shape}; expected (N,L) or (N,C,L)\")\n",
    "\n",
    "        # If raw int16, scale to [-1, 1]\n",
    "        if self.waves.dtype == np.int16:\n",
    "            self.waves = (self.waves.astype(np.float32) / 32768.0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.waves.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.waves[idx]  # (C, L), float32\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        # Single-draw augmentation: class L gets higher probability\n",
    "        if self.augment:\n",
    "            p = 0.6 if int(y) == 2 else self.augment_prob\n",
    "            if np.random.random() < p:\n",
    "                noise = np.random.normal(0.0, self.noise_std, x.shape).astype(x.dtype, copy=False)\n",
    "                x = x + noise  # remains float32\n",
    "\n",
    "        # Per-sample normalization across time dimension\n",
    "        if self.per_sample_norm:\n",
    "            mean = x.mean(axis=-1, keepdims=True)\n",
    "            std = x.std(axis=-1, keepdims=True) + 1e-6\n",
    "            x = (x - mean) / std\n",
    "\n",
    "        # Safety: ensure float32 before tensor conversion\n",
    "        x = x.astype(np.float32, copy=False)\n",
    "        return torch.from_numpy(x), torch.tensor(y)\n",
    "\n",
    "# ========================= FOCAL LOSS (no stacking with weights/smoothing) ========================= #\n",
    "class FocalLossWithLabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal wrapper around CE. In focal mode we DO NOT stack class weights or label smoothing,\n",
    "    because stacking them can massively over-amplify minority-class gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, gamma=1.5, smoothing=0.0, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "        self.class_weights = class_weights  # should be None in focal mode\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.smoothing > 0:\n",
    "            # (Not used in our focal config, but kept for completeness)\n",
    "            n_classes = inputs.size(-1)\n",
    "            smooth_targets = torch.zeros_like(inputs)\n",
    "            smooth_targets.fill_(self.smoothing / (n_classes - 1))\n",
    "            smooth_targets.scatter_(1, targets.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            log_probs = F.log_softmax(inputs, dim=-1)\n",
    "            loss = -torch.sum(smooth_targets * log_probs, dim=-1)\n",
    "            if self.class_weights is not None:\n",
    "                weight = self.class_weights[targets]\n",
    "                loss = loss * weight\n",
    "        else:\n",
    "            # Plain per-sample CE without weights; focal will handle the focusing\n",
    "            logp = F.log_softmax(inputs, dim=-1)\n",
    "            ce = -logp.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "            loss = ce\n",
    "\n",
    "        pt = torch.exp(-loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# ========================= MODEL: CNN + BiLSTM + Attention ========================= #\n",
    "class ImprovedConvBlock(nn.Module):\n",
    "    \"\"\"Conv block with residual connection, BN, ReLU, optional MaxPool, and dropout.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, k=7, p=None, s=1, pool=4, dropout=0.15):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = k // 2\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p)\n",
    "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool) if pool else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Residual shortcut projection when channels differ\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_ch, out_ch, kernel_size=1, stride=s),\n",
    "                nn.BatchNorm1d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Residual connection\n",
    "        identity = self.shortcut(identity)\n",
    "        if identity.shape[-1] != out.shape[-1]:\n",
    "            identity = F.adaptive_avg_pool1d(identity, out.shape[-1])\n",
    "\n",
    "        out += identity\n",
    "        out = self.act(out)\n",
    "        out = self.pool(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Minimal multi-head self-attention encoder; keep global mean pooling for comparability.\"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, H):  # H: (B, T, D)\n",
    "        B, T, D = H.shape\n",
    "\n",
    "        Q = self.q_proj(H).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(H).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(H).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B,H,T,T)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, V)                               # (B,H,T,dh)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, D)     # (B,T,D)\n",
    "\n",
    "        output = self.out_proj(attn_output)                                       # (B,T,D)\n",
    "\n",
    "        # Keep global mean pooling to remain comparable with prior setup\n",
    "        context = output.mean(dim=1)                                              # (B,D)\n",
    "        global_attn_weights = attn_weights.mean(dim=1).mean(dim=1)               # (B,T)\n",
    "\n",
    "        return context, global_attn_weights\n",
    "\n",
    "class EnhancedCNNBiLSTMAttn(nn.Module):\n",
    "    \"\"\"CNN (residual blocks) + BiLSTM + attention encoder + MLP head.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int = 1, n_classes: int = 3,\n",
    "                 lstm_hidden: int = 128, lstm_layers: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN front-end\n",
    "        self.block1 = ImprovedConvBlock(in_ch, 64, k=9, pool=4)\n",
    "        self.block2 = ImprovedConvBlock(64, 128, k=7, pool=4)\n",
    "        self.block3 = ImprovedConvBlock(128, 256, k=5, pool=4)\n",
    "\n",
    "        # BiLSTM encoder\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=lstm_hidden,\n",
    "                            num_layers=lstm_layers, batch_first=True,\n",
    "                            bidirectional=True, dropout=0.2)\n",
    "\n",
    "        feat_dim = lstm_hidden * 2\n",
    "\n",
    "        # Multi-head attention encoder\n",
    "        self.attention = MultiHeadAttention(feat_dim, num_heads=4)\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B, C, L)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)          # (B,256,L')\n",
    "        x = x.transpose(1, 2)       # (B,L',256)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)  # (B,L',2*hidden)\n",
    "        context, attn = self.attention(lstm_out)  # (B, 2*hidden)\n",
    "        logits = self.classifier(context)         # (B, n_classes)\n",
    "\n",
    "        return logits, attn\n",
    "\n",
    "# ========================= DATA LOADING (same logic) ========================= #\n",
    "def load_npz_positives(npz_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Load positives exactly as in the original.\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    for k in [\"waveforms\", \"detect_label\", \"mag_class\", \"mag_cls\", \"sample_ids\"]:\n",
    "        if k not in d.files:\n",
    "            raise KeyError(f\"Missing '{k}' in NPZ\")\n",
    "\n",
    "    detect = d[\"detect_label\"].astype(int)\n",
    "    pos_mask = detect == 1\n",
    "\n",
    "    waves = d[\"waveforms\"][pos_mask]\n",
    "    y = d[\"mag_class\"].astype(int)[pos_mask]\n",
    "    sample_ids = d[\"sample_ids\"][pos_mask]\n",
    "\n",
    "    uniq = np.unique(y)\n",
    "    if not set(uniq).issubset({0, 1, 2}):\n",
    "        raise ValueError(f\"mag_class should be in {{0,1,2}} for positives; got {uniq}\")\n",
    "\n",
    "    return waves, y, sample_ids\n",
    "\n",
    "def build_stratified_split(y: np.ndarray, test_size: float = 0.2, min_L: int = 8):\n",
    "    \"\"\"Stratified split, ensure at least min_L class-2 (L) samples in validation.\"\"\"\n",
    "    for seed in range(1000):\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "        idx_tr, idx_va = next(sss.split(np.zeros_like(y), y))\n",
    "        if (y[idx_va] == 2).sum() >= min_L:\n",
    "            return idx_tr, idx_va\n",
    "    raise RuntimeError(\"Failed to ensure enough L samples in validation. Try lowering min_L.\")\n",
    "\n",
    "def get_train_val_loaders(npz_path: str, cfg: Config):\n",
    "    \"\"\"Keep the same split logic; only improve the Dataset (augmentation).\"\"\"\n",
    "    waves, y, sids = load_npz_positives(npz_path)\n",
    "\n",
    "    # Unify sample_ids to str before matching (robust against bytes/int)\n",
    "    sids = np.array([str(s) for s in sids], dtype=object)\n",
    "\n",
    "    if cfg.use_frozen_split and os.path.exists(cfg.frozen_split_path):\n",
    "        try:\n",
    "            with open(cfg.frozen_split_path, \"r\") as f:\n",
    "                splits = json.load(f)\n",
    "            train_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "            val_ids   = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "            mask_tr = np.array([sid in train_ids for sid in sids])\n",
    "            mask_va = np.array([sid in val_ids for sid in sids])\n",
    "            idx_tr = np.where(mask_tr)[0]\n",
    "            idx_va = np.where(mask_va)[0]\n",
    "            if (y[idx_va] == 2).sum() < cfg.min_L_in_val:\n",
    "                print(\"[WARN] Frozen split val has insufficient L; falling back to local stratified split.\")\n",
    "                idx_tr, idx_va = build_stratified_split(y, test_size=0.2, min_L=cfg.min_L_in_val)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to use frozen split:\", e)\n",
    "            idx_tr, idx_va = build_stratified_split(y, test_size=0.2, min_L=cfg.min_L_in_val)\n",
    "    else:\n",
    "        idx_tr, idx_va = build_stratified_split(y, test_size=0.2, min_L=cfg.min_L_in_val)\n",
    "\n",
    "    X_tr, y_tr = waves[idx_tr], y[idx_tr]\n",
    "    X_va, y_va = waves[idx_va], y[idx_va]\n",
    "\n",
    "    # Augment only on training set\n",
    "    ds_tr = EnhancedWaveformDataset(X_tr, y_tr, per_sample_norm=True, augment=True,\n",
    "                                    augment_prob=cfg.augment_prob, noise_std=cfg.noise_std)\n",
    "    ds_va = EnhancedWaveformDataset(X_va, y_va, per_sample_norm=True, augment=False)\n",
    "\n",
    "    # Optional: set a generator + worker seed to improve reproducibility\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(cfg.seed)\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True,\n",
    "                       num_workers=cfg.num_workers, pin_memory=True,\n",
    "                       generator=g,\n",
    "                       worker_init_fn=(lambda _: np.random.seed(cfg.seed)))\n",
    "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False,\n",
    "                       num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "    return dl_tr, dl_va, y_tr, y_va\n",
    "\n",
    "# ========================= TRAIN / EVAL (same reporting) ========================= #\n",
    "def compute_class_weights(y_tr: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Compute class weights as in the XGBoost baseline (balanced frequency).\"\"\"\n",
    "    classes = np.array([0, 1, 2])\n",
    "    weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set seeds for reproducibility (PyTorch + NumPy + Python).\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def train_epoch(model, dl, optimizer, scaler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    for xb, yb in tqdm(dl, desc=\"train\", leave=False):\n",
    "        xb = xb.to(device, non_blocking=True).float()\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, _ = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits, _ = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CFG.grad_clip)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "def eval_epoch(model, dl, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    all_logits = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(dl, desc=\"valid\", leave=False):\n",
    "            xb = xb.to(device, non_blocking=True).float()\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "            logits, _ = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_targets.append(yb.cpu())\n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    targets = torch.cat(all_targets, dim=0)\n",
    "    preds = logits.argmax(dim=1).numpy()\n",
    "    y_true = targets.numpy()\n",
    "    return total_loss / max(1, n), preds, y_true\n",
    "\n",
    "def main(cfg: Config):\n",
    "    set_seed(cfg.seed)\n",
    "    ensure_dir(cfg.save_dir)\n",
    "\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else (\n",
    "            \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        )\n",
    "    )\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler() if (cfg.amp and device.type == \"cuda\") else None\n",
    "\n",
    "    dl_tr, dl_va, y_tr, y_va = get_train_val_loaders(cfg.npz_path, cfg)\n",
    "\n",
    "    # Build model\n",
    "    sample_batch = next(iter(dl_tr))[0]\n",
    "    in_ch = sample_batch.shape[1]\n",
    "    model = EnhancedCNNBiLSTMAttn(in_ch=in_ch, n_classes=3).to(device)\n",
    "\n",
    "    # Class weights computed as baseline reference (not used in focal)\n",
    "    class_w = compute_class_weights(y_tr).to(device)\n",
    "    print(\"class weights [S,M,L] =\", class_w.detach().cpu().numpy())\n",
    "\n",
    "    # ---- Initialize classifier bias with log-priors (stabilizes early predictions) ----\n",
    "    counts = np.bincount(y_tr, minlength=3).astype(np.float32)\n",
    "    priors = counts / counts.sum()\n",
    "    with torch.no_grad():\n",
    "        model.classifier[-1].bias.copy_(torch.log(torch.tensor(priors, device=device)))\n",
    "    print(\"log-prior init:\", np.round(np.log(priors + 1e-12), 3))\n",
    "\n",
    "    # ==== Loss ====\n",
    "    if cfg.use_focal_loss:\n",
    "        # Focal alone: no class weights or label smoothing here\n",
    "        criterion = FocalLossWithLabelSmoothing(\n",
    "            gamma=cfg.focal_gamma,\n",
    "            smoothing=0.0,\n",
    "            class_weights=None\n",
    "        )\n",
    "    else:\n",
    "        # Plain CE branch: here we DO use class weights + mild label smoothing\n",
    "        criterion = nn.CrossEntropyLoss(\n",
    "            weight=class_w,\n",
    "            label_smoothing=cfg.label_smoothing if cfg.use_label_smoothing else 0.0\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    best_val = math.inf\n",
    "    best_path = os.path.join(cfg.save_dir, \"best.pt\")\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        t0 = time.time()\n",
    "        tr_loss = train_epoch(model, dl_tr, optimizer, scaler, criterion, device)\n",
    "        va_loss, preds, y_true = eval_epoch(model, dl_va, criterion, device)\n",
    "        scheduler.step(va_loss)\n",
    "        secs = time.time() - t0\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "\n",
    "        print(f\"epoch {epoch:02d} | train {tr_loss:.4f} | val {va_loss:.4f} | ~{secs:.1f}s/epoch | lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if va_loss + 1e-6 < best_val:\n",
    "            best_val = va_loss\n",
    "            no_improve = 0\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"config\": cfg.__dict__,\n",
    "            }, best_path)\n",
    "            print(\"[saved] best checkpoint ->\", best_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve > cfg.patience:\n",
    "                print(\"[early stop] no improvement\")\n",
    "                break\n",
    "\n",
    "    # Load best and produce final validation report\n",
    "    ckpt = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    _, preds, y_true = eval_epoch(model, dl_va, criterion, device)\n",
    "\n",
    "    report = classification_report(y_true, preds, labels=[0, 1, 2],\n",
    "                                   target_names=[\"S\", \"M\", \"L\"], digits=4, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, preds, labels=[0, 1, 2])\n",
    "\n",
    "    print(\"== Enhanced CNN+BiLSTM+Attn (S/M/L) ==\")\n",
    "    print(report)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    with open(os.path.join(cfg.save_dir, \"metrics.txt\"), \"w\") as f:\n",
    "        f.write(report + \"\\n\")\n",
    "        f.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "    with open(os.path.join(cfg.save_dir, \"history.json\"), \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9eaf1",
   "metadata": {},
   "source": [
    "## Main Model ‚Äî CNN + BiLSTM + Attention (S/M/L)\n",
    "\n",
    "### Code Summary\n",
    "- **Data & Split**: Same dataset (`wave_mag_dataset.npz`) and frozen splits (`runs/frozen_splits.json`) as the XGBoost baseline.  \n",
    "  - Train/Val: 90%/10% stratified split from training IDs.  \n",
    "  - Test: strictly held-out test IDs.  \n",
    "- **Labels**: S ‚Üí 0, M ‚Üí 1, L ‚Üí 2.  \n",
    "- **Normalization**: Global z-score (mean/std computed from TRAIN only).  \n",
    "- **Class Imbalance**: Balanced class weights applied in `CrossEntropyLoss`.  \n",
    "- **Training**:  \n",
    "  - Epochs = 50, early stopping patience = 8.  \n",
    "  - Optimizer: AdamW, LR = 3e-4, weight_decay = 1e-2.  \n",
    "  - Scheduler: ReduceLROnPlateau (factor=0.5).  \n",
    "  - Batch size = 128, grad clipping = 1.0.  \n",
    "\n",
    "**Architecture**\n",
    "- **CNN Frontend**: Three 1D Conv blocks (Conv ‚Üí BN ‚Üí GELU ‚Üí MaxPool).  \n",
    "- **Sequence Encoder**: 2-layer BiLSTM (hidden=96, bidirectional).  \n",
    "- **Attention Layer**: Additive attention pooling over sequence outputs.  \n",
    "- **Classifier Head**: Linear ‚Üí GELU ‚Üí Dropout ‚Üí Linear.  \n",
    "\n",
    "---\n",
    "\n",
    "### Main Model Results (Test Set, N = 1392)\n",
    "\n",
    "- **Accuracy**: **0.9152**  \n",
    "- **Macro F1**: **0.6015**  \n",
    "- **Weighted F1**: **0.9211**\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|:-----:|:---------:|:------:|:--------:|:-------:|\n",
    "| **S** | 0.9733 | 0.9413 | 0.9570 | 1277 |\n",
    "| **M** | 0.4631 | 0.6699 | 0.5476 | 103 |\n",
    "| **L** | 0.3750 | 0.2500 | 0.3000 | 12 |\n",
    "\n",
    "**Confusion Matrix** (`rows = true, cols = predicted`):\n",
    "[[1202 73 2]\n",
    "[ 31 69 3]\n",
    "[ 2 7 3]]\n",
    "### Summary and Interpretation\n",
    "- The **deep model significantly outperforms the XGBoost baseline** on overall accuracy (91.5% vs 80.9%) and especially on **M/L classes**.  \n",
    "- **S (small events)** remain strongest, with near-perfect precision/recall.  \n",
    "- **M (moderate events)** see major improvement (Recall ~0.67, F1 ~0.55 vs baseline F1 ~0.26).  \n",
    "- **L (large events)** performance is still limited by scarce samples, but recall improved to 0.25 (baseline ~0.08).  \n",
    "\n",
    "### Conclusion\n",
    "The CNN+BiLSTM+Attention model demonstrates clear advantages in handling class imbalance and capturing temporal‚Äìspectral features of waveforms, offering a more robust solution for seismic magnitude classification compared to feature-engineered baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2243dc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] num_workers set to 0\n",
      "Epoch 01 | train 0.9910 f1 0.3766 | val 0.8644 f1 0.3919\n",
      "Epoch 02 | train 0.8113 f1 0.3908 | val 0.9060 f1 0.4040\n",
      "Epoch 03 | train 0.7813 f1 0.4390 | val 1.0060 f1 0.4749\n",
      "Epoch 04 | train 0.7642 f1 0.4926 | val 0.9216 f1 0.5606\n",
      "Epoch 05 | train 0.7541 f1 0.4673 | val 0.9229 f1 0.5459\n",
      "Epoch 06 | train 0.6798 f1 0.4936 | val 1.0527 f1 0.5397\n",
      "Epoch 07 | train 0.6934 f1 0.4879 | val 1.0341 f1 0.5097\n",
      "Epoch 08 | train 0.6519 f1 0.5042 | val 1.1542 f1 0.5448\n",
      "Epoch 09 | train 0.6706 f1 0.5204 | val 1.1548 f1 0.5620\n",
      "Epoch 10 | train 0.6679 f1 0.5082 | val 1.0522 f1 0.5271\n",
      "Epoch 11 | train 0.6389 f1 0.5264 | val 1.0551 f1 0.6749\n",
      "Epoch 12 | train 0.7649 f1 0.5190 | val 1.1191 f1 0.5736\n",
      "Epoch 13 | train 0.6897 f1 0.5520 | val 1.0072 f1 0.6441\n",
      "Epoch 14 | train 0.6445 f1 0.5542 | val 1.1064 f1 0.6003\n",
      "Epoch 15 | train 0.6138 f1 0.5385 | val 1.1222 f1 0.5905\n",
      "Epoch 16 | train 0.5975 f1 0.5557 | val 1.0547 f1 0.5914\n",
      "Epoch 17 | train 0.6321 f1 0.5492 | val 1.1614 f1 0.5555\n",
      "Epoch 18 | train 0.6387 f1 0.5587 | val 1.0834 f1 0.5466\n",
      "Epoch 19 | train 0.6409 f1 0.5569 | val 1.0312 f1 0.6361\n",
      "Early stopping.\n",
      "\n",
      "== CNN+BiLSTM+Attn (S/M/L, TEST) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9733    0.9413    0.9570      1277\n",
      "           M     0.4631    0.6699    0.5476       103\n",
      "           L     0.3750    0.2500    0.3000        12\n",
      "\n",
      "    accuracy                         0.9152      1392\n",
      "   macro avg     0.6038    0.6204    0.6015      1392\n",
      "weighted avg     0.9304    0.9152    0.9211      1392\n",
      "\n",
      "Confusion matrix:\n",
      " [[1202   73    2]\n",
      " [  31   69    3]\n",
      " [   2    7    3]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CNN + BiLSTM + Attention (STRICT comparable to XGBoost)\n",
    "=======================================================\n",
    "Only the MODEL differs. Everything else is aligned to the XGBoost baseline:\n",
    "- Sample scope: EXACTLY the same train/test ids from runs/frozen_splits.json AND present in features CSV.\n",
    "- Split usage: train_ids -> train; from train we make a small stratified val (10%). test_ids -> final test ONLY.\n",
    "- Label mapping: S->0, M->1, L->2 (verified against CSV; will error if mismatch).\n",
    "- Normalization: GLOBAL z-score using TRAIN-ONLY mean/std applied to train/val/test waveforms.\n",
    "- Class imbalance: balanced class weights computed from TRAIN labels, used in CrossEntropyLoss.\n",
    "- Evaluation protocol: classification_report + confusion_matrix on TEST ids (same format as XGB).\n",
    "\n",
    "Run:\n",
    "  python cnn_strict_comparable_baseline.py\n",
    "Outputs:\n",
    "  runs/cnn_strict/best.pt\n",
    "  runs/cnn_strict/summary.json  (macro_f1, confusion_matrix, counts)\n",
    "  runs/cnn_strict/preds_test.npz (y_true, y_pred, sample_id)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm.std import tqdm  # force plain-text tqdm, avoids ipywidgets warning\n",
    "\n",
    "# ------------------------------- Config -------------------------------- #\n",
    "@dataclass\n",
    "class Config:\n",
    "    npz_path: str = \"data/wave_mag_dataset.npz\"              # waveforms + labels + sample_id\n",
    "    features_csv: str = \"data/features_from_npz_mag.csv\"     # XGB features, used to verify scope/labels\n",
    "    frozen_split_path: str = \"runs/frozen_splits.json\"        # magcls.train_ids/test_ids\n",
    "    out_dir: str = \"runs/cnn_strict\"\n",
    "\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 128\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    grad_clip: float = 1.0\n",
    "    patience: int = 8                       # early stopping on val macro-F1\n",
    "    num_workers: int = 0\n",
    "\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "LABEL_MAP = {\"S\":0, \"M\":1, \"L\":2}\n",
    "INV_LABEL_MAP = {v:k for k,v in LABEL_MAP.items()}\n",
    "\n",
    "# --------------------------- Utilities & Data --------------------------- #\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class WaveDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, sid: np.ndarray, mean: float, std: float):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.sid = sid\n",
    "        self.mean = float(mean); self.std = float(std) if std>0 else 1.0\n",
    "\n",
    "        # shape to [N,1,L]\n",
    "        if self.X.ndim == 2:\n",
    "            self.X = self.X[:, None, :]\n",
    "        elif self.X.ndim != 3:\n",
    "            raise ValueError(f\"Unexpected waveform shape {self.X.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = (self.X[i] - self.mean) / self.std\n",
    "        return torch.from_numpy(x), torch.tensor(self.y[i]), self.sid[i]\n",
    "\n",
    "\n",
    "def load_everything(cfg: Config) -> Dict[str, Any]:\n",
    "    assert os.path.exists(cfg.npz_path), f\"Missing {cfg.npz_path}\"\n",
    "    assert os.path.exists(cfg.features_csv), f\"Missing {cfg.features_csv}\"\n",
    "    assert os.path.exists(cfg.frozen_split_path), f\"Missing {cfg.frozen_split_path}\"\n",
    "\n",
    "    # 1) load splits\n",
    "    with open(cfg.frozen_split_path, 'r') as f:\n",
    "        splits = json.load(f)\n",
    "    train_ids = set(map(str, splits['magcls']['train_ids']))\n",
    "    test_ids  = set(map(str, splits['magcls']['test_ids']))\n",
    "\n",
    "    # 2) load features CSV (defines XGB scope + labels)\n",
    "    df_feat = pd.read_csv(cfg.features_csv)\n",
    "    assert 'sample_id' in df_feat.columns and 'mag_cls' in df_feat.columns\n",
    "    df_feat['y'] = df_feat['mag_cls'].map(LABEL_MAP).astype(int)\n",
    "    df_feat['sample_id'] = df_feat['sample_id'].astype(str)\n",
    "\n",
    "    # restrict to the declared ids (scope reference = split ‚à© csv)\n",
    "    tr_ids_csv = set(df_feat[df_feat['sample_id'].isin(train_ids)]['sample_id'])\n",
    "    te_ids_csv = set(df_feat[df_feat['sample_id'].isin(test_ids)]['sample_id'])\n",
    "\n",
    "    # 3) load NPZ (waveforms/labels/sample_id)\n",
    "    npz = np.load(cfg.npz_path, allow_pickle=True)\n",
    "    # prefer 'sample_id' key, allow variants\n",
    "    sid = None\n",
    "    for k in ['sample_id','sample_ids','ids','sid']:\n",
    "        if k in npz: sid = np.array([str(s) for s in npz[k]]); break\n",
    "    assert sid is not None, \"NPZ must contain sample_id(s)\"\n",
    "\n",
    "    if 'mag_class' in npz:\n",
    "        y = npz['mag_class'].astype(int)\n",
    "    elif 'mag_cls' in npz:\n",
    "        v = npz['mag_cls']\n",
    "        y = np.array([LABEL_MAP[str(t)] if str(t) in LABEL_MAP else int(t) for t in v])\n",
    "    elif 'labels' in npz:\n",
    "        y = npz['labels'].astype(int)\n",
    "    else:\n",
    "        raise KeyError(\"NPZ needs one of mag_class/mag_cls/labels\")\n",
    "\n",
    "    X = npz['waveforms'] if 'waveforms' in npz else npz['X']\n",
    "\n",
    "    # if detect_label exists, keep positives only (classification dataset)\n",
    "    if 'detect_label' in npz:\n",
    "        pos = (npz['detect_label'].astype(int) == 1)\n",
    "        X, y, sid = X[pos], y[pos], sid[pos]\n",
    "\n",
    "    # 4) align scope exactly to CSV ids per split\n",
    "    m_tr = np.isin(sid, list(tr_ids_csv))\n",
    "    m_te = np.isin(sid, list(te_ids_csv))\n",
    "\n",
    "    X_tr_all, y_tr_all, sid_tr_all = X[m_tr], y[m_tr], sid[m_tr]\n",
    "    X_te,     y_te,     sid_te     = X[m_te], y[m_te], sid[m_te]\n",
    "\n",
    "    # 5) label consistency check with CSV\n",
    "    y_csv_tr = df_feat.set_index('sample_id').loc[list(sid_tr_all), 'y'].to_numpy()\n",
    "    y_csv_te = df_feat.set_index('sample_id').loc[list(sid_te),     'y'].to_numpy()\n",
    "    assert (y_csv_tr == y_tr_all).all(), \"Label mismatch found in TRAIN between CSV and NPZ\"\n",
    "    assert (y_csv_te == y_te).all(),     \"Label mismatch found in TEST between CSV and NPZ\"\n",
    "\n",
    "    # 6) build val from TRAIN (10%, stratified, stable)\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx_all = np.arange(len(y_tr_all))\n",
    "    idx_val_mask = np.zeros_like(idx_all, dtype=bool)\n",
    "    for c in [0,1,2]:\n",
    "        idx_c = idx_all[y_tr_all == c]\n",
    "        n_val = max(1, int(0.1 * len(idx_c)))\n",
    "        if len(idx_c) > 0:\n",
    "            take = rng.choice(idx_c, size=n_val, replace=False)\n",
    "            idx_val_mask[np.isin(idx_all, take)] = True\n",
    "    idx_va = idx_all[idx_val_mask]\n",
    "    idx_tr = idx_all[~idx_val_mask]\n",
    "\n",
    "    # 7) train-only stats for GLOBAL standardization\n",
    "    X_stats = X_tr_all if X_tr_all.ndim==2 else X_tr_all.reshape(len(X_tr_all), -1)\n",
    "    mean = float(X_stats.mean()); std = float(X_stats.std() + 1e-8)\n",
    "\n",
    "    return dict(\n",
    "        X_tr=X_tr_all[idx_tr], y_tr=y_tr_all[idx_tr], sid_tr=sid_tr_all[idx_tr],\n",
    "        X_va=X_tr_all[idx_va], y_va=y_tr_all[idx_va], sid_va=sid_tr_all[idx_va],\n",
    "        X_te=X_te, y_te=y_te, sid_te=sid_te,\n",
    "        mean=mean, std=std\n",
    "    )\n",
    "\n",
    "# ------------------------------- Model --------------------------------- #\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k//2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x); x = self.bn(x); x = F.gelu(x); x = self.pool(x); return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__(); self.W = nn.Linear(d,d); self.v = nn.Linear(d,1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U = torch.tanh(self.W(H)); a = self.v(U).squeeze(-1); a = torch.softmax(a, dim=1)\n",
    "        Z = torch.bmm(a.unsqueeze(1), H).squeeze(1); return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            ConvBlock(in_ch, 32),\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden, num_layers=layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2*hidden)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2*hidden, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)            # [B,C,L]\n",
    "        z = z.transpose(1,2)       # [B,L,C]\n",
    "        H,_ = self.lstm(z)         # [B,L,2H]\n",
    "        Z,_ = self.attn(H)         # [B,2H]\n",
    "        return self.head(Z)\n",
    "\n",
    "# ----------------------------- Train & Eval ----------------------------- #\n",
    "\n",
    "def compute_balanced_weights(y: np.ndarray, n_classes:int=3) -> torch.Tensor:\n",
    "    w = compute_class_weight(class_weight='balanced', classes=np.arange(n_classes), y=y)\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_epoch(model, dl, crit, opt, device, grad_clip=1.0):\n",
    "    model.train(); tot=0.0; preds=[]; trues=[]\n",
    "    for x,y,_ in dl:\n",
    "        x=x.to(device); y=y.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = crit(logits, y)\n",
    "        loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        opt.step()\n",
    "        tot += loss.item()*x.size(0)\n",
    "        preds.append(logits.argmax(1).detach().cpu().numpy()); trues.append(y.cpu().numpy())\n",
    "    p = np.concatenate(preds); t = np.concatenate(trues)\n",
    "    f1 = f1_score(t,p,average='macro')\n",
    "    return tot/len(dl.dataset), f1\n",
    "\n",
    "\n",
    "def eval_epoch(model, dl, crit, device):\n",
    "    model.eval(); tot=0.0; logits_all=[]; trues=[]; sids=[]\n",
    "    with torch.no_grad():\n",
    "        for x,y,sid in dl:\n",
    "            x=x.to(device); y=y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = crit(logits, y)\n",
    "            tot += loss.item()*x.size(0)\n",
    "            logits_all.append(logits.cpu().numpy()); trues.append(y.cpu().numpy()); sids.append(np.array(sid))\n",
    "    L = np.concatenate(logits_all); T = np.concatenate(trues); S = np.concatenate(sids)\n",
    "    P = L.argmax(1); f1 = f1_score(T,P,average='macro')\n",
    "    return tot/len(dl.dataset), f1, L, T, S\n",
    "\n",
    "# --------------------------------- Main -------------------------------- #\n",
    "\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    cfg = Config(); os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "    # --- Optional override for num_workers via CLI or ENV ---\n",
    "    try:\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(add_help=False)\n",
    "        parser.add_argument(\"--num-workers\", type=int, dest=\"num_workers\")\n",
    "        args, _ = parser.parse_known_args()\n",
    "        env_nw = os.environ.get(\"NUM_WORKERS\")\n",
    "        if args.num_workers is not None:\n",
    "            cfg.num_workers = int(args.num_workers)\n",
    "        elif env_nw is not None:\n",
    "            cfg.num_workers = int(env_nw)\n",
    "        print(f\"[Info] num_workers set to {cfg.num_workers}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] num_workers override failed: {e}\")\n",
    "\n",
    "    bundle = load_everything(cfg)\n",
    "    X_tr, y_tr, sid_tr = bundle['X_tr'], bundle['y_tr'], bundle['sid_tr']\n",
    "    X_va, y_va, sid_va = bundle['X_va'], bundle['y_va'], bundle['sid_va']\n",
    "    X_te, y_te, sid_te = bundle['X_te'], bundle['y_te'], bundle['sid_te']\n",
    "    mean, std = bundle['mean'], bundle['std']\n",
    "\n",
    "    ds_tr = WaveDataset(X_tr, y_tr, sid_tr, mean, std)\n",
    "    ds_va = WaveDataset(X_va, y_va, sid_va, mean, std)\n",
    "    ds_te = WaveDataset(X_te, y_te, sid_te, mean, std)\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=(cfg.device=='cuda'))\n",
    "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=(cfg.device=='cuda'))\n",
    "    dl_te = DataLoader(ds_te, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=(cfg.device=='cuda'))\n",
    "\n",
    "    in_ch = 1 if (X_tr.ndim==2 or X_tr.shape[1]==1) else X_tr.shape[1]\n",
    "    model = CNNBiLSTMAttn(in_ch=in_ch).to(cfg.device)\n",
    "\n",
    "    class_w = compute_balanced_weights(y_tr).to(cfg.device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_w)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "    best_f1=-1.0; best_state=None; patience=cfg.patience\n",
    "    for ep in range(1, cfg.epochs+1):\n",
    "        tr_loss,tr_f1 = train_epoch(model, dl_tr, criterion, optimizer, cfg.device, cfg.grad_clip)\n",
    "        va_loss,va_f1,_,_,_ = eval_epoch(model, dl_va, criterion, cfg.device)\n",
    "        scheduler.step(va_f1)\n",
    "        print(f\"Epoch {ep:02d} | train {tr_loss:.4f} f1 {tr_f1:.4f} | val {va_loss:.4f} f1 {va_f1:.4f}\")\n",
    "        if va_f1>best_f1:\n",
    "            best_f1=va_f1; patience=cfg.patience\n",
    "            best_state={k:(v.detach().cpu() if isinstance(v,torch.Tensor) else v) for k,v in model.state_dict().items()}\n",
    "            torch.save(best_state, os.path.join(cfg.out_dir,'best.pt'))\n",
    "        else:\n",
    "            patience-=1\n",
    "            if patience<=0:\n",
    "                print('Early stopping.'); break\n",
    "\n",
    "    if best_state is None:\n",
    "        best_state = torch.load(os.path.join(cfg.out_dir,'best.pt'), map_location='cpu')\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    te_loss, te_f1, L_te, y_true, sid_out = eval_epoch(model, dl_te, criterion, cfg.device)\n",
    "    y_pred = L_te.argmax(1)\n",
    "\n",
    "    print(\"\\n== CNN+BiLSTM+Attn (S/M/L, TEST) ==\")\n",
    "    print(classification_report(y_true, y_pred, labels=[0,1,2], target_names=['S','M','L'], digits=4, zero_division=0))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "    np.savez(os.path.join(cfg.out_dir,'preds_test.npz'), y_true=y_true, y_pred=y_pred, sample_id=sid_out)\n",
    "    rep = {\n",
    "        'macro_f1': float(f1_score(y_true, y_pred, average='macro')),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'counts_test': [int((y_true==i).sum()) for i in range(3)],\n",
    "        'class_weights': [float(x) for x in class_w.detach().cpu().numpy()],\n",
    "    }\n",
    "    with open(os.path.join(cfg.out_dir,'summary.json'),'w') as f:\n",
    "        json.dump(rep, f, indent=2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3617a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures to: /Users/donghui/figs_final\n"
     ]
    }
   ],
   "source": [
    "# ====== Publication-ready figures (no \"Test\"/\"text\" in titles) ======\n",
    "# Save to: ./figs_final/*.png\n",
    "# Requirements: matplotlib, numpy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patheffects\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1) Fill in your numbers (youÂèØ‰ª•Êîπ) ----------\n",
    "classes = [\"S\",\"M\",\"L\"]\n",
    "\n",
    "# XGBoost (baseline)\n",
    "xgb_overall = {\"macro_f1\": 0.4092, \"accuracy\": 0.8096}\n",
    "xgb_f1 = [0.8939, 0.2567, 0.0769]\n",
    "xgb_cm = np.array([[1083,184,10],\n",
    "                   [  57,  43, 3],\n",
    "                   [   6,   5, 1]])\n",
    "\n",
    "# CNN (proposed)\n",
    "cnn_overall = {\"macro_f1\": 0.6015, \"accuracy\": 0.9152}\n",
    "cnn_f1 = [0.9570, 0.5476, 0.3000]\n",
    "cnn_cm = np.array([[1202, 73, 2],\n",
    "                   [  31, 69, 3],\n",
    "                   [   2,  7, 3]])\n",
    "\n",
    "# Learning curves (macro-F1)\n",
    "epochs = list(range(1, 20))\n",
    "train_f1 = [0.3766,0.3908,0.4390,0.4926,0.4673,0.4936,0.4879,0.5042,0.5204,0.5082,0.5264,0.5190,0.5520,0.5542,0.5385,0.5557,0.5492,0.5587,0.5569]\n",
    "val_f1   = [0.3919,0.4040,0.4749,0.5606,0.5459,0.5397,0.5097,0.5448,0.5620,0.5271,0.6749,0.5736,0.6441,0.6003,0.5905,0.5914,0.5555,0.5466,0.6361]\n",
    "\n",
    "out = Path(\"figs_final\"); out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- 2) Helpers ----------\n",
    "def save_figure(fig, path):\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_cm_high_contrast(cm, labels, title, save_path, cmap=\"plasma\"):\n",
    "    fig = plt.figure(figsize=(7,5.6))\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.ax.set_ylabel(\"Count\", rotation=90, va=\"center\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "\n",
    "    # white text with black outline for readability\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            txt = ax.text(j, i, str(cm[i,j]), ha=\"center\", va=\"center\",\n",
    "                          color=\"white\", fontsize=12, fontweight=\"bold\")\n",
    "            txt.set_path_effects([patheffects.Stroke(linewidth=2.5, foreground='black'),\n",
    "                                  patheffects.Normal()])\n",
    "    save_figure(fig, save_path)\n",
    "\n",
    "def plot_cm_row_normalized(cm, labels, title, save_path, cmap=\"plasma\"):\n",
    "    # row-normalize to percentages\n",
    "    row_sum = cm.sum(axis=1, keepdims=True).astype(float)\n",
    "    pct = np.divide(cm, np.maximum(row_sum, 1e-9)) * 100.0\n",
    "    fig = plt.figure(figsize=(7,5.6))\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(pct, interpolation='nearest', cmap=cmap, vmin=0, vmax=100)\n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.ax.set_ylabel(\"%\", rotation=0, va=\"center\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "\n",
    "    for i in range(pct.shape[0]):\n",
    "        for j in range(pct.shape[1]):\n",
    "            val = f\"{pct[i,j]:.1f}%\"\n",
    "            txt = ax.text(j, i, val, ha=\"center\", va=\"center\",\n",
    "                          color=\"white\", fontsize=12, fontweight=\"bold\")\n",
    "            txt.set_path_effects([patheffects.Stroke(linewidth=2.5, foreground='black'),\n",
    "                                  patheffects.Normal()])\n",
    "    save_figure(fig, save_path)\n",
    "\n",
    "# ---------- 3) Overall metrics (Macro-F1, Accuracy) ----------\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = plt.gca()\n",
    "names = [\"Macro-F1\",\"Accuracy\"]\n",
    "xgb_vals = [xgb_overall[\"macro_f1\"], xgb_overall[\"accuracy\"]]\n",
    "cnn_vals = [cnn_overall[\"macro_f1\"], cnn_overall[\"accuracy\"]]\n",
    "x = np.arange(len(names)); w=0.35\n",
    "ax.bar(x - w/2, xgb_vals, w, label=\"XGBoost\")\n",
    "ax.bar(x + w/2, cnn_vals, w, label=\"CNN\")\n",
    "ax.set_xticks(x); ax.set_xticklabels(names)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Overall Metrics\")\n",
    "ax.legend()\n",
    "save_figure(fig, out/\"Fig01_Overall_MacroF1_Accuracy.png\")\n",
    "\n",
    "# ---------- 4) Per-class F1 ----------\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = plt.gca()\n",
    "x = np.arange(len(classes)); w=0.35\n",
    "ax.bar(x - w/2, xgb_f1, w, label=\"XGBoost\")\n",
    "ax.bar(x + w/2, cnn_f1, w, label=\"CNN\")\n",
    "ax.set_xticks(x); ax.set_xticklabels(classes)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_ylabel(\"F1\")\n",
    "ax.set_title(\"Per-class F1\")\n",
    "ax.legend()\n",
    "save_figure(fig, out/\"Fig02_PerClassF1_Comparison.png\")\n",
    "\n",
    "# ---------- 5) Learning curves ----------\n",
    "fig = plt.figure(figsize=(7,4))\n",
    "ax = plt.gca()\n",
    "ax.plot(epochs, train_f1, marker=\"o\", label=\"Train Macro-F1\")\n",
    "ax.plot(epochs, val_f1, marker=\"o\", label=\"Val Macro-F1\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Macro-F1\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "ax.legend()\n",
    "ax.set_title(\"Learning Curves (CNN+BiLSTM+Attn)\")\n",
    "save_figure(fig, out/\"Fig03_LearningCurves_CNN.png\")\n",
    "\n",
    "# ---------- 6) Confusion matrices (counts) ----------\n",
    "plot_cm_high_contrast(xgb_cm, classes, \"Confusion Matrix ‚Äî XGBoost\", out/\"Fig04_ConfusionMatrix_XGBoost.png\")\n",
    "plot_cm_high_contrast(cnn_cm, classes, \"Confusion Matrix ‚Äî CNN+BiLSTM+Attn\", out/\"Fig05_ConfusionMatrix_CNN.png\")\n",
    "\n",
    "# ---------- 7) Confusion matrices (row-normalized %) ----------\n",
    "plot_cm_row_normalized(xgb_cm, classes, \"Row-normalized Confusion ‚Äî XGBoost\", out/\"Fig06_ConfusionMatrix_XGBoost_RowNorm.png\")\n",
    "plot_cm_row_normalized(cnn_cm, classes, \"Row-normalized Confusion ‚Äî CNN+BiLSTM+Attn\", out/\"Fig07_ConfusionMatrix_CNN_RowNorm.png\")\n",
    "\n",
    "print(f\"Saved figures to: {out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e024cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved train-only mean/std -> runs/cnn_strict/mean_std.json {'mean': -5.059500217437744, 'std': 396355.37500001}\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "\n",
    "NPZ_PATH   = \"data/wave_mag_dataset.npz\"\n",
    "CSV_PATH   = \"data/features_from_npz_mag.csv\"\n",
    "SPLIT_PATH = \"runs/frozen_splits.json\"\n",
    "OUT_STATS  = \"runs/cnn_strict/mean_std.json\"\n",
    "\n",
    "LABEL_MAP = {\"S\":0,\"M\":1,\"L\":2}\n",
    "\n",
    "# 1) load split\n",
    "with open(SPLIT_PATH, \"r\") as f:\n",
    "    splits = json.load(f)\n",
    "train_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "\n",
    "# 2) CSV scope & labels\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "df[\"y_csv\"] = df[\"mag_cls\"].map(LABEL_MAP).astype(int)\n",
    "tr_ids_csv = set(df[df[\"sample_id\"].isin(train_ids)][\"sample_id\"])\n",
    "\n",
    "# 3) NPZ positives only + scopeÂØπÈΩê\n",
    "d   = np.load(NPZ_PATH, allow_pickle=True)\n",
    "X   = d[\"waveforms\"]\n",
    "sid = np.array([str(s) for s in (d[\"sample_id\"] if \"sample_id\" in d else d[\"sample_ids\"])])\n",
    "if \"detect_label\" in d:\n",
    "    pos = d[\"detect_label\"].astype(int) == 1\n",
    "    X, y, sid = X[pos], y[pos], sid[pos]\n",
    "\n",
    "mask_tr = np.isin(sid, list(tr_ids_csv))\n",
    "X_tr_all, y_tr_all, sid_tr_all = X[mask_tr], y[mask_tr], sid[mask_tr]\n",
    "\n",
    "# 4) label ÂØπÈΩêÊ†°È™å\n",
    "y_csv_tr = df.set_index(\"sample_id\").loc[list(sid_tr_all), \"y_csv\"].to_numpy()\n",
    "assert (y_csv_tr == y_tr_all).all(), \"TRAIN Ê†áÁ≠æÂú® CSV ‰∏é NPZ Èó¥‰∏ç‰∏ÄËá¥\"\n",
    "\n",
    "# 5) TRAIN-only GLOBAL mean/stdÔºà‰∏é‰Ω†ËÑöÊú¨‰∏ÄËá¥Ôºâ\n",
    "X_stats = X_tr_all if X_tr_all.ndim == 2 else X_tr_all.reshape(len(X_tr_all), -1)\n",
    "mean = float(X_stats.mean())\n",
    "std  = float(X_stats.std() + 1e-8)\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_STATS), exist_ok=True)\n",
    "with open(OUT_STATS, \"w\") as f:\n",
    "    json.dump({\"mean\": mean, \"std\": std}, f, indent=2)\n",
    "print(\"[OK] saved train-only mean/std ->\", OUT_STATS, {\"mean\": mean, \"std\": std})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d346c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Detection] recall=0.1825 | triggers=2332 | hours=408.0\n",
      "\n",
      "== CNN (cascade on detected events, GLOBAL z-score) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9174    0.8772    0.8969       228\n",
      "           M     0.1714    0.2727    0.2105        22\n",
      "           L     0.0000    0.0000    0.0000         4\n",
      "\n",
      "    accuracy                         0.8110       254\n",
      "   macro avg     0.3630    0.3833    0.3691       254\n",
      "weighted avg     0.8384    0.8110    0.8233       254\n",
      "\n",
      "Confusion matrix:\n",
      " [[200  27   1]\n",
      " [ 16   6   0]\n",
      " [  2   2   0]]\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ---------- paths & params ----------\n",
    "NPZ_PATH   = \"data/wave_mag_dataset.npz\"\n",
    "CSV_PATH   = \"data/features_from_npz_mag.csv\"\n",
    "SPLIT_PATH = \"runs/frozen_splits.json\"\n",
    "MSEED_DIR  = \"waveforms\"\n",
    "MSEED_NAME = \"MAJO_{date}.mseed\"       # ‰Ω†ÁöÑÊó•Êñá‰ª∂ÂëΩÂêç\n",
    "BEST_PT    = \"runs/cnn_strict/best.pt\" # ËøôÁâà‰∏•Ê†ºÊ®°ÂûãÁöÑÊùÉÈáçÔºàÁ∫Ø state_dictÔºâ\n",
    "STATS_JSON = \"runs/cnn_strict/mean_std.json\"\n",
    "\n",
    "FS        = 20\n",
    "BAND      = (0.1, 8.0)\n",
    "PRE, POST = 20, 70\n",
    "STA, LTA  = 2.0, 20.0\n",
    "OFF       = 1.0\n",
    "REFRACT   = 300               # Âª∫ËÆÆÊõ¥Âº∫ÂêàÂπ∂\n",
    "CHOSEN_ON = 4.50              # ‚Üê Áî®‰Ω†ÈòàÂÄºÊâ´ÊèèÁöÑÁªìÊûúÊõøÊç¢\n",
    "\n",
    "# ---------- load test positives (catalog windows) ----------\n",
    "with open(SPLIT_PATH,\"r\") as f: splits = json.load(f)\n",
    "test_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "\n",
    "d   = np.load(NPZ_PATH, allow_pickle=True, mmap_mode=\"r\")\n",
    "X   = d[\"waveforms\"]\n",
    "sid = np.array([str(s) for s in (d[\"sample_id\"] if \"sample_id\" in d else d[\"sample_ids\"])])\n",
    "y   = (d[\"mag_class\"] if \"mag_class\" in d else d[\"labels\"]).astype(int)\n",
    "wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "\n",
    "if \"detect_label\" in d:\n",
    "    pos = d[\"detect_label\"].astype(int) == 1\n",
    "    X, y, sid, wst = X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "mask_te = np.isin(sid, list(set(pd.read_csv(CSV_PATH)[\"sample_id\"].astype(str)) & test_ids))\n",
    "y_te_all = y[mask_te]  # ‰ªÖÁî®‰∫éÂØπÈΩêÈïøÂ∫¶\n",
    "evt_time = (wst[mask_te].reset_index(drop=True) + pd.to_timedelta(PRE, \"s\"))\n",
    "df_te    = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "\n",
    "# ---------- build daily CFT & triggers ----------\n",
    "def build_cft(fp, fs=FS, band=BAND, sta=STA, lta=LTA):\n",
    "    st = read(fp).merge(method=1, fill_value='interpolate')\n",
    "    tr = st[0]\n",
    "    if abs(tr.stats.sampling_rate-fs)>1e-6: tr.resample(fs)\n",
    "    tr.detrend(\"demean\"); tr.filter(\"bandpass\", freqmin=band[0], freqmax=band[1])\n",
    "    x = tr.data.astype(np.float32)\n",
    "    cft = classic_sta_lta(x, int(sta*fs), int(lta*fs))\n",
    "    hours = float((tr.stats.endtime - tr.stats.starttime) / 3600.0)\n",
    "    return {\"cft\": cft, \"fs\": fs, \"t0\": tr.stats.starttime, \"hours\": hours}\n",
    "\n",
    "def triggers_from_cft(cftd, on, off=OFF, refract=REFRACT):\n",
    "    onoff = trigger_onset(cftd[\"cft\"], on, off)\n",
    "    fs, t0 = cftd[\"fs\"], cftd[\"t0\"]\n",
    "    picks = [pd.Timestamp((UTCDateTime(t0 + (i_on/fs))).datetime) for i_on,_ in onoff]\n",
    "    picks.sort()\n",
    "    merged=[]\n",
    "    for t in picks:\n",
    "        if not merged or (t-merged[-1]).total_seconds() > refract:\n",
    "            merged.append(t)\n",
    "    return merged\n",
    "\n",
    "dates = sorted(set(t.date() for t in df_te[\"event_time\"]))\n",
    "cft_cache, hours, dates_ok = {}, 0.0, []\n",
    "for dt in dates:\n",
    "    fp = os.path.join(MSEED_DIR, MSEED_NAME.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        print(\"[WARN] missing\", fp); \n",
    "        continue\n",
    "    cft_cache[dt] = build_cft(fp); hours += cft_cache[dt][\"hours\"]; dates_ok.append(dt)\n",
    "\n",
    "df_te = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].reset_index(drop=True)\n",
    "\n",
    "trig_list=[]\n",
    "for dt in dates_ok:\n",
    "    picks = triggers_from_cft(cft_cache[dt], CHOSEN_ON)\n",
    "    if picks:\n",
    "        trig_list.append(pd.DataFrame({\"trigger_time\": picks, \"date\": dt}))\n",
    "trig_all = pd.concat(trig_list, ignore_index=True).sort_values(\"trigger_time\") if trig_list else pd.DataFrame(columns=[\"trigger_time\",\"date\"])\n",
    "\n",
    "# ---------- window-aware match: which events are detected ----------\n",
    "ev = df_te.copy()\n",
    "ev[\"start\"]= ev[\"event_time\"] - pd.to_timedelta(PRE, \"s\")\n",
    "ev[\"end\"]  = ev[\"event_time\"] + pd.to_timedelta(POST, \"s\")\n",
    "m_ev = pd.merge_asof(ev[[\"event_time\",\"start\",\"end\"]],\n",
    "                     trig_all[[\"trigger_time\"]],\n",
    "                     left_on=\"start\", right_on=\"trigger_time\",\n",
    "                     direction=\"forward\")\n",
    "hit = m_ev[\"trigger_time\"].notna() & (m_ev[\"trigger_time\"] <= m_ev[\"end\"])\n",
    "det_recall = float(hit.mean())\n",
    "print(f\"[Detection] recall={det_recall:.4f} | triggers={len(trig_all)} | hours={hours:.1f}\")\n",
    "\n",
    "# ---------- cut windows at trigger_time, then GLOBAL z-score ----------\n",
    "win_len = FS*(PRE+POST)\n",
    "streams = {}\n",
    "waves, y_true = [], []\n",
    "y_te_series = pd.Series(y[mask_te]).reset_index(drop=True)\n",
    "\n",
    "for t, ygt in zip(m_ev.loc[hit,\"trigger_time\"], y_te_series.loc[hit]):\n",
    "    dt = t.date()\n",
    "    fp = os.path.join(MSEED_DIR, MSEED_NAME.format(date=dt))\n",
    "    if not os.path.exists(fp): continue\n",
    "    if dt not in streams:\n",
    "        st = read(fp).merge(method=1, fill_value='interpolate'); tr = st[0]\n",
    "        if abs(tr.stats.sampling_rate-FS)>1e-6: tr.resample(FS)\n",
    "        tr.detrend(\"demean\"); tr.filter(\"bandpass\", freqmin=BAND[0], freqmax=BAND[1])\n",
    "        streams[dt] = tr\n",
    "    tr = streams[dt]\n",
    "    t0 = UTCDateTime(t.to_pydatetime()) - PRE\n",
    "    t1 = UTCDateTime(t.to_pydatetime()) + POST\n",
    "    x = tr.slice(t0, t1).data\n",
    "    if len(x) >= win_len:\n",
    "        waves.append(x[:win_len].astype(np.float32))\n",
    "        y_true.append(int(ygt))\n",
    "\n",
    "if len(waves) == 0:\n",
    "    raise SystemExit(\"No matched windows ‚Äî adjust CHOSEN_ON/REFRACT/STA/LTA.\")\n",
    "\n",
    "X = np.stack(waves)\n",
    "\n",
    "# === GLOBAL z-score using TRAIN stats ===\n",
    "with open(STATS_JSON, \"r\") as f:\n",
    "    stats = json.load(f)\n",
    "mean, std = stats[\"mean\"], stats[\"std\"]\n",
    "Xn = (X - mean) / (std if std > 0 else 1.0)\n",
    "Xn = Xn[:, None, :]\n",
    "\n",
    "# ---------- define the SAME model class as your strict script ----------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k//2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x); x = self.bn(x); x = nn.functional.gelu(x); x = self.pool(x); return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__(); self.W = nn.Linear(d,d); self.v = nn.Linear(d,1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U = torch.tanh(self.W(H)); a = self.v(U).squeeze(-1); a = torch.softmax(a, dim=1)\n",
    "        Z = torch.bmm(a.unsqueeze(1), H).squeeze(1); return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(ConvBlock(in_ch,32), ConvBlock(32,64), ConvBlock(64,128))\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden, num_layers=layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2*hidden)\n",
    "        self.head = nn.Sequential(nn.Linear(2*hidden,128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128,n_classes))\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x); z = z.transpose(1,2); H,_ = self.lstm(z); Z,_ = self.attn(H); return self.head(Z)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "model  = CNNBiLSTMAttn().to(device)\n",
    "state  = torch.load(BEST_PT, map_location=device)        # <- Á∫Ø state_dict\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ---------- inference ----------\n",
    "y_pred=[]\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(Xn), 512):\n",
    "        xb = torch.from_numpy(Xn[i:i+512]).to(device)\n",
    "        logits = model(xb).cpu().numpy()\n",
    "        y_pred.extend(np.argmax(logits, axis=1))\n",
    "y_true = np.array(y_true, dtype=int); y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "print(\"\\n== CNN (cascade on detected events, GLOBAL z-score) ==\")\n",
    "print(classification_report(y_true, y_pred, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred, labels=[0,1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd8a6438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adaptive ON] triggers=468 over 408.0 hours | q=0.999 alpha=1.1 OFF=1.0 REFR=300s min_dur=0.0s\n"
     ]
    }
   ],
   "source": [
    "# ===== Adaptive daily STA/LTA threshold (drop-in replacement) =====\n",
    "import os, numpy as np, pandas as pd\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# --- params (reuse your existing globals where possible) ---\n",
    "FS        = 20\n",
    "BAND      = (0.1, 8.0)            # keep consistent with your pipeline\n",
    "STA, LTA  = 2.0, 20.0\n",
    "OFF       = 1.0\n",
    "REFRACT   = 300                   # seconds, refractory merging\n",
    "ADAPT_Q   = 0.999                 # daily CFT quantile (e.g., 99.9th percentile)\n",
    "ALPHA     = 1.10                  # scale factor on top of quantile (1.05‚Äì1.20 typical)\n",
    "MIN_DUR   = 0.0                   # seconds; set to 0.5‚Äì1.0 if you want per-trigger min duration\n",
    "\n",
    "# --- build CFT for a day (with bandpass) ---\n",
    "def build_cft(fp, fs=FS, band=BAND, sta=STA, lta=LTA):\n",
    "    st = read(fp).merge(method=1, fill_value='interpolate')\n",
    "    tr = st[0]\n",
    "    if abs(tr.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr.resample(fs)\n",
    "    tr.detrend(\"demean\")\n",
    "    tr.filter(\"bandpass\", freqmin=band[0], freqmax=band[1])\n",
    "    x = tr.data.astype(np.float32, copy=False)\n",
    "    cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "    hours = float((tr.stats.endtime - tr.stats.starttime) / 3600.0)\n",
    "    return {\"cft\": cft, \"fs\": fs, \"t0\": tr.stats.starttime, \"trace\": tr, \"hours\": hours}\n",
    "\n",
    "# --- convert a CFT to triggers using an ON/OFF pair (+ optional min duration) ---\n",
    "def triggers_from_cft(cftd, on, off=OFF, refract=REFRACT, min_dur=MIN_DUR):\n",
    "    onoff = trigger_onset(cftd[\"cft\"], on, off)  # list of [start_idx, end_idx]\n",
    "    fs, t0 = cftd[\"fs\"], cftd[\"t0\"]\n",
    "    picks = []\n",
    "    for a, b in onoff:\n",
    "        if min_dur > 0 and (b - a) / fs < min_dur:\n",
    "            continue\n",
    "        t = t0 + a / fs\n",
    "        ts = pd.Timestamp(UTCDateTime(t).datetime)\n",
    "        if not picks or (ts - picks[-1]).total_seconds() > refract:\n",
    "            picks.append(ts)\n",
    "    return picks\n",
    "\n",
    "# --- adaptive ON per day: quantile(cft) * alpha ---\n",
    "def day_adaptive_on(cft, q=ADAPT_Q, alpha=ALPHA):\n",
    "    base = np.quantile(cft, q)     # daily quantile on original CFT scale\n",
    "    return float(alpha * base)\n",
    "\n",
    "# -------- build daily CFTs (cache) --------\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for dt in sorted(set(t.date() for t in df_te[\"event_time\"])):  # df_te from your earlier code\n",
    "    fp = os.path.join(MSEED_DIR, MSEED_NAME.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        print(f\"[WARN] missing mseed: {fp}\")\n",
    "        continue\n",
    "    cftd = build_cft(fp)\n",
    "    cft_cache[dt] = cftd\n",
    "    total_hours += cftd[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "\n",
    "# keep only events on days we actually have\n",
    "df_te = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].reset_index(drop=True)\n",
    "\n",
    "# -------- generate triggers using daily-adaptive ON --------\n",
    "trig_list = []\n",
    "for dt in dates_ok:\n",
    "    entry = cft_cache[dt]\n",
    "    on_dt = day_adaptive_on(entry[\"cft\"], q=ADAPT_Q, alpha=ALPHA)\n",
    "    picks = triggers_from_cft(entry, on=on_dt, off=OFF, refract=REFRACT, min_dur=MIN_DUR)\n",
    "    if picks:\n",
    "        trig_list.append(pd.DataFrame({\"trigger_time\": picks, \"date\": dt, \"on_used\": on_dt}))\n",
    "\n",
    "trig_all = (pd.concat(trig_list, ignore_index=True).sort_values(\"trigger_time\")\n",
    "            if len(trig_list) else pd.DataFrame(columns=[\"trigger_time\",\"date\",\"on_used\"]))\n",
    "\n",
    "print(f\"[Adaptive ON] triggers={len(trig_all)} over {total_hours:.1f} hours \"\n",
    "      f\"| q={ADAPT_Q} alpha={ALPHA} OFF={OFF} REFR={REFRACT}s min_dur={MIN_DUR}s\")\n",
    "\n",
    "# --- downstream stays the same ---\n",
    "# 1) window-aware matching to compute detection recall on [t-20s, t+70s]\n",
    "# 2) (optional) re-center triggers around local envelope peak (recommended)\n",
    "# 3) standardize with GLOBAL (train) mean/std, run your strict CNN, report cascade metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a888cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adaptive ON] triggers=468 over 408.0 hours | q=0.999 alpha=1.1 OFF=1.0 REFR=300s min_dur=0.0s\n",
      "[Detection] recall=0.0524 | triggers=468 | hours=408.0\n",
      "\n",
      "== CNN (cascade with Adaptive ON + Re-centering, GLOBAL z-score) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.8000    0.9167    0.8544        48\n",
      "           M     0.7778    0.5600    0.6512        25\n",
      "           L     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.7945    0.7945    0.7945        73\n",
      "   macro avg     0.5259    0.4922    0.5018        73\n",
      "weighted avg     0.7924    0.7945    0.7848        73\n",
      "\n",
      "Confusion matrix:\n",
      " [[44  4  0]\n",
      " [11 14  0]\n",
      " [ 0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Cascade evaluation with Adaptive STA/LTA ON + Re-centering\n",
    "# - Builds daily STA/LTA CFT (0.1‚Äì8.0 Hz)\n",
    "# - Uses adaptive ON per day: ON = quantile(CFT, q) * alpha\n",
    "# - Computes detection recall with window-aware matching\n",
    "# - Recenters cut window around local envelope peak\n",
    "# - Applies GLOBAL z-score using TRAIN-only mean/std (strict comparable)\n",
    "# - Loads your strict CNN (state_dict) and reports cascade metrics\n",
    "# ================================================================\n",
    "\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "from obspy.signal.filter import envelope\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Data artifacts\n",
    "    NPZ_PATH:   str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH:   str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH: str = \"runs/frozen_splits.json\"\n",
    "    MEANSTD:    str = \"runs/cnn_strict/mean_std.json\"   # TRAIN-only mean/std (GLOBAL z-score)\n",
    "    BEST_PT:    str = \"runs/cnn_strict/best.pt\"         # strict CNN weights (pure state_dict)\n",
    "\n",
    "    # Waveform source for detection\n",
    "    MSEED_DIR:  str = \"waveforms\"\n",
    "    MSEED_FMT:  str = \"MAJO_{date}.mseed\"\n",
    "\n",
    "    # Window policy (must match your strict training)\n",
    "    FS:   int   = 20\n",
    "    BAND: tuple = (0.1, 8.0)   # bandpass for STA/LTA & cutting\n",
    "    PRE:  int   = 20           # seconds before center\n",
    "    POST: int   = 70           # seconds after center\n",
    "\n",
    "    # STA/LTA parameters\n",
    "    STA: float  = 2.0\n",
    "    LTA: float  = 20.0\n",
    "    OFF: float  = 1.0\n",
    "    REFR: int   = 300          # refractory merging in seconds\n",
    "\n",
    "    # Adaptive ON per day\n",
    "    ADAPT_Q: float = 0.999     # daily quantile\n",
    "    ALPHA:   float = 1.10      # scale factor on top of quantile\n",
    "    MIN_DUR: float = 0.0       # optional min ON duration (sec), e.g., 0.5‚Äì1.0 to suppress FPs\n",
    "\n",
    "    # Re-centering search window around trigger (to reduce misalignment)\n",
    "    RC_PRE:  int   = 10        # seconds before trigger to search envelope peak\n",
    "    RC_POST: int   = 20        # seconds after trigger to search envelope peak\n",
    "\n",
    "    # Device\n",
    "    DEVICE:  str   = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "CFG = Cfg()\n",
    "\n",
    "# ------------------------ Utilities: IO & Scope ------------------------\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    \"\"\"Return train/test ids (as strings) constrained by features CSV.\"\"\"\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    \"\"\"Load positives from NPZ (detect_label==1). Return X, y, sid, window_start (tz-naive pandas Series).\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X   = d[\"waveforms\"]\n",
    "    sid = np.array([str(s) for s in (d[\"sample_id\"] if \"sample_id\" in d else d[\"sample_ids\"])])\n",
    "    y   = (d[\"mag_class\"] if \"mag_class\" in d else d[\"labels\"]).astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    if \"detect_label\" in d:\n",
    "        pos = d[\"detect_label\"].astype(int) == 1\n",
    "        X, y, sid, wst = X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "    return X, y, sid, wst\n",
    "\n",
    "# --------------------- Step A: Build test event list --------------------\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"Create df_te with test event origin times (window_start + PRE).\"\"\"\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    mask_te = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mask_te].reset_index(drop=True) + pd.to_timedelta(cfg.PRE, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te, y[mask_te]  # y for later alignment\n",
    "\n",
    "df_te, y_te_all = build_test_events(CFG)\n",
    "\n",
    "# ------------------ Step B: STA/LTA per day (Adaptive ON) ------------------\n",
    "def build_cft_for_day(fp, fs, band, sta, lta):\n",
    "    \"\"\"Read MiniSEED, resample, detrend, bandpass, compute STA/LTA CFT.\"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr = st[0]\n",
    "    if abs(tr.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr.resample(fs)\n",
    "    tr.detrend(\"demean\")\n",
    "    tr.filter(\"bandpass\", freqmin=band[0], freqmax=band[1])\n",
    "    x = tr.data.astype(np.float32, copy=False)\n",
    "    cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "    hours = float((tr.stats.endtime - tr.stats.starttime) / 3600.0)\n",
    "    return {\"cft\": cft, \"fs\": fs, \"t0\": tr.stats.starttime, \"trace\": tr, \"hours\": hours}\n",
    "\n",
    "def adaptive_on(cft, q, alpha):\n",
    "    \"\"\"Daily ON threshold computed from CFT quantile * alpha.\"\"\"\n",
    "    base = np.quantile(cft, q)\n",
    "    return float(alpha * base)\n",
    "\n",
    "def triggers_from_cft(cftd, on, off, refr, min_dur=0.0):\n",
    "    \"\"\"Convert CFT to trigger times with ON/OFF + refractory + optional min duration.\"\"\"\n",
    "    onoff = trigger_onset(cftd[\"cft\"], on, off)\n",
    "    picks, fs, t0 = [], cftd[\"fs\"], cftd[\"t0\"]\n",
    "    for a, b in onoff:\n",
    "        if min_dur > 0.0 and (b - a) / fs < min_dur:\n",
    "            continue\n",
    "        t = t0 + a / fs\n",
    "        ts = pd.Timestamp(UTCDateTime(t).datetime)\n",
    "        if not picks or (ts - picks[-1]).total_seconds() > refr:\n",
    "            picks.append(ts)\n",
    "    return picks\n",
    "\n",
    "# Cache daily CFT and triggers\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for dt in sorted(set(t.date() for t in df_te[\"event_time\"])):\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        print(f\"[WARN] missing mseed: {fp}\")\n",
    "        continue\n",
    "    cftd = build_cft_for_day(fp, CFG.FS, CFG.BAND, CFG.STA, CFG.LTA)\n",
    "    cft_cache[dt] = cftd\n",
    "    total_hours += cftd[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "\n",
    "# Keep only events on days with available mseed/CFT\n",
    "df_te = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].reset_index(drop=True)\n",
    "\n",
    "# Generate triggers per day using adaptive ON\n",
    "trig_list = []\n",
    "for dt in dates_ok:\n",
    "    entry = cft_cache[dt]\n",
    "    on_dt = adaptive_on(entry[\"cft\"], CFG.ADAPT_Q, CFG.ALPHA)\n",
    "    picks = triggers_from_cft(entry, on=on_dt, off=CFG.OFF, refr=CFG.REFR, min_dur=CFG.MIN_DUR)\n",
    "    if picks:\n",
    "        trig_list.append(pd.DataFrame({\"trigger_time\": picks, \"date\": dt, \"on_used\": on_dt}))\n",
    "\n",
    "trig_all = (pd.concat(trig_list, ignore_index=True).sort_values(\"trigger_time\")\n",
    "            if len(trig_list) else pd.DataFrame(columns=[\"trigger_time\",\"date\",\"on_used\"]))\n",
    "\n",
    "print(f\"[Adaptive ON] triggers={len(trig_all)} over {total_hours:.1f} hours \"\n",
    "      f\"| q={CFG.ADAPT_Q} alpha={CFG.ALPHA} OFF={CFG.OFF} REFR={CFG.REFR}s min_dur={CFG.MIN_DUR}s\")\n",
    "\n",
    "# ---------------- Step C: Detection recall (window-aware) ----------------\n",
    "def window_recall(df_events, trig_df, pre, post):\n",
    "    \"\"\"Compute detection recall by matching trigger to event windows [t-PRE, t+POST].\"\"\"\n",
    "    if trig_df is None or len(trig_df) == 0 or len(df_events) == 0:\n",
    "        return 0.0, pd.DataFrame()\n",
    "    ev = df_events.copy()\n",
    "    ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(pre, \"s\")\n",
    "    ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(post, \"s\")\n",
    "    m = pd.merge_asof(ev[[\"event_time\",\"start\",\"end\"]],\n",
    "                      trig_df[[\"trigger_time\"]].sort_values(\"trigger_time\"),\n",
    "                      left_on=\"start\", right_on=\"trigger_time\",\n",
    "                      direction=\"forward\")\n",
    "    hit = m[\"trigger_time\"].notna() & (m[\"trigger_time\"] <= m[\"end\"])\n",
    "    return float(hit.mean()), m\n",
    "\n",
    "det_recall, match_df = window_recall(df_te, trig_all, CFG.PRE, CFG.POST)\n",
    "print(f\"[Detection] recall={det_recall:.4f} | triggers={len(trig_all)} | hours={total_hours:.1f}\")\n",
    "\n",
    "# ------------- Step D: Re-center around local envelope peak --------------\n",
    "def recenter_trigger(tr, t_pd, fs, pre=10, post=20):\n",
    "    \"\"\"Move center to local envelope peak near the trigger time.\"\"\"\n",
    "    t0 = UTCDateTime(t_pd.to_pydatetime()) - pre\n",
    "    t1 = UTCDateTime(t_pd.to_pydatetime()) + post\n",
    "    x = tr.slice(t0, t1).data.astype(np.float32, copy=False)\n",
    "    need = int((pre + post) * fs)\n",
    "    if len(x) < need:\n",
    "        return t_pd  # fallback\n",
    "    env = envelope(x)\n",
    "    i = int(np.argmax(env))\n",
    "    t_pk = t0 + i / fs\n",
    "    return pd.Timestamp(t_pk.datetime)\n",
    "\n",
    "# ------------------- Step E: Prepare GT labels for test -------------------\n",
    "# Align y_true to df_te order; then select detected subset (hit==True)\n",
    "X_all, y_all, sid_all, wst_all = load_npz_pos(CFG.NPZ_PATH)\n",
    "_, te_scope = get_ids_split(CFG.CSV_PATH, CFG.SPLIT_PATH)\n",
    "mask_te_all = np.isin(sid_all, list(te_scope))\n",
    "evt_time_full = (wst_all[mask_te_all] + pd.to_timedelta(CFG.PRE, \"s\"))\n",
    "order = np.argsort(evt_time_full.values)\n",
    "evt_sorted = evt_time_full.iloc[order].reset_index(drop=True)\n",
    "y_sorted   = pd.Series(y_all[mask_te_all]).iloc[order].reset_index(drop=True)\n",
    "\n",
    "# Keep only days we actually evaluated\n",
    "keep = evt_sorted.dt.date.isin(set(df_te[\"event_time\"].dt.date))\n",
    "evt_sorted = evt_sorted[keep].reset_index(drop=True)\n",
    "y_sorted   = y_sorted[keep].reset_index(drop=True)\n",
    "\n",
    "hit_mask = match_df[\"trigger_time\"].notna() & (match_df[\"trigger_time\"] <= match_df[\"end\"])\n",
    "matched_times = match_df.loc[hit_mask, \"trigger_time\"].reset_index(drop=True)\n",
    "y_true_ev = y_sorted.iloc[np.where(hit_mask.values)[0]].to_numpy()\n",
    "\n",
    "# ----------------- Step F: Cut windows (re-centered) ---------------------\n",
    "win_len = CFG.FS * (CFG.PRE + CFG.POST)\n",
    "streams = {}  # cache trace per day\n",
    "waves, y_true = [], []\n",
    "\n",
    "for t_pd, ygt in zip(matched_times, y_true_ev):\n",
    "    dt = t_pd.date()\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    if dt not in streams:\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\"); tr = st[0]\n",
    "        if abs(tr.stats.sampling_rate - CFG.FS) > 1e-6:\n",
    "            tr.resample(CFG.FS)\n",
    "        tr.detrend(\"demean\")\n",
    "        tr.filter(\"bandpass\", freqmin=CFG.BAND[0], freqmax=CFG.BAND[1])\n",
    "        streams[dt] = tr\n",
    "    tr = streams[dt]\n",
    "\n",
    "    # Re-center around local envelope peak\n",
    "    t_center = recenter_trigger(tr, t_pd, CFG.FS, pre=CFG.RC_PRE, post=CFG.RC_POST)\n",
    "\n",
    "    t0 = UTCDateTime(t_center.to_pydatetime()) - CFG.PRE\n",
    "    t1 = UTCDateTime(t_center.to_pydatetime()) + CFG.POST\n",
    "    x = tr.slice(t0, t1).data\n",
    "    if len(x) >= win_len:\n",
    "        waves.append(x[:win_len].astype(np.float32, copy=False))\n",
    "        y_true.append(int(ygt))\n",
    "\n",
    "if len(waves) == 0:\n",
    "    raise SystemExit(\"No matched windows after cutting. Consider lowering ALPHA / increasing REFR / reducing MIN_DUR.\")\n",
    "\n",
    "X_cut = np.stack(waves)\n",
    "y_true = np.array(y_true, dtype=int)\n",
    "\n",
    "# --------------- Step G: GLOBAL z-score (TRAIN-only stats) ---------------\n",
    "def ensure_train_meanstd(cfg: Cfg):\n",
    "    \"\"\"Load train-only mean/std; if missing, compute from TRAIN positives (strict comparable).\"\"\"\n",
    "    if os.path.exists(cfg.MEANSTD):\n",
    "        with open(cfg.MEANSTD, \"r\") as f:\n",
    "            s = json.load(f)\n",
    "        return float(s[\"mean\"]), float(s[\"std\"])\n",
    "    # Compute if not found\n",
    "    tr_scope, _ = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    X, y, sid, _ = load_npz_pos(cfg.NPZ_PATH)\n",
    "    mask_tr = np.isin(sid, list(tr_scope))\n",
    "    X_tr = X[mask_tr]\n",
    "    X_stats = X_tr if X_tr.ndim == 2 else X_tr.reshape(len(X_tr), -1)\n",
    "    mean = float(X_stats.mean()); std = float(X_stats.std() + 1e-8)\n",
    "    os.makedirs(os.path.dirname(cfg.MEANSTD), exist_ok=True)\n",
    "    with open(cfg.MEANSTD, \"w\") as f:\n",
    "        json.dump({\"mean\": mean, \"std\": std}, f, indent=2)\n",
    "    print(\"[Info] saved train-only mean/std ->\", cfg.MEANSTD)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = ensure_train_meanstd(CFG)\n",
    "Xn = (X_cut - mean) / (std if std > 0 else 1.0)\n",
    "Xn = Xn[:, None, :]\n",
    "\n",
    "# ---------------- Step H: Strict CNN (same as your training) -------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k // 2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn   = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = nn.functional.gelu(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(d, d)\n",
    "        self.v = nn.Linear(d, 1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U  = torch.tanh(self.W(H))\n",
    "        a  = self.v(U).squeeze(-1)\n",
    "        a  = torch.softmax(a, dim=1)\n",
    "        Z  = torch.bmm(a.unsqueeze(1), H).squeeze(1)\n",
    "        return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn  = nn.Sequential(ConvBlock(in_ch, 32), ConvBlock(32, 64), ConvBlock(64, 128))\n",
    "        self.lstm = nn.LSTM(128, hidden, num_layers=layers, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2*hidden)\n",
    "        self.head = nn.Sequential(nn.Linear(2*hidden, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, n_classes))\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)              # [B,C,L]\n",
    "        z = z.transpose(1, 2)        # [B,L,C]\n",
    "        H, _ = self.lstm(z)          # [B,L,2H]\n",
    "        Z, _ = self.attn(H)          # [B,2H]\n",
    "        return self.head(Z)\n",
    "\n",
    "device = torch.device(CFG.DEVICE)\n",
    "model  = CNNBiLSTMAttn().to(device)\n",
    "\n",
    "# BEST_PT is a pure state_dict saved by your strict script\n",
    "state_dict = torch.load(CFG.BEST_PT, map_location=device)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ------------------------ Step I: Inference & Report ----------------------\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(Xn), 512):\n",
    "        xb = torch.from_numpy(Xn[i:i+512]).to(device)\n",
    "        logits = model(xb).cpu().numpy()\n",
    "        y_pred.extend(np.argmax(logits, axis=1))\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "print(\"\\n== CNN (cascade with Adaptive ON + Re-centering, GLOBAL z-score) ==\")\n",
    "print(classification_report(y_true, y_pred, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred, labels=[0,1,2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8f0d7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] mseed files found: 20\n",
      "[Sample files] ['MAJO_2011-03-01.mseed', 'MAJO_2011-03-02.mseed', 'MAJO_2011-03-03.mseed', 'MAJO_2011-03-04.mseed', 'MAJO_2011-03-05.mseed']\n",
      "[Check] test event days: 17 (e.g., [datetime.date(2011, 3, 1), datetime.date(2011, 3, 3), datetime.date(2011, 3, 4)]...)\n",
      "[Check] days with mseed: 17; missing: 0\n",
      "\n",
      "[Debug-day] 2011-03-05 -> MAJO_2011-03-05.mseed\n",
      "[CFT] p90=2.32  p99=4.25  p99.9=5.82  ON(adapt)=5.59\n",
      "[Adaptive] segments=165\n",
      "[Adaptive] picks=109 (show up to 5) -> [Timestamp('2011-03-05 00:05:31.069500'), Timestamp('2011-03-05 00:11:22.769500'), Timestamp('2011-03-05 00:28:39.469500'), Timestamp('2011-03-05 00:35:25.319500'), Timestamp('2011-03-05 00:40:35.019500')]\n",
      "\n",
      "[OK] Adaptive ON produced picks on this day.\n"
     ]
    }
   ],
   "source": [
    "# ===================== QUICK STA/LTA DEBUG =====================\n",
    "import os, numpy as np, pandas as pd, random\n",
    "from glob import glob\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# ---- YOUR SETTINGS (adjust if needed) ----\n",
    "MSEED_DIR  = \"waveforms\"\n",
    "MSEED_FMT  = \"MAJO_{date}.mseed\"   # e.g., MAJO_2011-03-11.mseed\n",
    "FS         = 20\n",
    "BAND       = (0.1, 8.0)            # single-band debug\n",
    "STA, LTA   = 1.5, 20.0\n",
    "OFF        = 1.0\n",
    "ADAPT_Q    = 0.998\n",
    "ALPHA      = 1.03\n",
    "REFRACT    = 300\n",
    "\n",
    "# ---- 1) Basic inventory checks ----\n",
    "mseed_files = sorted(glob(os.path.join(MSEED_DIR, \"*.mseed\")))\n",
    "print(f\"[Check] mseed files found: {len(mseed_files)}\")\n",
    "if mseed_files[:5]:\n",
    "    print(\"[Sample files]\", [os.path.basename(x) for x in mseed_files[:5]])\n",
    "\n",
    "# df_te: your test events (pandas DataFrame with 'event_time')\n",
    "assert \"df_te\" in globals(), \"df_te not found in globals(). Make sure you've built the test event list.\"\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "print(f\"[Check] test event days: {len(event_days)} (e.g., {event_days[:3]}...)\")\n",
    "\n",
    "# map event days ‚Üí expected mseed path\n",
    "missing = []\n",
    "present = []\n",
    "for d in event_days:\n",
    "    fp = os.path.join(MSEED_DIR, MSEED_FMT.format(date=d))\n",
    "    if os.path.exists(fp): present.append((d, fp))\n",
    "    else: missing.append(str(d))\n",
    "print(f\"[Check] days with mseed: {len(present)}; missing: {len(missing)}\")\n",
    "if missing[:5]:\n",
    "    print(\"[Missing sample days]\", missing[:5])\n",
    "\n",
    "if not present:\n",
    "    raise SystemExit(\"No overlapping days between events and mseed files. Fix filenames or date range.\")\n",
    "\n",
    "# ---- pick a random available day for deep debug ----\n",
    "dt, fp = random.choice(present)\n",
    "print(f\"\\n[Debug-day] {dt} -> {os.path.basename(fp)}\")\n",
    "\n",
    "# ---- 2) Build trace and CFT (single band) ----\n",
    "st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "tr = st[0]\n",
    "if abs(tr.stats.sampling_rate - FS) > 1e-6:\n",
    "    tr.resample(FS)\n",
    "tr.detrend(\"demean\")\n",
    "tr.filter(\"bandpass\", freqmin=BAND[0], freqmax=BAND[1])\n",
    "x = tr.data.astype(np.float32, copy=False)\n",
    "cft = classic_sta_lta(x, int(STA*FS), int(LTA*FS))\n",
    "\n",
    "q90, q99, q999 = np.quantile(cft, [0.90, 0.99, 0.999])\n",
    "on_adapt = float(np.quantile(cft, ADAPT_Q) * ALPHA)\n",
    "print(f\"[CFT] p90={q90:.2f}  p99={q99:.2f}  p99.9={q999:.2f}  ON(adapt)={on_adapt:.2f}\")\n",
    "\n",
    "# ---- 3) Try adaptive ON ----\n",
    "onoff = trigger_onset(cft, on_adapt, OFF)\n",
    "seg_adapt = len(onoff)\n",
    "print(f\"[Adaptive] segments={seg_adapt}\")\n",
    "\n",
    "def picks_from_onoff(onoff, fs, t0, refract=REFRACT):\n",
    "    picks=[]; last=None\n",
    "    for a,b in onoff:\n",
    "        t = t0 + a/fs\n",
    "        ts = pd.Timestamp(UTCDateTime(t).datetime)\n",
    "        if (last is None) or ((ts - last).total_seconds() > refract):\n",
    "            picks.append(ts); last = ts\n",
    "    return picks\n",
    "\n",
    "picks_adapt = picks_from_onoff(onoff, FS, tr.stats.starttime, REFRACT)\n",
    "print(f\"[Adaptive] picks={len(picks_adapt)} (show up to 5) -> {picks_adapt[:5]}\")\n",
    "\n",
    "# ---- 4) If 0 picks ‚Üí fallback to constant ON (sanity) ----\n",
    "if len(picks_adapt) == 0:\n",
    "    CONST_ON = 3.25\n",
    "    onoff_c = trigger_onset(cft, CONST_ON, OFF)\n",
    "    picks_c = picks_from_onoff(onoff_c, FS, tr.stats.starttime, REFRACT)\n",
    "    print(f\"[Fallback-const ON={CONST_ON}] segments={len(onoff_c)}  picks={len(picks_c)} -> {picks_c[:5]}\")\n",
    "    if len(picks_c) == 0:\n",
    "        print(\"\\n[Hint] Still 0 picks. Try these quick relaxations:\")\n",
    "        print(\"  - Use STA=1.5s (already), or even 1.0s\")\n",
    "        print(\"  - Set CONST_ON=3.0\")\n",
    "        print(\"  - Check that the bandpass covers your signals (e.g., (0.5, 8.0))\")\n",
    "    else:\n",
    "        print(\"\\n[OK] Constant ON works. Your adaptive ON is too tight. Lower ADAPT_Q/ALPHA.\")\n",
    "else:\n",
    "    print(\"\\n[OK] Adaptive ON produced picks on this day.\")\n",
    "\n",
    "# ===================== END QUICK STA/LTA DEBUG =====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7386271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adaptive ON] triggers=1565 over 408.0 hours | q=0.998 alpha=1.03\n",
      "[Detection] recall=0.1070 | FPH=3.84 | events=1392\n"
     ]
    }
   ],
   "source": [
    "# ===== Build triggers for ALL days + compute detection recall (fixed) =====\n",
    "import os, numpy as np, pandas as pd\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# --- settings (adjust if needed) ---\n",
    "MSEED_DIR  = \"waveforms\"\n",
    "MSEED_FMT  = \"MAJO_{date}.mseed\"\n",
    "FS         = 20\n",
    "BAND       = (0.1, 8.0)\n",
    "STA, LTA   = 1.5, 20.0\n",
    "OFF        = 1.0\n",
    "ADAPT_Q    = 0.998\n",
    "ALPHA      = 1.03\n",
    "REFRACT    = 300            # <-- fixed name\n",
    "PRE, POST  = 20, 70         # event window for window-aware matching\n",
    "\n",
    "assert \"df_te\" in globals(), \"df_te not found. Please build the test event list first.\"\n",
    "\n",
    "def build_cft(fp):\n",
    "    \"\"\"Read MiniSEED, resample, detrend, bandpass, compute STA/LTA CFT.\"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr = st[0]\n",
    "    if abs(tr.stats.sampling_rate - FS) > 1e-6:\n",
    "        tr.resample(FS)\n",
    "    tr.detrend(\"demean\")\n",
    "    tr.filter(\"bandpass\", freqmin=BAND[0], freqmax=BAND[1])\n",
    "    x = tr.data.astype(np.float32, copy=False)\n",
    "    cft = classic_sta_lta(x, int(STA*FS), int(LTA*FS))\n",
    "    hours = float((tr.stats.endtime - tr.stats.starttime) / 3600.0)\n",
    "    return dict(cft=cft, fs=FS, t0=tr.stats.starttime, hours=hours)\n",
    "\n",
    "def picks_from_onoff(onoff, fs, t0, refract=REFRACT):\n",
    "    \"\"\"Convert on/off segments to de-duplicated picks using a refractory period.\"\"\"\n",
    "    picks = []; last = None\n",
    "    for a, b in onoff:\n",
    "        t = t0 + a / fs\n",
    "        ts = pd.Timestamp(UTCDateTime(t).datetime)\n",
    "        if (last is None) or ((ts - last).total_seconds() > refract):\n",
    "            picks.append(ts); last = ts\n",
    "    return picks\n",
    "\n",
    "def adaptive_on(cft, q=ADAPT_Q, alpha=ALPHA):\n",
    "    \"\"\"Daily ON threshold = quantile(CFT, q) * alpha.\"\"\"\n",
    "    return float(np.quantile(cft, q) * alpha)\n",
    "\n",
    "# 1) Build daily CFTs for all event days that have waveforms\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for dt in event_days:\n",
    "    fp = os.path.join(MSEED_DIR, MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    cftd = build_cft(fp)\n",
    "    cft_cache[dt] = cftd\n",
    "    total_hours += cftd[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "\n",
    "# 2) Generate triggers per day using adaptive ON (FIX: use refract=REFRACT)\n",
    "trigs = []\n",
    "for dt in dates_ok:\n",
    "    entry = cft_cache[dt]\n",
    "    on_dt = adaptive_on(entry[\"cft\"], ADAPT_Q, ALPHA)\n",
    "    onoff = trigger_onset(entry[\"cft\"], on_dt, OFF)\n",
    "    picks = picks_from_onoff(onoff, entry[\"fs\"], entry[\"t0\"], refract=REFRACT)  # <-- fixed\n",
    "    for t in picks:\n",
    "        trigs.append({\"trigger_time\": t, \"date\": dt, \"on_used\": on_dt})\n",
    "\n",
    "trig_all = (pd.DataFrame(trigs).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "            if len(trigs) else pd.DataFrame(columns=[\"trigger_time\",\"date\",\"on_used\"]))\n",
    "\n",
    "print(f\"[Adaptive ON] triggers={len(trig_all)} over {total_hours:.1f} hours | q={ADAPT_Q} alpha={ALPHA}\")\n",
    "\n",
    "# 3) Window-aware detection recall: a trigger hits if it falls in [event_time-PRE, event_time+POST]\n",
    "ev = df_te.copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(PRE, \"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(POST, \"s\")\n",
    "\n",
    "m = pd.merge_asof(\n",
    "    ev[[\"event_time\",\"start\",\"end\"]],\n",
    "    trig_all[[\"trigger_time\"]].sort_values(\"trigger_time\"),\n",
    "    left_on=\"start\", right_on=\"trigger_time\",\n",
    "    direction=\"forward\"\n",
    ")\n",
    "hit = m[\"trigger_time\"].notna() & (m[\"trigger_time\"] <= m[\"end\"])\n",
    "\n",
    "det_recall = float(hit.mean())\n",
    "fph = len(trig_all) / max(1e-6, total_hours)\n",
    "print(f\"[Detection] recall={det_recall:.4f} | FPH={fph:.2f} | events={len(ev)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02fa9f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] events considered: 1392 on 17 days | hours of data: 408.0\n",
      "[Adaptive] q=0.998 alpha=1.03 | recall=0.263 | FPH=3.97 | triggers=1618\n",
      "\n",
      "Top candidates (by recall desc, FPH asc):\n",
      "       q  alpha    recall       fph  triggers\n",
      "0  0.997   1.02  0.318247  5.026964      2051\n",
      "1  0.997   1.03  0.313937  4.904415      2001\n",
      "2  0.997   1.05  0.309626  4.713238      1923\n",
      "3  0.997   1.08  0.301724  4.431375      1808\n",
      "4  0.997   1.12  0.291667  4.051473      1653\n",
      "5  0.998   1.02  0.266523  4.056375      1655\n",
      "6  0.998   1.03  0.262931  3.965689      1618\n",
      "7  0.998   1.05  0.239943  3.718139      1517\n",
      "8  0.998   1.08  0.231322  3.389708      1383\n",
      "9  0.998   1.12  0.213362  2.975492      1214\n",
      "\n",
      "[Const ON] 4.50 -> {'on': 4.5, 'recall': 0.48419540229885055, 'fph': 7.002455032738818, 'triggers': 2857}\n",
      "[Const ON] 3.25 -> {'on': 3.25, 'recall': 0.5373563218390804, 'fph': 10.051476405061916, 'triggers': 4101}\n",
      "\n",
      "Hints:\n",
      "- If all recalls are low, first increase POST_DET to 300 (detection-only).\n",
      "- To increase recall: lower q (e.g., 0.997) or lower alpha (e.g., 1.02).\n",
      "- To reduce false alarms: raise alpha or add MIN_DUR=0.5‚Äì1.0s (then re-run).\n",
      "- If still no improvement, try STA=1.0s (more sensitive) or BAND_DET=(1.0, 5.0).\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STA/LTA detection ‚Äì full pipeline + parameter scan (English)\n",
    "# ============================================================\n",
    "\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Data artifacts (as in your project)\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Waveform source for detection\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"   # e.g., MAJO_2011-03-05.mseed\n",
    "\n",
    "    # Sampling and filters\n",
    "    FS         : int   = 20\n",
    "    BAND_DET   : tuple = (0.5, 8.0)          # detection bandpass (robust for local EQ)\n",
    "\n",
    "    # STA/LTA windows (seconds)\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "\n",
    "    # Trigger logic\n",
    "    OFF        : float = 1.0\n",
    "    REFRACT    : int   = 300                 # refractory period (seconds)\n",
    "    MIN_DUR    : float = 0.0                 # set >0.0 only after recall is acceptable\n",
    "\n",
    "    # Adaptive ON (these are just defaults; grid scan will search better ones)\n",
    "    ADAPT_Q    : float = 0.998\n",
    "    ALPHA      : float = 1.03\n",
    "\n",
    "    # Window for detection evaluation (NOT for CNN cutting)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 180                 # allow later arrivals at the station\n",
    "\n",
    "CFG = Cfg()\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "LABEL_MAP = {\"S\":0, \"M\":1, \"L\":2}\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid = np.array([str(s) for s in (d[\"sample_id\"] if \"sample_id\" in d else d[\"sample_ids\"])])\n",
    "    y = d[\"mag_class\"].astype(int) if \"mag_class\" in d else d[\"labels\"].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos = d[\"detect_label\"].astype(int) == 1 if \"detect_label\" in d else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"Test events = positives in TEST scope, event_time = window_start + 20s.\"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "def build_cft_for_day(fp, fs, band, sta, lta):\n",
    "    \"\"\"Read MiniSEED ‚Üí resample ‚Üí detrend ‚Üí bandpass ‚Üí STA/LTA CFT.\"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr = st[0]\n",
    "    if abs(tr.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr.resample(fs)\n",
    "    tr.detrend(\"demean\")\n",
    "    tr.filter(\"bandpass\", freqmin=band[0], freqmax=band[1])\n",
    "    x = tr.data.astype(np.float32, copy=False)\n",
    "    cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "    hours = float((tr.stats.endtime - tr.stats.starttime) / 3600.0)\n",
    "    return dict(cft=cft, fs=fs, t0=tr.stats.starttime, hours=hours)\n",
    "\n",
    "def picks_from_onoff(onoff, fs, t0, refract):\n",
    "    \"\"\"Convert on/off segments to picks with refractory merging.\"\"\"\n",
    "    picks, last = [], None\n",
    "    for a, b in onoff:\n",
    "        t = t0 + a / fs\n",
    "        ts = pd.Timestamp(UTCDateTime(t).datetime)\n",
    "        if (last is None) or ((ts - last).total_seconds() > refract):\n",
    "            picks.append(ts); last = ts\n",
    "    return picks\n",
    "\n",
    "def vectorized_any_hit(trig_times_ns, ev_start_ns, ev_end_ns):\n",
    "    \"\"\"Any trigger inside [start, end] window counts as a hit (vectorized).\"\"\"\n",
    "    i = np.searchsorted(trig_times_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trig_times_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Build df_te & daily CFTs --------------------\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for dt in event_days:\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    cftd = build_cft_for_day(fp, CFG.FS, CFG.BAND_DET, CFG.STA, CFG.LTA)\n",
    "    cft_cache[dt] = cftd\n",
    "    total_hours += cftd[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between df_te and waveform files.\")\n",
    "\n",
    "# Detection evaluation windows (wider POST for matching only)\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, \"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, \"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "print(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours of data: {total_hours:.1f}\")\n",
    "\n",
    "# -------------------- 1) Run adaptive once (quick check) --------------------\n",
    "def run_adaptive_once(q, alpha, cfg=CFG):\n",
    "    trigs = []\n",
    "    for dt in dates_ok:\n",
    "        entry = cft_cache[dt]\n",
    "        on_dt = float(np.quantile(entry[\"cft\"], q) * alpha)\n",
    "        onoff = trigger_onset(entry[\"cft\"], on_dt, cfg.OFF)\n",
    "        picks = picks_from_onoff(onoff, entry[\"fs\"], entry[\"t0\"], cfg.REFRACT)\n",
    "        for t in picks:\n",
    "            trigs.append(t)\n",
    "    trigs_sorted = np.array(sorted(trigs), dtype=\"datetime64[ns]\")\n",
    "    if trigs_sorted.size == 0:\n",
    "        return dict(q=q, alpha=alpha, recall=0.0, fph=0.0, triggers=0)\n",
    "    hit = vectorized_any_hit(trigs_sorted, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    fph = trigs_sorted.size / max(1e-6, total_hours)\n",
    "    return dict(q=q, alpha=alpha, recall=recall, fph=fph, triggers=int(trigs_sorted.size))\n",
    "\n",
    "res0 = run_adaptive_once(CFG.ADAPT_Q, CFG.ALPHA)\n",
    "print(f\"[Adaptive] q={res0['q']} alpha={res0['alpha']} | recall={res0['recall']:.3f} | FPH={res0['fph']:.2f} | triggers={res0['triggers']}\")\n",
    "\n",
    "# -------------------- 2) Grid search q/alpha --------------------\n",
    "cands_q     = [0.997, 0.998, 0.999]\n",
    "cands_alpha = [1.02, 1.03, 1.05, 1.08, 1.12]\n",
    "rows = []\n",
    "for q in cands_q:\n",
    "    for a in cands_alpha:\n",
    "        rows.append(run_adaptive_once(q, a))\n",
    "\n",
    "scan = pd.DataFrame(rows).sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nTop candidates (by recall desc, FPH asc):\")\n",
    "print(scan.head(10))\n",
    "\n",
    "# -------------------- 3) (Optional) Constant ON baselines --------------------\n",
    "def run_const(on, cfg=CFG):\n",
    "    trigs = []\n",
    "    for dt in dates_ok:\n",
    "        entry = cft_cache[dt]\n",
    "        onoff = trigger_onset(entry[\"cft\"], on, cfg.OFF)\n",
    "        picks = picks_from_onoff(onoff, entry[\"fs\"], entry[\"t0\"], cfg.REFRACT)\n",
    "        for t in picks:\n",
    "            trigs.append(t)\n",
    "    trigs_sorted = np.array(sorted(trigs), dtype=\"datetime64[ns]\")\n",
    "    if trigs_sorted.size == 0:\n",
    "        return dict(on=on, recall=0.0, fph=0.0, triggers=0)\n",
    "    hit = vectorized_any_hit(trigs_sorted, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    fph = trigs_sorted.size / max(1e-6, total_hours)\n",
    "    return dict(on=on, recall=recall, fph=fph, triggers=int(trigs_sorted.size))\n",
    "\n",
    "base_hi = run_const(4.50)  # low-FP baseline (likely low recall)\n",
    "base_lo = run_const(3.25)  # high-recall baseline (higher FP)\n",
    "print(\"\\n[Const ON] 4.50 ->\", base_hi)\n",
    "print(\"[Const ON] 3.25 ->\", base_lo)\n",
    "\n",
    "# -------------------- 4) Hints (what to tweak next) --------------------\n",
    "print(\"\\nHints:\")\n",
    "print(\"- If all recalls are low, first increase POST_DET to 300 (detection-only).\")\n",
    "print(\"- To increase recall: lower q (e.g., 0.997) or lower alpha (e.g., 1.02).\")\n",
    "print(\"- To reduce false alarms: raise alpha or add MIN_DUR=0.5‚Äì1.0s (then re-run).\")\n",
    "print(\"- If still no improvement, try STA=1.0s (more sensitive) or BAND_DET=(1.0, 5.0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1296c7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       q  alpha  min_dur    recall       fph  triggers\n",
      "0  0.997   1.03     0.00  0.313937  4.904415      2001\n",
      "1  0.997   1.03     0.25  0.313937  4.904415      2001\n",
      "2  0.997   1.03     0.50  0.313937  4.904415      2001\n",
      "3  0.997   1.03     1.00  0.313937  4.904415      2001\n",
      "\n",
      "[Chosen] q=0.997 alpha=1.03 min_dur=0.0 | recall=0.314 FPH=4.90 triggers=2001\n",
      "[Saved] runs/cascade_eval/triggers_adapt_q0.997_a1.03_md0.0.csv (rows=2001)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Sweep MIN_DUR on chosen (q, alpha) and save triggers  (FIXED)\n",
    "# ============================================================\n",
    "import numpy as np, pandas as pd, os\n",
    "from obspy.signal.trigger import trigger_onset\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "# ---- pick your adaptive combo here ----\n",
    "Q_CHOSEN     = 0.997\n",
    "ALPHA_CHOSEN = 1.03\n",
    "\n",
    "# ---- detection constants (reuse from previous cell) ----\n",
    "OFF      = CFG.OFF\n",
    "REFRACT  = CFG.REFRACT\n",
    "FS       = CFG.FS\n",
    "\n",
    "def picks_from_onoff_with_mindur(onoff, fs, t0, refract, min_dur):\n",
    "    \"\"\"Filter on/off by min duration (seconds) and apply refractory merging.\"\"\"\n",
    "    picks, last = [], None\n",
    "    for a, b in onoff:\n",
    "        dur_s = (b - a) / fs\n",
    "        if dur_s < min_dur:\n",
    "            continue\n",
    "        t = t0 + a / fs\n",
    "        ts = pd.Timestamp(UTCDateTime(t).datetime)\n",
    "        if (last is None) or ((ts - last).total_seconds() > refract):\n",
    "            picks.append(ts); last = ts\n",
    "    return picks\n",
    "\n",
    "def run_adaptive_with_mindur(q, alpha, min_dur):\n",
    "    # collect triggers across all available days\n",
    "    trigs = []\n",
    "    for dt, entry in cft_cache.items():\n",
    "        on = float(np.quantile(entry[\"cft\"], q) * alpha)\n",
    "        onoff = trigger_onset(entry[\"cft\"], on, OFF)\n",
    "        picks = picks_from_onoff_with_mindur(onoff, entry[\"fs\"], entry[\"t0\"], REFRACT, min_dur)\n",
    "        for t in picks:\n",
    "            trigs.append(t)\n",
    "    trigs_sorted = np.array(sorted(trigs), dtype=\"datetime64[ns]\")\n",
    "    if trigs_sorted.size == 0:\n",
    "        return dict(q=q, alpha=alpha, min_dur=min_dur, recall=0.0, fph=0.0, triggers=0, trigs=trigs_sorted)\n",
    "    # any-trigger-in-window hit logic (ev_start_ns / ev_end_ns defined earlier)\n",
    "    i = np.searchsorted(trigs_sorted, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_sorted, ev_end_ns,   side=\"right\")\n",
    "    hit = (j - i) > 0\n",
    "    recall = float(hit.mean())\n",
    "    fph = trigs_sorted.size / max(1e-6, total_hours)\n",
    "    return dict(q=q, alpha=alpha, min_dur=min_dur, recall=recall, fph=fph, triggers=int(trigs_sorted.size), trigs=trigs_sorted)\n",
    "\n",
    "# ---- sweep MIN_DUR ----\n",
    "cand_mindur = [0.0, 0.25, 0.5, 1.0]\n",
    "rows = []\n",
    "res_cache = {}\n",
    "for md in cand_mindur:\n",
    "    res = run_adaptive_with_mindur(Q_CHOSEN, ALPHA_CHOSEN, md)\n",
    "    rows.append({k: v for k, v in res.items() if k not in (\"trigs\",)})\n",
    "    res_cache[md] = res\n",
    "scan_md = pd.DataFrame(rows).sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(scan_md)\n",
    "\n",
    "# ---- pick a working point (example: best recall under FPH<=5) ----\n",
    "budget_fph = 5.0\n",
    "candidates = scan_md[scan_md[\"fph\"] <= budget_fph]\n",
    "if len(candidates) == 0:\n",
    "    chosen_row = scan_md.iloc[0]\n",
    "else:\n",
    "    # highest recall within budget_fph, tie-breaker by lower fph\n",
    "    chosen_row = candidates.sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "\n",
    "md_chosen = float(chosen_row[\"min_dur\"])\n",
    "trigs_final = res_cache[md_chosen][\"trigs\"]\n",
    "\n",
    "print(f\"\\n[Chosen] q={Q_CHOSEN} alpha={ALPHA_CHOSEN} min_dur={md_chosen} \"\n",
    "      f\"| recall={chosen_row['recall']:.3f} FPH={chosen_row['fph']:.2f} triggers={int(chosen_row['triggers'])}\")\n",
    "\n",
    "# ---- save triggers to CSV for downstream (re-centering + CNN cascade) ----\n",
    "out_dir = \"runs/cascade_eval\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "csv_out = os.path.join(out_dir, f\"triggers_adapt_q{Q_CHOSEN}_a{ALPHA_CHOSEN}_md{md_chosen}.csv\")\n",
    "\n",
    "if trigs_final.size == 0:\n",
    "    # still save empty file with correct columns\n",
    "    pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved EMPTY] {csv_out}\")\n",
    "else:\n",
    "    trig_ts = pd.to_datetime(trigs_final)                       # DatetimeIndex\n",
    "    date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')       # <-- FIX: get date as string safely\n",
    "    trig_df = pd.DataFrame({\n",
    "        \"trigger_time\": trig_ts,\n",
    "        \"date\": date_str\n",
    "    }).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "    trig_df.to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved] {csv_out} (rows={len(trig_df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4023fdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] events=1392 | hits=437 | hit_rate=0.314\n",
      "[Cut] windows=437 (of 437 hits) | shape=(437, 1800)\n",
      "\n",
      "== CNN (cascade, adaptive ON + re-center) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9333    0.9044    0.9186       387\n",
      "           M     0.3833    0.4694    0.4220        49\n",
      "           L     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.8535       437\n",
      "   macro avg     0.4389    0.4579    0.4469       437\n",
      "weighted avg     0.8695    0.8535    0.8608       437\n",
      "\n",
      "Confusion matrix:\n",
      " [[350  36   1]\n",
      " [ 25  23   1]\n",
      " [  0   1   0]]\n",
      "[Saved] runs/cascade_eval/cascade_report.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cascade eval: triggers CSV -> re-center -> cut -> z-score -> strict CNN\n",
    "# ============================================================\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.filter import envelope\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "    TRIG_CSV   : str = \"runs/cascade_eval/triggers_adapt_q0.997_a1.03_md0.0.csv\"\n",
    "\n",
    "    # Waveforms (for cutting windows)\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"      # e.g., MAJO_2011-03-05.mseed\n",
    "    FS         : int  = 20\n",
    "    BAND_CUT   : tuple = (0.5, 8.0)             # bandpass used before cutting\n",
    "\n",
    "    # Windowing\n",
    "    PRE        : int = 20                       # seconds (for CNN input)\n",
    "    POST       : int = 70\n",
    "    # Detection-eval window (should match youÂâçÈù¢ÁöÑËØÑ‰º∞ÔºõÂè™Áî®‰∫éÈÄâÊã©ÂëΩ‰∏≠Ëß¶Âèë)\n",
    "    PRE_DET    : int = 20\n",
    "    POST_DET   : int = 180\n",
    "\n",
    "    # Re-centering search around trigger\n",
    "    RC_PRE     : int = 10\n",
    "    RC_POST    : int = 20\n",
    "\n",
    "    # Model\n",
    "    BEST_PT    : str = \"runs/cnn_strict/best.pt\"\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "LABEL_MAP = {\"S\":0, \"M\":1, \"L\":2}\n",
    "\n",
    "# -------------------- Data helpers --------------------\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid = np.array([str(s) for s in (d[\"sample_id\"] if \"sample_id\" in d else d[\"sample_ids\"])])\n",
    "    y = d[\"mag_class\"].astype(int) if \"mag_class\" in d else d[\"labels\"].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos = d[\"detect_label\"].astype(int) == 1 if \"detect_label\" in d else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_split_scopes(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f: splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path); df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"Return df_te sorted by event_time, and y_te_sorted aligned.\"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_split_scopes(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(cfg.PRE, \"s\"))\n",
    "    y_te = pd.Series(y[mte]).reset_index(drop=True)\n",
    "    order = np.argsort(evt_time.values)\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time.values[order]})\n",
    "    y_te_sorted = y_te.iloc[order].reset_index(drop=True).to_numpy()\n",
    "    return df_te, y_te_sorted\n",
    "\n",
    "def train_mean_std(cfg: Cfg):\n",
    "    \"\"\"Global mean/std from TRAIN positives (strict comparable).\"\"\"\n",
    "    X, y, sid, _ = load_npz_pos(cfg.NPZ_PATH)\n",
    "    tr_scope, _ = get_split_scopes(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mtr = np.isin(sid, list(tr_scope))\n",
    "    Xtr = X[mtr]\n",
    "    flat = Xtr.reshape(len(Xtr), -1)\n",
    "    mean, std = float(flat.mean()), float(flat.std() + 1e-8)\n",
    "    # optional: also save for reuse\n",
    "    with open(os.path.join(cfg.OUT_DIR, \"mean_std_from_train.json\"), \"w\") as f:\n",
    "        json.dump({\"mean\": mean, \"std\": std}, f, indent=2)\n",
    "    return mean, std\n",
    "\n",
    "# -------------------- Waveform cutting --------------------\n",
    "_trace_cache = {}\n",
    "def get_trace_for_day(cfg: Cfg, day_str: str):\n",
    "    if day_str not in _trace_cache:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=day_str))\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        if abs(tr.stats.sampling_rate - cfg.FS) > 1e-6:\n",
    "            tr.resample(cfg.FS)\n",
    "        tr.detrend(\"demean\")\n",
    "        tr.filter(\"bandpass\", freqmin=cfg.BAND_CUT[0], freqmax=cfg.BAND_CUT[1])\n",
    "        _trace_cache[day_str] = tr\n",
    "    return _trace_cache[day_str]\n",
    "\n",
    "def recenter_trigger(tr, t_ts, fs, pre, post):\n",
    "    \"\"\"Find local envelope peak in [t- pre, t+ post] and return adjusted timestamp.\"\"\"\n",
    "    t0 = UTCDateTime(t_ts.to_pydatetime()) - pre\n",
    "    t1 = UTCDateTime(t_ts.to_pydatetime()) + post\n",
    "    x = tr.slice(t0, t1).data.astype(np.float32, copy=False)\n",
    "    need = int((pre + post) * fs)\n",
    "    if len(x) < need:\n",
    "        return t_ts\n",
    "    env = envelope(x)\n",
    "    i = int(np.argmax(env))\n",
    "    t_pk = t0 + i / fs\n",
    "    return pd.Timestamp(UTCDateTime(t_pk).datetime)\n",
    "\n",
    "# -------------------- Strict CNN model (same as training) --------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k // 2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn   = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x); x = self.bn(x); x = F.gelu(x); x = self.pool(x); return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d): \n",
    "        super().__init__(); self.W = nn.Linear(d, d); self.v = nn.Linear(d, 1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U = torch.tanh(self.W(H)); a = self.v(U).squeeze(-1); a = torch.softmax(a, dim=1)\n",
    "        Z = torch.bmm(a.unsqueeze(1), H).squeeze(1); return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(ConvBlock(in_ch, 32), ConvBlock(32, 64), ConvBlock(64, 128))\n",
    "        self.lstm = nn.LSTM(128, hidden, num_layers=layers, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2 * hidden)\n",
    "        self.head = nn.Sequential(nn.Linear(2 * hidden, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, n_classes))\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)          # [B,C,L]\n",
    "        z = z.transpose(1, 2)    # [B,L,C]\n",
    "        H, _ = self.lstm(z)      # [B,L,2H]\n",
    "        Z, _ = self.attn(H)      # [B,2H]\n",
    "        return self.head(Z)\n",
    "\n",
    "# -------------------- 1) Build test events & load triggers --------------------\n",
    "df_te, y_te_sorted = build_test_events(CFG)\n",
    "trig_df = pd.read_csv(CFG.TRIG_CSV)\n",
    "trig_df[\"trigger_time\"] = pd.to_datetime(trig_df[\"trigger_time\"])\n",
    "trig_df = trig_df.sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "\n",
    "# detection-eval windows (only for selecting hits)\n",
    "ev = df_te.copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, \"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, \"s\")\n",
    "\n",
    "tr = trig_df[\"trigger_time\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "ev_start = ev[\"start\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "ev_end   = ev[\"end\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "\n",
    "# searchsorted to find the first trigger >= start, then check if <= end\n",
    "i = np.searchsorted(tr, ev_start, side=\"left\")\n",
    "j = np.searchsorted(tr, ev_end,   side=\"right\")\n",
    "hit = (j - i) > 0\n",
    "first_idx = np.where(hit, i, -1)\n",
    "\n",
    "print(f\"[Info] events={len(ev)} | hits={int(hit.sum())} | hit_rate={hit.mean():.3f}\")\n",
    "\n",
    "# -------------------- 2) Re-center & cut CNN windows for hits --------------------\n",
    "win_len = CFG.FS * (CFG.PRE + CFG.POST)\n",
    "X_cut, y_hit, kept = [], [], []\n",
    "for idx in np.where(hit)[0]:\n",
    "    t_first = pd.Timestamp(tr[first_idx[idx]].astype(\"datetime64[ns]\").astype('datetime64[ns]').astype('datetime64[ns]'))\n",
    "    day_str = str(t_first.date())\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=day_str))\n",
    "    if not os.path.exists(fp): \n",
    "        continue\n",
    "    tr_day = get_trace_for_day(CFG, day_str)\n",
    "    t_center = recenter_trigger(tr_day, t_first, fs=CFG.FS, pre=CFG.RC_PRE, post=CFG.RC_POST)\n",
    "    t0 = UTCDateTime(t_center.to_pydatetime()) - CFG.PRE\n",
    "    t1 = UTCDateTime(t_center.to_pydatetime()) + CFG.POST\n",
    "    x = tr_day.slice(t0, t1).data\n",
    "    if len(x) >= win_len:\n",
    "        X_cut.append(x[:win_len].astype(np.float32))\n",
    "        y_hit.append(y_te_sorted[idx])\n",
    "        kept.append(idx)\n",
    "\n",
    "if len(X_cut) == 0:\n",
    "    raise SystemExit(\"No windows cut. Check MSEED filenames and detection window settings.\")\n",
    "\n",
    "X_cut = np.stack(X_cut, axis=0)\n",
    "y_hit = np.array(y_hit, dtype=int)\n",
    "\n",
    "print(f\"[Cut] windows={len(X_cut)} (of {int(hit.sum())} hits) | shape={X_cut.shape}\")\n",
    "\n",
    "# -------------------- 3) Global z-score (TRAIN-only mean/std) --------------------\n",
    "mean, std = train_mean_std(CFG)\n",
    "Xn = (X_cut - mean) / (std if std > 0 else 1.0)\n",
    "Xn = Xn[:, None, :]  # [N,1,T]\n",
    "\n",
    "# -------------------- 4) Load strict CNN and infer --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "model = CNNBiLSTMAttn(in_ch=1, n_classes=3).to(device)\n",
    "state = torch.load(CFG.BEST_PT, map_location=device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i0 in range(0, len(Xn), 512):\n",
    "        xb = torch.from_numpy(Xn[i0:i0+512]).to(device)\n",
    "        logits = model(xb)\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# -------------------- 5) Report & save --------------------\n",
    "rep = classification_report(y_hit, y_pred, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0)\n",
    "cm = confusion_matrix(y_hit, y_pred, labels=[0,1,2])\n",
    "macro_f1 = f1_score(y_hit, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n== CNN (cascade, adaptive ON + re-center) ==\")\n",
    "print(rep)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "out_txt = os.path.join(CFG.OUT_DIR, \"cascade_report.txt\")\n",
    "with open(out_txt, \"w\") as f:\n",
    "    f.write(rep + \"\\n\")\n",
    "    f.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "\n",
    "np.savez(os.path.join(CFG.OUT_DIR, \"cascade_preds.npz\"),\n",
    "         y_true=y_hit, y_pred=y_pred, kept_event_indices=np.array(kept, dtype=int))\n",
    "\n",
    "print(f\"[Saved] {out_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a860e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] events considered: 1392 on 17 days | hours=408.0\n",
      "\n",
      "Top (q,alpha) candidates before MIN_DUR:\n",
      "       q  alpha  min_dur    recall       fph  triggers\n",
      "0  0.997   1.02      0.0  0.508621  3.811277      1555\n",
      "1  0.997   1.03      0.0  0.495690  3.713237      1515\n",
      "2  0.997   1.05      0.0  0.486351  3.566179      1455\n",
      "3  0.997   1.08      0.0  0.461925  3.362747      1372\n",
      "4  0.997   1.12      0.0  0.426006  3.044119      1242\n",
      "5  0.998   1.02      0.0  0.407328  2.946080      1202\n",
      "6  0.998   1.03      0.0  0.397270  2.852943      1164\n",
      "7  0.998   1.05      0.0  0.378592  2.671570      1090\n",
      "8  0.998   1.08      0.0  0.329023  2.355394       961\n",
      "9  0.998   1.12      0.0  0.267960  1.867648       762\n",
      "\n",
      "[Chosen (pre-MIN_DUR)] q=0.997 alpha=1.02 | recall=0.509 FPH=3.81 triggers=1555\n",
      "\n",
      "After MIN_DUR sweep:\n",
      "       q  alpha  min_dur    recall       fph  triggers\n",
      "0  0.997   1.02      0.0  0.508621  3.811277      1555\n",
      "1  0.997   1.02      1.0  0.507902  3.767159      1537\n",
      "2  0.997   1.02      0.5  0.507902  3.791669      1547\n",
      "\n",
      "[Final] q=0.997 alpha=1.02 min_dur=0.0 | recall=0.509 FPH=3.81 triggers=1555\n",
      "[Saved] runs/cascade_eval/triggers_MB_2of3_q0.997_a1.02_md0.0.csv (rows=1555)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STA/LTA detection (POST_DET=300, REFRACT=120, multi-band 2-of-3)\n",
    "# - Rebuild test events from your artifacts\n",
    "# - Build daily CFTs on 3 bands\n",
    "# - Scan (q, alpha) with 2-of-3 fusion, pick best under FPH<=6/h\n",
    "# - Then sweep MIN_DUR to reduce FPH further\n",
    "# - Save final triggers CSV for cascade use\n",
    "# ============================================================\n",
    "\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"   # e.g., MAJO_2011-03-05.mseed\n",
    "\n",
    "    # Detection sampling\n",
    "    FS         : int   = 20\n",
    "\n",
    "    # Three bands for 2-of-3 fusion\n",
    "    BANDS      : tuple = ((0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA windows\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "\n",
    "    # Trigger logic\n",
    "    OFF        : float = 1.0\n",
    "    REFRACT    : int   = 120       # step-2: shorter refractory period\n",
    "\n",
    "    # Detection-eval window (NOT used for cutting CNN windows)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300       # step-1: wider POST for evaluation only\n",
    "\n",
    "    # Adaptive parameters (grids below will override)\n",
    "    ADAPT_Q    : float = 0.998\n",
    "    ALPHA      : float = 1.03\n",
    "\n",
    "    # Search grids / budgets\n",
    "    GRID_Q     : tuple = (0.997, 0.998, 0.999)\n",
    "    GRID_ALPHA : tuple = (1.02, 1.03, 1.05, 1.08, 1.12)\n",
    "    FPH_BUDGET : float = 6.0       # pick the highest recall under this FPH\n",
    "\n",
    "    # Second-stage min duration sweep (after picking q/alpha)\n",
    "    MIN_DUR_SET: tuple = (0.0, 0.5, 1.0)\n",
    "\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- Helpers to rebuild test events --------------------\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid = np.array([str(s) for s in (d[\"sample_id\"] if \"sample_id\" in d else d[\"sample_ids\"])])\n",
    "    y = d[\"mag_class\"].astype(int) if \"mag_class\" in d else d[\"labels\"].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos = d[\"detect_label\"].astype(int) == 1 if \"detect_label\" in d else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path); df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"\n",
    "    Test events = positives within TEST scope.\n",
    "    Event time (catalog-origin) = window_start + 20s (PRE for classification dataset).\n",
    "    \"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "# -------------------- Build daily multi-band CFTs --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    \"\"\"\n",
    "    Read one MiniSEED file once; resample & detrend; then for each band:\n",
    "    - bandpass\n",
    "    - compute STA/LTA characteristic function (CFT)\n",
    "    Returns: dict(fs, t0, hours, cft_list=[cft_band1, cft_band2, cft_band3])\n",
    "    \"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime) / 3600.0)\n",
    "\n",
    "    cft_list = []\n",
    "    for (fmin, fmax) in bands:\n",
    "        tr = tr0.copy()\n",
    "        tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "        cft_list.append(cft)\n",
    "\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "def fuse_onoff_2of3(onoffs):\n",
    "    \"\"\"\n",
    "    2-of-3 fusion on interval level.\n",
    "    onoffs: list of arrays [[(a1,b1), (a2,b2),...], ...] in sample indices (half-open).\n",
    "    Return fused intervals where >=2 bands are ON simultaneously.\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    for arr in onoffs:\n",
    "        for a, b in arr:\n",
    "            events.append((int(a), +1))\n",
    "            events.append((int(b), -1))\n",
    "    if not events:\n",
    "        return np.empty((0,2), dtype=int)\n",
    "    # Sort with starts before ends at the same position\n",
    "    events.sort(key=lambda x: (x[0], -x[1]))\n",
    "    fused = []\n",
    "    active = 0\n",
    "    current_start = None\n",
    "    for pos, delta in events:\n",
    "        prev = active\n",
    "        active += delta\n",
    "        if prev < 2 and active >= 2:\n",
    "            current_start = pos\n",
    "        elif prev >= 2 and active < 2 and current_start is not None:\n",
    "            fused.append((current_start, pos))\n",
    "            current_start = None\n",
    "    return np.array(fused, dtype=int) if fused else np.empty((0,2), dtype=int)\n",
    "\n",
    "def filter_min_dur(onoff, fs, min_dur_s):\n",
    "    if onoff.size == 0 or min_dur_s <= 0:\n",
    "        return onoff\n",
    "    keep = ((onoff[:,1] - onoff[:,0]) / fs) >= float(min_dur_s)\n",
    "    return onoff[keep]\n",
    "\n",
    "def picks_from_onoff(onoff, fs, t0, refract):\n",
    "    \"\"\"\n",
    "    Convert fused intervals to pick times (start of each interval), \n",
    "    with refractory merging in wall-clock time.\n",
    "    \"\"\"\n",
    "    picks, last = [], None\n",
    "    for a, b in onoff:\n",
    "        t = t0 + a / fs\n",
    "        ts = pd.Timestamp(UTCDateTime(t).datetime)\n",
    "        if (last is None) or ((ts - last).total_seconds() > refract):\n",
    "            picks.append(ts); last = ts\n",
    "    return picks\n",
    "\n",
    "# -------------------- Main detection build & scan --------------------\n",
    "# 1) Test events and available days\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for dt in event_days:\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    cftd = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    cft_cache[dt] = cftd\n",
    "    total_hours += cftd[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between df_te and waveform files.\")\n",
    "\n",
    "# Detection-eval windows (use wider POST_DET=300 for evaluation)\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, \"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, \"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "print(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    if trigs_ns.size == 0:\n",
    "        return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "def run_adaptive_multiband(q, alpha, min_dur_s=0.0):\n",
    "    \"\"\"\n",
    "    For each day:\n",
    "      - per-band ON = quantile(cft_band, q) * alpha\n",
    "      - per-band on/off = trigger_onset(cft_band, ON, OFF)\n",
    "      - 2-of-3 fusion on intervals\n",
    "      - filter by min_dur\n",
    "      - refractory merge to picks\n",
    "    Aggregate all picks across days, then compute recall/FPH.\n",
    "    \"\"\"\n",
    "    all_picks = []\n",
    "    for dt, entry in cft_cache.items():\n",
    "        onoffs = []\n",
    "        for cft in entry[\"cft_list\"]:\n",
    "            on_val = float(np.quantile(cft, q) * alpha)\n",
    "            onoff  = trigger_onset(cft, on_val, CFG.OFF)\n",
    "            onoffs.append(onoff)\n",
    "        fused = fuse_onoff_2of3(onoffs)\n",
    "        fused = filter_min_dur(fused, entry[\"fs\"], min_dur_s)\n",
    "        picks = picks_from_onoff(fused, entry[\"fs\"], entry[\"t0\"], CFG.REFRACT)\n",
    "        all_picks.extend(picks)\n",
    "    trigs_ns = np.array(sorted(all_picks), dtype=\"datetime64[ns]\")\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    return dict(q=q, alpha=alpha, min_dur=min_dur_s, recall=recall, fph=fph, triggers=int(trigs_ns.size), trigs=trigs_ns)\n",
    "\n",
    "# 2) Grid search (q, alpha) under FPH<=budget, with MIN_DUR fixed at 0.0\n",
    "rows = []\n",
    "for q in CFG.GRID_Q:\n",
    "    for a in CFG.GRID_ALPHA:\n",
    "        rows.append(run_adaptive_multiband(q, a, 0.0))\n",
    "scan = pd.DataFrame([{k:v for k,v in r.items() if k!='trigs'} for r in rows])\n",
    "scan = scan.sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nTop (q,alpha) candidates before MIN_DUR:\")\n",
    "print(scan.head(10))\n",
    "\n",
    "# Pick best recall under FPH budget\n",
    "cands = scan[scan[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "if len(cands) == 0:\n",
    "    chosen = scan.iloc[0]\n",
    "else:\n",
    "    chosen = cands.sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "Q_CHOSEN, A_CHOSEN = float(chosen[\"q\"]), float(chosen[\"alpha\"])\n",
    "print(f\"\\n[Chosen (pre-MIN_DUR)] q={Q_CHOSEN} alpha={A_CHOSEN} | recall={chosen['recall']:.3f} FPH={chosen['fph']:.2f} triggers={int(chosen['triggers'])}\")\n",
    "\n",
    "# 3) Sweep MIN_DUR at the chosen (q, alpha)\n",
    "rows_md = []\n",
    "res_cache = {}\n",
    "for md in CFG.MIN_DUR_SET:\n",
    "    r = run_adaptive_multiband(Q_CHOSEN, A_CHOSEN, md)\n",
    "    rows_md.append({k:v for k,v in r.items() if k!='trigs'})\n",
    "    res_cache[md] = r\n",
    "scan_md = pd.DataFrame(rows_md).sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nAfter MIN_DUR sweep:\")\n",
    "print(scan_md)\n",
    "\n",
    "# Choose highest recall under budget (tie-breaker: lower FPH)\n",
    "cands2 = scan_md[scan_md[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "if len(cands2) == 0:\n",
    "    final = scan_md.iloc[0]\n",
    "else:\n",
    "    final = cands2.sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "\n",
    "MD_CHOSEN = float(final[\"min_dur\"])\n",
    "trigs_final = res_cache[MD_CHOSEN][\"trigs\"]\n",
    "\n",
    "print(f\"\\n[Final] q={Q_CHOSEN} alpha={A_CHOSEN} min_dur={MD_CHOSEN} | recall={final['recall']:.3f} FPH={final['fph']:.2f} triggers={int(final['triggers'])}\")\n",
    "\n",
    "# 4) Save triggers CSV for cascade\n",
    "csv_out = os.path.join(CFG.OUT_DIR, f\"triggers_MB_2of3_q{Q_CHOSEN}_a{A_CHOSEN}_md{MD_CHOSEN}.csv\")\n",
    "if trigs_final.size == 0:\n",
    "    pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved EMPTY] {csv_out}\")\n",
    "else:\n",
    "    trig_ts = pd.to_datetime(trigs_final)\n",
    "    date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "    trig_df = pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "    trig_df.to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved] {csv_out} (rows={len(trig_df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec3d79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] events=1392 | hits=708 | hit_rate=0.509\n",
      "[Cut] windows=708 (of 708 hits) | shape=(708, 1800)\n",
      "\n",
      "== CNN (cascade on detected events) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9384    0.9104    0.9242       636\n",
      "           M     0.3448    0.4348    0.3846        69\n",
      "           L     0.0000    0.0000    0.0000         3\n",
      "\n",
      "    accuracy                         0.8602       708\n",
      "   macro avg     0.4277    0.4484    0.4363       708\n",
      "weighted avg     0.8766    0.8602    0.8677       708\n",
      "\n",
      "Confusion matrix:\n",
      " [[579  54   3]\n",
      " [ 38  30   1]\n",
      " [  0   3   0]]\n",
      "[End-to-End] S recall = 0.453 (total 1277)\n",
      "[End-to-End] M recall = 0.291 (total 103)\n",
      "[End-to-End] L recall = 0.000 (total 12)\n",
      "[Saved] runs/cascade_eval/cascade_MB_2of3_report.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cascade: use multi-band 2-of-3 triggers -> re-center -> cut -> z-score -> strict CNN\n",
    "# Uses: POST_DET=300s evaluation, REFRACT=120s (already in detection stage)\n",
    "# ============================================================\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.filter import envelope\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Triggers from your latest detection run:\n",
    "    TRIG_CSV   : str = \"runs/cascade_eval/triggers_MB_2of3_q0.997_a1.02_md0.0.csv\"\n",
    "\n",
    "    # Continuous waveforms (same station/day naming)\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int  = 20\n",
    "    BAND_CUT   : tuple = (0.5, 8.0)  # band for cutting\n",
    "\n",
    "    # CNN window (must match training: 20s pre + 70s post)\n",
    "    PRE        : int = 20\n",
    "    POST       : int = 70\n",
    "\n",
    "    # Detection-eval window (should match the detection eval you just used)\n",
    "    PRE_DET    : int = 20\n",
    "    POST_DET   : int = 300\n",
    "\n",
    "    # Re-centering search window around the trigger\n",
    "    RC_PRE     : int = 10\n",
    "    RC_POST    : int = 20\n",
    "\n",
    "    # Strict CNN checkpoint (the ‚Äúcomparable‚Äù model you trained)\n",
    "    BEST_PT    : str = \"runs/cnn_strict/best.pt\"\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "LABEL_MAP = {\"S\":0, \"M\":1, \"L\":2}\n",
    "\n",
    "# -------------------- Data helpers --------------------\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid = np.array([str(s) for s in (d.get(\"sample_id\", d.get(\"sample_ids\")))])\n",
    "    y = d.get(\"mag_class\", d.get(\"labels\")).astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos = d.get(\"detect_label\", np.ones(len(y), dtype=int)).astype(int) == 1\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path); df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"Test events = positives in TEST scope; event_time = window_start + 20s.\"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(cfg.PRE, \"s\"))\n",
    "    y_te = pd.Series(y[mte]).reset_index(drop=True)\n",
    "    order = np.argsort(evt_time.values)\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time.values[order]})\n",
    "    y_te_sorted = y_te.iloc[order].to_numpy()\n",
    "    return df_te.reset_index(drop=True), y_te_sorted\n",
    "\n",
    "def train_mean_std(cfg: Cfg):\n",
    "    \"\"\"Compute global mean/std from TRAIN positives for strict comparability.\"\"\"\n",
    "    X, y, sid, _ = load_npz_pos(cfg.NPZ_PATH)\n",
    "    tr_scope, _ = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mtr = np.isin(sid, list(tr_scope))\n",
    "    flat = X[mtr].reshape(np.sum(mtr), -1)\n",
    "    mean, std = float(flat.mean()), float(flat.std() + 1e-8)\n",
    "    with open(os.path.join(cfg.OUT_DIR, \"mean_std_from_train.json\"), \"w\") as f:\n",
    "        json.dump({\"mean\": mean, \"std\": std}, f, indent=2)\n",
    "    return mean, std\n",
    "\n",
    "# -------------------- Waveform cutting --------------------\n",
    "_trace_cache = {}\n",
    "def get_trace_for_day(cfg: Cfg, day_str: str):\n",
    "    if day_str not in _trace_cache:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=day_str))\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        if abs(tr.stats.sampling_rate - cfg.FS) > 1e-6:\n",
    "            tr.resample(cfg.FS)\n",
    "        tr.detrend(\"demean\")\n",
    "        tr.filter(\"bandpass\", freqmin=cfg.BAND_CUT[0], freqmax=cfg.BAND_CUT[1])\n",
    "        _trace_cache[day_str] = tr\n",
    "    return _trace_cache[day_str]\n",
    "\n",
    "def recenter_trigger(tr, t_ts, fs, pre, post):\n",
    "    \"\"\"Find envelope peak near trigger to center the CNN window.\"\"\"\n",
    "    t0 = UTCDateTime(t_ts.to_pydatetime()) - pre\n",
    "    t1 = UTCDateTime(t_ts.to_pydatetime()) + post\n",
    "    x = tr.slice(t0, t1).data.astype(np.float32, copy=False)\n",
    "    need = int((pre + post) * fs)\n",
    "    if len(x) < need:\n",
    "        return t_ts\n",
    "    env = envelope(x)\n",
    "    i = int(np.argmax(env))\n",
    "    t_pk = t0 + i / fs\n",
    "    return pd.Timestamp(UTCDateTime(t_pk).datetime)\n",
    "\n",
    "# -------------------- Strict CNN (same as your training code) --------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k // 2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn   = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x); x = self.bn(x); x = F.gelu(x); x = self.pool(x); return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d): \n",
    "        super().__init__(); self.W = nn.Linear(d, d); self.v = nn.Linear(d, 1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U = torch.tanh(self.W(H)); a = self.v(U).squeeze(-1); a = torch.softmax(a, dim=1)\n",
    "        Z = torch.bmm(a.unsqueeze(1), H).squeeze(1); return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(ConvBlock(in_ch, 32), ConvBlock(32, 64), ConvBlock(64, 128))\n",
    "        self.lstm = nn.LSTM(128, hidden, num_layers=layers, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2 * hidden)\n",
    "        self.head = nn.Sequential(nn.Linear(2 * hidden, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, n_classes))\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)          # [B,C,L]\n",
    "        z = z.transpose(1, 2)    # [B,L,C]\n",
    "        H, _ = self.lstm(z)      # [B,L,2H]\n",
    "        Z, _ = self.attn(H)      # [B,2H]\n",
    "        return self.head(Z)\n",
    "\n",
    "# -------------------- 1) Build test events & load triggers --------------------\n",
    "df_te, y_te_sorted = build_test_events(CFG)\n",
    "\n",
    "trig_df = pd.read_csv(CFG.TRIG_CSV)\n",
    "trig_df[\"trigger_time\"] = pd.to_datetime(trig_df[\"trigger_time\"])\n",
    "trig_df = trig_df.sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "\n",
    "# detection-eval windows (match POST_DET=300)\n",
    "ev = df_te.copy()\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, \"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, \"s\")\n",
    "\n",
    "tr = trig_df[\"trigger_time\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "ev_start = ev[\"start\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "ev_end   = ev[\"end\"].to_numpy(dtype=\"datetime64[ns]\")\n",
    "\n",
    "i = np.searchsorted(tr, ev_start, side=\"left\")\n",
    "j = np.searchsorted(tr, ev_end,   side=\"right\")\n",
    "hit = (j - i) > 0\n",
    "first_idx = np.where(hit, i, -1)\n",
    "\n",
    "print(f\"[Info] events={len(ev)} | hits={int(hit.sum())} | hit_rate={hit.mean():.3f}\")\n",
    "\n",
    "# -------------------- 2) Re-center & cut CNN windows for hits --------------------\n",
    "win_len = CFG.FS * (CFG.PRE + CFG.POST)\n",
    "X_cut, y_hit, kept = [], [], []\n",
    "for idx in np.where(hit)[0]:\n",
    "    t_first_ns = tr[first_idx[idx]]\n",
    "    t_first = pd.Timestamp(t_first_ns)\n",
    "    day_str = str(t_first.date())\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=day_str))\n",
    "    if not os.path.exists(fp): \n",
    "        continue\n",
    "    tr_day = get_trace_for_day(CFG, day_str)\n",
    "    t_center = recenter_trigger(tr_day, t_first, fs=CFG.FS, pre=CFG.RC_PRE, post=CFG.RC_POST)\n",
    "    t0 = UTCDateTime(t_center.to_pydatetime()) - CFG.PRE\n",
    "    t1 = UTCDateTime(t_center.to_pydatetime()) + CFG.POST\n",
    "    x = tr_day.slice(t0, t1).data\n",
    "    if len(x) >= win_len:\n",
    "        X_cut.append(x[:win_len].astype(np.float32))\n",
    "        y_hit.append(y_te_sorted[idx])\n",
    "        kept.append(idx)\n",
    "\n",
    "if len(X_cut) == 0:\n",
    "    raise SystemExit(\"No windows cut; check filenames or time bounds.\")\n",
    "\n",
    "X_cut = np.stack(X_cut, axis=0)\n",
    "y_hit = np.array(y_hit, dtype=int)\n",
    "print(f\"[Cut] windows={len(X_cut)} (of {int(hit.sum())} hits) | shape={X_cut.shape}\")\n",
    "\n",
    "# -------------------- 3) Global z-score (TRAIN-only mean/std) --------------------\n",
    "mean, std = train_mean_std(CFG)\n",
    "Xn = (X_cut - mean) / (std if std > 0 else 1.0)\n",
    "Xn = Xn[:, None, :]  # [N,1,T]\n",
    "\n",
    "# -------------------- 4) Load strict CNN and infer --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "model = CNNBiLSTMAttn(in_ch=1, n_classes=3).to(device)\n",
    "state = torch.load(CFG.BEST_PT, map_location=device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i0 in range(0, len(Xn), 512):\n",
    "        xb = torch.from_numpy(Xn[i0:i0+512]).to(device)\n",
    "        logits = model(xb)\n",
    "        y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# -------------------- 5) Reports: within-detected & end-to-end recall --------------------\n",
    "rep = classification_report(y_hit, y_pred, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0)\n",
    "cm = confusion_matrix(y_hit, y_pred, labels=[0,1,2])\n",
    "macro_f1 = f1_score(y_hit, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n== CNN (cascade on detected events) ==\")\n",
    "print(rep)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# End-to-end per-class recall (missed detections count as errors)\n",
    "lab = y_te_sorted\n",
    "det = hit\n",
    "correct_mask = np.zeros_like(lab, dtype=bool)\n",
    "# mark correct classifications among detected events\n",
    "kept_idx = np.array(kept, dtype=int)\n",
    "# map: kept event index -> correct?\n",
    "for k, idx in enumerate(kept_idx):\n",
    "    if y_hit[k] == y_pred[k]:\n",
    "        correct_mask[idx] = True\n",
    "\n",
    "for c, name in enumerate([\"S\",\"M\",\"L\"]):\n",
    "    total_c = (lab == c).sum()\n",
    "    e2e_c = correct_mask[lab == c].sum() / max(1, total_c)\n",
    "    print(f\"[End-to-End] {name} recall = {e2e_c:.3f} (total {total_c})\")\n",
    "\n",
    "# Save artifacts\n",
    "with open(os.path.join(CFG.OUT_DIR, \"cascade_MB_2of3_report.txt\"), \"w\") as f:\n",
    "    f.write(rep + \"\\n\")\n",
    "    f.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "\n",
    "np.savez(os.path.join(CFG.OUT_DIR, \"cascade_MB_2of3_preds.npz\"),\n",
    "         y_true=y_hit, y_pred=y_pred, kept_event_indices=np.array(kept, dtype=int))\n",
    "\n",
    "print(f\"[Saved] {os.path.join(CFG.OUT_DIR, 'cascade_MB_2of3_report.txt')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dda13d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] events considered: 1392 on 17 days | hours=408.0\n",
      "\n",
      "Top candidates before MIN_DUR:\n",
      "        q  alpha  min_dur    recall       fph  triggers  refract adapt\n",
      "0   0.994   1.00      0.0  0.772270  9.352947      3816       60  hour\n",
      "1   0.994   1.01      0.0  0.764368  9.022064      3681       60  hour\n",
      "2   0.994   1.00      0.0  0.761494  8.833338      3604       90  hour\n",
      "3   0.994   1.00      0.0  0.756466  8.946084      3650       60   day\n",
      "4   0.994   1.01      0.0  0.753592  8.553927      3490       90  hour\n",
      "5   0.994   1.01      0.0  0.753592  8.639711      3525       60   day\n",
      "6   0.994   1.02      0.0  0.752155  8.649515      3529       60  hour\n",
      "7   0.994   1.00      0.0  0.750718  8.485299      3462       90   day\n",
      "8   0.994   1.01      0.0  0.747845  8.210789      3350       90   day\n",
      "9   0.994   1.00      0.0  0.746408  8.365201      3413      120  hour\n",
      "10  0.994   1.02      0.0  0.742098  8.218142      3353       90  hour\n",
      "11  0.994   1.02      0.0  0.742098  8.352946      3408       60   day\n",
      "\n",
      "[Chosen pre-MIN_DUR] q=0.995 alpha=1.05 refract=120 scope=day | recall=0.644 FPH=5.96 triggers=2431\n",
      "\n",
      "After MIN_DUR sweep:\n",
      "       q  alpha  min_dur    recall       fph  triggers  refract adapt\n",
      "0  0.995   1.05      0.0  0.643678  5.958337      2431      120   day\n",
      "1  0.995   1.05      0.5  0.640805  5.857847      2390      120   day\n",
      "2  0.995   1.05      1.0  0.638649  5.705886      2328      120   day\n",
      "[Saved] runs/cascade_eval/triggers_MB_kofn2_q0.995_a1.05_rf120_scday_md0.0.csv (rows=2431)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multiband STA/LTA Detection v2 (FIXED)\n",
    "# - Rebuild TEST events from your NPZ + frozen_splits\n",
    "# - Build daily CFTs across 4 bands (default; you can drop to 3)\n",
    "# - Grid-search (q, alpha, refractory, adapt_scope) under FPH budget\n",
    "# - Fusion: K-of-N (default 2-of-N) OR optional OR+NMS fusion\n",
    "# - Sweep MIN_DUR\n",
    "# - Save final triggers CSV for cascade use\n",
    "# Notes:\n",
    "# * This version normalizes trigger_onset() output to ndarray, fixing\n",
    "#   environments where it returns a list instead of numpy array.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"   # e.g., MAJO_2011-03-05.mseed\n",
    "    FS         : int = 20\n",
    "\n",
    "    # Bands for multiband STA/LTA (v2 adds a low-frequency band)\n",
    "    # You can switch back to 3 bands by removing the first tuple\n",
    "    BANDS      : tuple = ((0.2, 1.0), (0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA windows (in seconds)\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "\n",
    "    # Trigger hysteresis: LTA OFF threshold\n",
    "    OFF        : float = 1.0\n",
    "\n",
    "    # Refractory period (seconds): will also be grid-searched\n",
    "    REFRACT    : int   = 120\n",
    "\n",
    "    # Detection-eval window (NOT used for cutting CNN windows)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300\n",
    "\n",
    "    # Search grids / budgets (expanded q/alpha and refractory)\n",
    "    GRID_Q      : tuple = (0.994, 0.995, 0.996, 0.997)\n",
    "    GRID_ALPHA  : tuple = (1.00, 1.01, 1.02, 1.03, 1.05)\n",
    "    GRID_REFRACT: tuple = (60, 90, 120)\n",
    "    ADAPT_SCOPES: tuple = (\"day\", \"hour\")  # 'hour' = hourly quantiles (better under drift)\n",
    "    FPH_BUDGET  : float = 6.0\n",
    "\n",
    "    # Second-stage min duration sweep (after picking q/alpha/refract/scope)\n",
    "    MIN_DUR_SET: tuple = (0.0, 0.5, 1.0)\n",
    "\n",
    "    # Fusion mode: \"kofn\" (default 2-of-N) or \"or_nms\" (union then NMS + refractory)\n",
    "    FUSE_MODE   : str  = \"kofn\"\n",
    "    K_FOR_KOFN  : int  = 2  # 2-of-N\n",
    "\n",
    "    # OR+NMS settings (only if FUSE_MODE == \"or_nms\")\n",
    "    NMS_SEC     : int  = 45  # minimum separation before applying refractory\n",
    "\n",
    "    OUT_DIR     : str  = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- NPZ helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    \"\"\"Return the first existing key in NPZ among candidates.\"\"\"\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    \"\"\"Load positives from NPZ and return waveforms, labels, ids, window_starts (UTC-naive).\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    \"\"\"Return train/test scopes as sets of sample_id strings.\"\"\"\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"\n",
    "    Test events = positives within TEST scope.\n",
    "    Event time (catalog-origin) = window_start + 20s (classification dataset convention).\n",
    "    \"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "# -------------------- CFT build --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    \"\"\"\n",
    "    Read one MiniSEED file; resample & detrend; then for each band:\n",
    "    - bandpass\n",
    "    - compute STA/LTA characteristic function (CFT)\n",
    "    Returns: dict(fs, t0, hours, cft_list=[cft_band1, ...])\n",
    "    \"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime) / 3600.0)\n",
    "\n",
    "    cft_list = []\n",
    "    for (fmin, fmax) in bands:\n",
    "        tr = tr0.copy()\n",
    "        tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "        cft_list.append(cft)\n",
    "\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "# -------------------- Normalizers & fusion helpers --------------------\n",
    "def to_onoff_array(x):\n",
    "    \"\"\"\n",
    "    Normalize trigger_onset output to a (N,2) int ndarray.\n",
    "    ObsPy may return a list on some versions; this makes it consistent.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(x, dtype=int)\n",
    "    if arr.size == 0:\n",
    "        return np.empty((0, 2), dtype=int)\n",
    "    # ensure shape (N,2)\n",
    "    if arr.ndim == 1:\n",
    "        if arr.shape[0] % 2 != 0:\n",
    "            raise ValueError(\"Trigger on/off array length must be even.\")\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    elif arr.shape[1] != 2:\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    return arr\n",
    "\n",
    "def fuse_onoff_kofn(onoffs, k=2):\n",
    "    \"\"\"\n",
    "    K-of-N fusion on interval level.\n",
    "    onoffs: list of arrays [[(a1,b1), (a2,b2),...], ...] in sample indices (half-open).\n",
    "    Return fused intervals where >=k bands are ON simultaneously.\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    for arr in onoffs:\n",
    "        if arr is None:\n",
    "            continue\n",
    "        for a, b in arr:\n",
    "            events.append((int(a), +1))\n",
    "            events.append((int(b), -1))\n",
    "    if not events:\n",
    "        return np.empty((0,2), dtype=int)\n",
    "    # Sort with starts before ends at the same position\n",
    "    events.sort(key=lambda x: (x[0], -x[1]))\n",
    "    fused = []\n",
    "    active = 0\n",
    "    current_start = None\n",
    "    for pos, delta in events:\n",
    "        prev = active\n",
    "        active += delta\n",
    "        if prev < k and active >= k:\n",
    "            current_start = pos\n",
    "        elif prev >= k and active < k and current_start is not None:\n",
    "            fused.append((current_start, pos))\n",
    "            current_start = None\n",
    "    return np.array(fused, dtype=int) if fused else np.empty((0,2), dtype=int)\n",
    "\n",
    "def picks_or_then_nms(onoffs, fs, t0, nms_sec=45, refract=60):\n",
    "    \"\"\"\n",
    "    OR fusion (union of all band ON starts) followed by:\n",
    "      - NMS-like temporal suppression with min spacing `nms_sec`\n",
    "      - final refractory thinning with `refract`\n",
    "    Returns a list of pandas.Timestamp.\n",
    "    \"\"\"\n",
    "    picks = []\n",
    "    for arr in onoffs:\n",
    "        if arr is None or len(arr) == 0:\n",
    "            continue\n",
    "        for a, b in arr:\n",
    "            # Use the start of each ON interval as a tentative pick\n",
    "            picks.append(t0 + a / fs)  # UTCDateTime\n",
    "    if not picks:\n",
    "        return []\n",
    "    picks.sort()\n",
    "    # NMS-style pre-merge\n",
    "    fused, last = [], None\n",
    "    for t in picks:\n",
    "        if (last is None) or ((t - last) > nms_sec):\n",
    "            fused.append(t); last = t\n",
    "    # Final refractory thinning\n",
    "    out, last = [], None\n",
    "    for t in fused:\n",
    "        if (last is None) or ((t - last) > refract):\n",
    "            out.append(pd.Timestamp(UTCDateTime(t).datetime)); last = t\n",
    "    return out\n",
    "\n",
    "def filter_min_dur(onoff, fs, min_dur_s):\n",
    "    \"\"\"Filter intervals shorter than `min_dur_s` seconds.\"\"\"\n",
    "    if onoff.size == 0 or min_dur_s <= 0:\n",
    "        return onoff\n",
    "    keep = ((onoff[:,1] - onoff[:,0]) / fs) >= float(min_dur_s)\n",
    "    return onoff[keep]\n",
    "\n",
    "def picks_from_onoff(onoff, fs, t0, refract):\n",
    "    \"\"\"\n",
    "    Convert fused intervals to pick times (use the start of each interval),\n",
    "    with refractory merging in wall-clock time.\n",
    "    \"\"\"\n",
    "    picks, last = [], None\n",
    "    for a, b in onoff:\n",
    "        t = t0 + a / fs\n",
    "        ts = pd.Timestamp(UTCDateTime(t).datetime)\n",
    "        if (last is None) or ((ts - last).total_seconds() > refract):\n",
    "            picks.append(ts); last = ts\n",
    "    return picks\n",
    "\n",
    "# -------------------- Vectorized eval helpers --------------------\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    \"\"\"Return boolean hits for each event window; a hit if any trigger ‚àà [start, end].\"\"\"\n",
    "    if trigs_ns.size == 0:\n",
    "        return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Main detection build & scan --------------------\n",
    "# 1) Test events and available days\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for dt in event_days:\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    cftd = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    cft_cache[dt] = cftd\n",
    "    total_hours += cftd[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between df_te and waveform files.\")\n",
    "\n",
    "# Detection-eval windows (use wider POST_DET=300 for evaluation)\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, \"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, \"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "print(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "\n",
    "def run_adaptive_multiband(q, alpha, min_dur_s=0.0, refract=None, adapt_scope=\"day\", fuse_mode=\"kofn\", k_for_kofn=2):\n",
    "    \"\"\"\n",
    "    For each day:\n",
    "      - per-band ON threshold via quantile(cft, q) * alpha\n",
    "        * adapt_scope = 'day' -> whole-day quantile\n",
    "        * adapt_scope = 'hour' -> per-hour quantiles (robust to drift)\n",
    "      - per-band on/off = trigger_onset(cft, ON, OFF)  (normalized to ndarray)\n",
    "      - fuse intervals:\n",
    "          * 'kofn': K-of-N interval fusion\n",
    "          * 'or_nms': OR union of all bands' starts -> NMS -> refractory\n",
    "      - filter by min_dur (interval domain only)\n",
    "      - refractory merge to picks (for 'kofn'), or already applied in 'or_nms'\n",
    "    Aggregate picks across days, then compute recall/FPH.\n",
    "    \"\"\"\n",
    "    refract = CFG.REFRACT if refract is None else int(refract)\n",
    "    all_picks = []\n",
    "    for dt, entry in cft_cache.items():\n",
    "        onoffs = []\n",
    "        for cft in entry[\"cft_list\"]:\n",
    "            if adapt_scope == \"day\":\n",
    "                if cft.size == 0:\n",
    "                    onoff = np.empty((0,2), dtype=int)\n",
    "                else:\n",
    "                    on_val = float(np.quantile(cft, q) * alpha)\n",
    "                    raw = trigger_onset(cft, on_val, CFG.OFF)\n",
    "                    onoff = to_onoff_array(raw)  # <-- normalize\n",
    "            elif adapt_scope == \"hour\":\n",
    "                fs = entry[\"fs\"]; H = int(3600 * fs)\n",
    "                segs = []\n",
    "                for s in range(0, len(cft), H):\n",
    "                    e = min(s + H, len(cft))\n",
    "                    seg = cft[s:e]\n",
    "                    if seg.size == 0:\n",
    "                        _onoff = np.empty((0, 2), dtype=int)\n",
    "                    else:\n",
    "                        on_val = float(np.quantile(seg, q) * alpha)\n",
    "                        raw = trigger_onset(seg, on_val, CFG.OFF)\n",
    "                        _onoff = to_onoff_array(raw)  # <-- normalize\n",
    "                        if _onoff.size:\n",
    "                            _onoff[:,0] += s; _onoff[:,1] += s\n",
    "                    segs.append(_onoff)\n",
    "                onoff = np.vstack(segs) if len(segs) else np.empty((0,2), dtype=int)\n",
    "            else:\n",
    "                raise ValueError(\"adapt_scope must be 'day' or 'hour'\")\n",
    "            onoffs.append(onoff)\n",
    "\n",
    "        if fuse_mode == \"kofn\":\n",
    "            fused = fuse_onoff_kofn(onoffs, k=k_for_kofn)\n",
    "            fused = filter_min_dur(fused, entry[\"fs\"], min_dur_s)\n",
    "            picks = picks_from_onoff(fused, entry[\"fs\"], entry[\"t0\"], refract)\n",
    "        elif fuse_mode == \"or_nms\":\n",
    "            # min_dur does not apply here since we use pick starts only\n",
    "            picks = picks_or_then_nms(onoffs, entry[\"fs\"], entry[\"t0\"], nms_sec=CFG.NMS_SEC, refract=refract)\n",
    "        else:\n",
    "            raise ValueError(\"fuse_mode must be 'kofn' or 'or_nms'\")\n",
    "\n",
    "        all_picks.extend(picks)\n",
    "\n",
    "    trigs_ns = np.array(sorted(all_picks), dtype=\"datetime64[ns]\")\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    return dict(q=q, alpha=alpha, min_dur=min_dur_s, recall=recall, fph=fph,\n",
    "                triggers=int(trigs_ns.size), trigs=trigs_ns, refract=refract, adapt=adapt_scope)\n",
    "\n",
    "# 2) Grid search (q, alpha, refractory, adapt_scope) under FPH budget; MIN_DUR fixed at 0.0\n",
    "rows = []\n",
    "for scope in CFG.ADAPT_SCOPES:\n",
    "    for q in CFG.GRID_Q:\n",
    "        for a in CFG.GRID_ALPHA:\n",
    "            for rf in CFG.GRID_REFRACT:\n",
    "                rows.append(run_adaptive_multiband(q, a, 0.0, refract=rf, adapt_scope=scope,\n",
    "                                                   fuse_mode=CFG.FUSE_MODE, k_for_kofn=CFG.K_FOR_KOFN))\n",
    "\n",
    "scan = pd.DataFrame([{k:v for k,v in r.items() if k!='trigs'} for r in rows])\n",
    "scan = scan.sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nTop candidates before MIN_DUR:\")\n",
    "print(scan.head(12))\n",
    "\n",
    "# Pick best recall under FPH budget (tie-breaker: lower FPH)\n",
    "cands = scan[scan[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "chosen = (cands if len(cands) else scan).sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "Q_CHOSEN, A_CHOSEN = float(chosen[\"q\"]), float(chosen[\"alpha\"])\n",
    "RF_CHOSEN, SCOPE_CHOSEN = int(chosen[\"refract\"]), str(chosen[\"adapt\"])\n",
    "print(f\"\\n[Chosen pre-MIN_DUR] q={Q_CHOSEN} alpha={A_CHOSEN} refract={RF_CHOSEN} scope={SCOPE_CHOSEN} \"\n",
    "      f\"| recall={chosen['recall']:.3f} FPH={chosen['fph']:.2f} triggers={int(chosen['triggers'])}\")\n",
    "\n",
    "# 3) Sweep MIN_DUR at the chosen (q, alpha, refractory, scope)\n",
    "rows_md = []\n",
    "res_cache = {}\n",
    "for md in CFG.MIN_DUR_SET:\n",
    "    r = run_adaptive_multiband(Q_CHOSEN, A_CHOSEN, md, refract=RF_CHOSEN, adapt_scope=SCOPE_CHOSEN,\n",
    "                               fuse_mode=CFG.FUSE_MODE, k_for_kofn=CFG.K_FOR_KOFN)\n",
    "    rows_md.append({k:v for k,v in r.items() if k!='trigs'})\n",
    "    res_cache[md] = r\n",
    "\n",
    "scan_md = pd.DataFrame(rows_md).sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nAfter MIN_DUR sweep:\")\n",
    "print(scan_md)\n",
    "\n",
    "# Choose highest recall under budget (tie-breaker: lower FPH)\n",
    "cands2 = scan_md[scan_md[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "final = (cands2 if len(cands2) else scan_md).sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "MD_CHOSEN = float(final[\"min_dur\"])\n",
    "trigs_final = res_cache[MD_CHOSEN][\"trigs\"]\n",
    "\n",
    "# 4) Save triggers CSV for cascade (encode key params in the filename)\n",
    "mode_tag = f\"{CFG.FUSE_MODE}{CFG.K_FOR_KOFN if CFG.FUSE_MODE=='kofn' else ''}\"\n",
    "csv_name = f\"triggers_MB_{mode_tag}_q{Q_CHOSEN}_a{A_CHOSEN}_rf{RF_CHOSEN}_sc{SCOPE_CHOSEN}_md{MD_CHOSEN}.csv\"\n",
    "csv_out = os.path.join(CFG.OUT_DIR, csv_name)\n",
    "\n",
    "if trigs_final.size == 0:\n",
    "    pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved EMPTY] {csv_out}\")\n",
    "else:\n",
    "    trig_ts = pd.to_datetime(trigs_final)\n",
    "    date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "    trig_df = pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "    trig_df.to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved] {csv_out} (rows={len(trig_df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9af3bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Auto-picked latest trigger CSV] runs/cascade_eval/triggers_MB_kofn2_q0.995_a1.05_rf120_scday_md0.0.csv\n",
      "[Info] events=1392 | hits=896 | hit_rate=0.644\n",
      "[Cut] windows=896 (of 896 hits) | shape=(896, 1800)\n",
      "\n",
      "== CNN (cascade on detected events) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9495    0.9261    0.9377       812\n",
      "           M     0.3737    0.4684    0.4157        79\n",
      "           L     0.0000    0.0000    0.0000         5\n",
      "\n",
      "    accuracy                         0.8806       896\n",
      "   macro avg     0.4411    0.4648    0.4511       896\n",
      "weighted avg     0.8934    0.8806    0.8864       896\n",
      "\n",
      "Confusion matrix:\n",
      " [[752  57   3]\n",
      " [ 40  37   2]\n",
      " [  0   5   0]]\n",
      "[End-to-End] S recall = 0.589 (total 1277)\n",
      "[End-to-End] M recall = 0.359 (total 103)\n",
      "[End-to-End] L recall = 0.000 (total 12)\n",
      "[Saved] runs/cascade_eval/cascade_v2_report.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cascade v2: Use multiband triggers -> re-center -> cut -> z-score -> strict CNN\n",
    "# - Wider re-centering window to stabilize alignment\n",
    "# - Global z-score from TRAIN positives (strict comparability)\n",
    "# - Optional post-softmax remapping to bump M/L recall (configurable)\n",
    "# - Robust trigger CSV resolver (auto-pick latest triggers_MB_*.csv)\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, json, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.filter import envelope\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Either set to an existing CSV, or leave as default and resolver will auto-pick latest\n",
    "    TRIG_CSV   : str = \"runs/cascade_eval/triggers_MB_kofn2_q0.997_a1.02_rf120_scday_md0.0.csv\"\n",
    "\n",
    "    # Continuous waveforms (same station/day naming)\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "    BAND_CUT   : tuple = (0.5, 8.0)  # band for cutting windows\n",
    "\n",
    "    # CNN window (must match your training: 20s pre + 70s post)\n",
    "    PRE        : int = 20\n",
    "    POST       : int = 70\n",
    "\n",
    "    # Detection-eval window (should match detection eval)\n",
    "    PRE_DET    : int = 20\n",
    "    POST_DET   : int = 300\n",
    "\n",
    "    # Wider re-centering search window around the trigger (v2 change)\n",
    "    RC_PRE     : int = 20   # was 10\n",
    "    RC_POST    : int = 40   # was 20\n",
    "\n",
    "    # Strict CNN checkpoint (baseline-comparable)\n",
    "    BEST_PT    : str = \"runs/cnn_strict/best.pt\"\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "    # Optional: post-softmax remapping to slightly raise M/L recall (keep modest)\n",
    "    REMAP_ENABLE  : bool  = False\n",
    "    REMAP_M_THRES : float = 0.35\n",
    "    REMAP_L_THRES : float = 0.25\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "LABEL_MAP = {\"S\":0, \"M\":1, \"L\":2}\n",
    "\n",
    "# ---------- Robust trigger CSV resolver ----------\n",
    "def resolve_trig_csv(cfg):\n",
    "    \"\"\"\n",
    "    Resolve the trigger CSV to use:\n",
    "      1) If cfg.TRIG_CSV is set and exists, use it.\n",
    "      2) Else, auto-pick the newest 'triggers_MB_*.csv' under cfg.OUT_DIR.\n",
    "      3) Else, try the legacy filename from your earlier run.\n",
    "      4) Else, raise a clear error listing what's available.\n",
    "    \"\"\"\n",
    "    if cfg.TRIG_CSV and os.path.exists(cfg.TRIG_CSV):\n",
    "        print(f\"[Using provided trigger CSV] {cfg.TRIG_CSV}\")\n",
    "        return cfg.TRIG_CSV\n",
    "\n",
    "    pattern = os.path.join(cfg.OUT_DIR, \"triggers_MB_*.csv\")\n",
    "    cands = sorted(glob.glob(pattern), key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    if cands:\n",
    "        print(f\"[Auto-picked latest trigger CSV] {cands[0]}\")\n",
    "        return cands[0]\n",
    "\n",
    "    legacy = os.path.join(cfg.OUT_DIR, \"triggers_MB_2of3_q0.997_a1.02_md0.0.csv\")\n",
    "    if os.path.exists(legacy):\n",
    "        print(f\"[Fallback to legacy triggers] {legacy}\")\n",
    "        return legacy\n",
    "\n",
    "    print(f\"[ERROR] No trigger CSV found.\\n\"\n",
    "          f\"- Working dir: {os.getcwd()}\\n\"\n",
    "          f\"- Looked for: {pattern}\\n\"\n",
    "          f\"- cfg.TRIG_CSV was: {cfg.TRIG_CSV}\\n\"\n",
    "          f\"Next steps:\\n\"\n",
    "          f\"  a) Run the detector first (detect_multiband_v2.py) and note the '[Saved] ...csv' path,\\n\"\n",
    "          f\"  b) Or place a triggers CSV into {cfg.OUT_DIR}.\\n\")\n",
    "    raise FileNotFoundError(\"No triggers_MB_*.csv could be found.\")\n",
    "\n",
    "# -------------------- Data helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    \"\"\"Load positives from NPZ and return waveforms, labels, ids, window_starts (UTC-naive).\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path); df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"Test events = positives in TEST scope; event_time = window_start + PRE seconds.\"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(cfg.PRE, \"s\"))\n",
    "    y_te = pd.Series(y[mte]).reset_index(drop=True)\n",
    "    order = np.argsort(evt_time.values)\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time.values[order]})\n",
    "    y_te_sorted = y_te.iloc[order].to_numpy()\n",
    "    return df_te.reset_index(drop=True), y_te_sorted\n",
    "\n",
    "def train_mean_std(cfg: Cfg):\n",
    "    \"\"\"Compute global mean/std from TRAIN positives for strict comparability.\"\"\"\n",
    "    X, y, sid, _ = load_npz_pos(cfg.NPZ_PATH)\n",
    "    tr_scope, _ = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mtr = np.isin(sid, list(tr_scope))\n",
    "    flat = X[mtr].reshape(np.sum(mtr), -1)\n",
    "    mean, std = float(flat.mean()), float(flat.std() + 1e-8)\n",
    "    with open(os.path.join(cfg.OUT_DIR, \"mean_std_from_train.json\"), \"w\") as f:\n",
    "        json.dump({\"mean\": mean, \"std\": std}, f, indent=2)\n",
    "    return mean, std\n",
    "\n",
    "# -------------------- Waveform cutting --------------------\n",
    "_trace_cache = {}\n",
    "def get_trace_for_day(cfg: Cfg, day_str: str):\n",
    "    \"\"\"Load, resample, detrend, and bandpass a day's trace; cache per day.\"\"\"\n",
    "    if day_str not in _trace_cache:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=day_str))\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        if abs(tr.stats.sampling_rate - cfg.FS) > 1e-6:\n",
    "            tr.resample(cfg.FS)\n",
    "        tr.detrend(\"demean\")\n",
    "        tr.filter(\"bandpass\", freqmin=cfg.BAND_CUT[0], freqmax=cfg.BAND_CUT[1])\n",
    "        _trace_cache[day_str] = tr\n",
    "    return _trace_cache[day_str]\n",
    "\n",
    "def recenter_trigger(tr, t_ts, fs, pre, post):\n",
    "    \"\"\"\n",
    "    Find an envelope peak near the trigger to center the CNN window.\n",
    "    v2: wider coarse search (¬±pre/post), then a fine re-peak in ¬±5 s around the coarse peak.\n",
    "    \"\"\"\n",
    "    t0 = UTCDateTime(t_ts.to_pydatetime()) - pre\n",
    "    t1 = UTCDateTime(t_ts.to_pydatetime()) + post\n",
    "    x = tr.slice(t0, t1).data.astype(np.float32, copy=False)\n",
    "    need = int((pre + post) * fs)\n",
    "    if len(x) < need:\n",
    "        return t_ts  # fallback: keep original trigger\n",
    "    env = envelope(x)\n",
    "    i = int(np.argmax(env))\n",
    "    # Fine search around the coarse peak within ¬±5 s (clamped to array bounds)\n",
    "    fine = int(5 * fs)\n",
    "    a = max(0, i - fine); b = min(len(env), i + fine + 1)\n",
    "    j = a + int(np.argmax(env[a:b]))\n",
    "    t_pk = t0 + j / fs\n",
    "    return pd.Timestamp(UTCDateTime(t_pk).datetime)\n",
    "\n",
    "# -------------------- Strict CNN (same as your training code) --------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k // 2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn   = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x); x = self.bn(x); x = F.gelu(x); x = self.pool(x); return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d): \n",
    "        super().__init__(); self.W = nn.Linear(d, d); self.v = nn.Linear(d, 1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U = torch.tanh(self.W(H)); a = self.v(U).squeeze(-1); a = torch.softmax(a, dim=1)\n",
    "        Z = torch.bmm(a.unsqueeze(1), H).squeeze(1); return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(ConvBlock(in_ch, 32), ConvBlock(32, 64), ConvBlock(64, 128))\n",
    "        self.lstm = nn.LSTM(128, hidden, num_layers=layers, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2 * hidden)\n",
    "        self.head = nn.Sequential(nn.Linear(2 * hidden, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, n_classes))\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)          # [B,C,L]\n",
    "        z = z.transpose(1, 2)    # [B,L,C]\n",
    "        H, _ = self.lstm(z)      # [B,L,2H]\n",
    "        Z, _ = self.attn(H)      # [B,2H]\n",
    "        return self.head(Z)\n",
    "\n",
    "def softmax_remap(logits, enable=False, m_th=0.35, l_th=0.25):\n",
    "    \"\"\"\n",
    "    Optional post-softmax remapping to gently bump M/L recall.\n",
    "    Only applied if `enable=True`. Keep thresholds modest to avoid harming S too much.\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(1).cpu().numpy()\n",
    "    if not enable:\n",
    "        return preds\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    pM, pL = probs[:,1], probs[:,2]\n",
    "    for n in range(len(preds)):\n",
    "        if pL[n] >= l_th:\n",
    "            preds[n] = 2\n",
    "        elif (pM[n] >= m_th) and (preds[n] == 0):\n",
    "            preds[n] = 1\n",
    "    return preds\n",
    "\n",
    "# -------------------- 1) Build test events & load triggers --------------------\n",
    "df_te, y_te_sorted = build_test_events(CFG)\n",
    "\n",
    "trig_csv_path = resolve_trig_csv(CFG)  # auto-pick if not present\n",
    "trig_df = pd.read_csv(trig_csv_path)\n",
    "if \"trigger_time\" not in trig_df.columns:\n",
    "    raise ValueError(f\"'trigger_time' column not found in {trig_csv_path}\")\n",
    "trig_df[\"trigger_time\"] = pd.to_datetime(trig_df[\"trigger_time\"], errors=\"coerce\", utc=False)\n",
    "trig_df = trig_df.sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "\n",
    "# detection-eval windows (match POST_DET=300)\n",
    "ev = df_te.copy()\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "\n",
    "# safer dtype conversion\n",
    "tr       = trig_df[\"trigger_time\"].values.astype(\"datetime64[ns]\")\n",
    "ev_start = ev[\"start\"].values.astype(\"datetime64[ns]\")\n",
    "ev_end   = ev[\"end\"].values.astype(\"datetime64[ns]\")\n",
    "\n",
    "# Vectorized hit test\n",
    "i = np.searchsorted(tr, ev_start, side=\"left\")\n",
    "j = np.searchsorted(tr, ev_end,   side=\"right\")\n",
    "hit = (j - i) > 0\n",
    "first_idx = np.where(hit, i, -1)\n",
    "\n",
    "print(f\"[Info] events={len(ev)} | hits={int(hit.sum())} | hit_rate={hit.mean():.3f}\")\n",
    "\n",
    "\n",
    "# -------------------- 2) Re-center & cut CNN windows for hits --------------------\n",
    "win_len = CFG.FS * (CFG.PRE + CFG.POST)\n",
    "X_cut, y_hit, kept = [], [], []\n",
    "for idx in np.where(hit)[0]:\n",
    "    t_first_ns = tr[first_idx[idx]]\n",
    "    t_first = pd.Timestamp(t_first_ns)\n",
    "    day_str = str(t_first.date())\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=day_str))\n",
    "    if not os.path.exists(fp): \n",
    "        continue\n",
    "    tr_day = get_trace_for_day(CFG, day_str)\n",
    "    t_center = recenter_trigger(tr_day, t_first, fs=CFG.FS, pre=CFG.RC_PRE, post=CFG.RC_POST)\n",
    "    t0 = UTCDateTime(t_center.to_pydatetime()) - CFG.PRE\n",
    "    t1 = UTCDateTime(t_center.to_pydatetime()) + CFG.POST\n",
    "    x = tr_day.slice(t0, t1).data\n",
    "    if len(x) >= win_len:\n",
    "        X_cut.append(x[:win_len].astype(np.float32))\n",
    "        y_hit.append(y_te_sorted[idx])\n",
    "        kept.append(idx)\n",
    "\n",
    "if len(X_cut) == 0:\n",
    "    raise SystemExit(\"No windows cut; check filenames or time bounds.\")\n",
    "\n",
    "X_cut = np.stack(X_cut, axis=0)\n",
    "y_hit = np.array(y_hit, dtype=int)\n",
    "print(f\"[Cut] windows={len(X_cut)} (of {int(hit.sum())} hits) | shape={X_cut.shape}\")\n",
    "\n",
    "# -------------------- 3) Global z-score (TRAIN-only mean/std) --------------------\n",
    "mean, std = train_mean_std(CFG)\n",
    "Xn = (X_cut - mean) / (std if std > 0 else 1.0)\n",
    "Xn = Xn[:, None, :]  # [N,1,T]\n",
    "\n",
    "# -------------------- 4) Load strict CNN and infer --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "model = CNNBiLSTMAttn(in_ch=1, n_classes=3).to(device)\n",
    "state = torch.load(CFG.BEST_PT, map_location=device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i0 in range(0, len(Xn), 512):\n",
    "        xb = torch.from_numpy(Xn[i0:i0+512]).to(device)\n",
    "        logits = model(xb)\n",
    "        preds = softmax_remap(logits, enable=CFG.REMAP_ENABLE, m_th=CFG.REMAP_M_THRES, l_th=CFG.REMAP_L_THRES)\n",
    "        y_pred.extend(preds)\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# -------------------- 5) Reports: within-detected & end-to-end recall --------------------\n",
    "rep = classification_report(y_hit, y_pred, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0)\n",
    "cm = confusion_matrix(y_hit, y_pred, labels=[0,1,2])\n",
    "macro_f1 = f1_score(y_hit, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n== CNN (cascade on detected events) ==\")\n",
    "print(rep)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# End-to-end per-class recall (missed detections count as errors)\n",
    "lab = y_te_sorted\n",
    "correct_mask = np.zeros_like(lab, dtype=bool)\n",
    "kept_idx = np.array(kept, dtype=int)\n",
    "\n",
    "# Mark correct classifications among detected events\n",
    "for k, idx in enumerate(kept_idx):\n",
    "    if y_hit[k] == y_pred[k]:\n",
    "        correct_mask[idx] = True\n",
    "\n",
    "for c, name in enumerate([\"S\",\"M\",\"L\"]):\n",
    "    total_c = (lab == c).sum()\n",
    "    e2e_c = correct_mask[lab == c].sum() / max(1, total_c)\n",
    "    print(f\"[End-to-End] {name} recall = {e2e_c:.3f} (total {total_c})\")\n",
    "\n",
    "# Save artifacts\n",
    "with open(os.path.join(CFG.OUT_DIR, \"cascade_v2_report.txt\"), \"w\") as f:\n",
    "    f.write(rep + \"\\n\")\n",
    "    f.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "\n",
    "np.savez(os.path.join(CFG.OUT_DIR, \"cascade_v2_preds.npz\"),\n",
    "         y_true=y_hit, y_pred=y_pred, kept_event_indices=np.array(kept, dtype=int))\n",
    "\n",
    "print(f\"[Saved] {os.path.join(CFG.OUT_DIR, 'cascade_v2_report.txt')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c6d2ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] events considered: 1392 on 17 days | hours=408.0\n",
      "\n",
      "Top candidates (pre MIN_DUR):\n",
      "        q  alpha  min_dur    recall        fph  triggers  refract adapt\n",
      "0   0.992   0.98      0.0  0.995690  32.176489     13128       60   day\n",
      "1   0.992   0.98      0.0  0.994971  32.845607     13401       60  hour\n",
      "2   0.992   1.00      0.0  0.994253  31.262273     12755       60   day\n",
      "3   0.992   1.00      0.0  0.994253  31.855411     12997       60  hour\n",
      "4   0.992   0.98      0.0  0.993534  25.426485     10374       90   day\n",
      "5   0.992   1.00      0.0  0.992816  25.169132     10269       90  hour\n",
      "6   0.992   0.98      0.0  0.992816  25.781878     10519       90  hour\n",
      "7   0.992   1.02      0.0  0.992098  30.191194     12318       60   day\n",
      "8   0.993   0.98      0.0  0.992098  30.661782     12510       60   day\n",
      "9   0.992   1.01      0.0  0.992098  30.696096     12524       60   day\n",
      "10  0.992   1.01      0.0  0.992098  31.291685     12767       60  hour\n",
      "11  0.992   1.00      0.0  0.991379  24.894622     10157       90   day\n",
      "\n",
      "[Chosen] q=0.992 alpha=0.98 refract=60 scope=day | recall=0.996 FPH=32.18 triggers=13128\n",
      "\n",
      "After MIN_DUR sweep:\n",
      "       q  alpha  min_dur   recall        fph  triggers  refract adapt\n",
      "0  0.992   0.98      0.0  0.99569  32.176489     13128       60   day\n",
      "1  0.992   0.98      0.5  0.99569  32.176489     13128       60   day\n",
      "2  0.992   0.98      1.0  0.99569  32.176489     13128       60   day\n",
      "[Saved] runs/cascade_eval/triggers_MB_ornms_q0.992_a0.98_rf60_scday_md0.0.csv (rows=13128)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multiband STA/LTA Detection v2 (OR+NMS + Hour-level thresholds)\n",
    "# - Rebuild TEST events from NPZ + frozen_splits\n",
    "# - Build daily CFTs across 4 bands (incl. 0.1‚Äì1.0 Hz for far/small events)\n",
    "# - Grid-search (q, alpha, refractory, adapt_scope) under FPH budget\n",
    "# - Fusion: OR of per-band ON-starts -> NMS -> refractory (simple & recall-friendly)\n",
    "# - Sweep MIN_DUR (kept for completeness; not used in OR mode)\n",
    "# - Save final triggers CSV for cascade\n",
    "# Notes:\n",
    "# * Compatible with ObsPy versions where trigger_onset may return list.\n",
    "# * Keeps FPH_BUDGET=6/h as your constraint.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"   # e.g., MAJO_2011-03-05.mseed\n",
    "    FS         : int = 20\n",
    "\n",
    "    # Bands (v2+: add a looser low band for far/small events)\n",
    "    BANDS      : tuple = ((0.1, 1.0), (0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA windows (seconds)\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "\n",
    "    # Hysteresis OFF threshold\n",
    "    OFF        : float = 1.0\n",
    "\n",
    "    # Default refractory (also grid-searched)\n",
    "    REFRACT    : int   = 120\n",
    "\n",
    "    # Detection-eval window (NOT used for cutting CNN windows)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300\n",
    "\n",
    "    # ---------------- Recall-friendly grid & budgets ----------------\n",
    "    # Lower q / allow alpha<=1; try short refractory; prefer hour-level quantiles\n",
    "    GRID_Q       : tuple = (0.992, 0.993, 0.994, 0.995, 0.996)\n",
    "    GRID_ALPHA   : tuple = (0.98, 1.00, 1.01, 1.02)\n",
    "    GRID_REFRACT : tuple = (60, 90, 120)\n",
    "    ADAPT_SCOPES : tuple = (\"hour\", \"day\")   # try hour first; day as fallback\n",
    "    FPH_BUDGET   : float = 6.0\n",
    "\n",
    "    # MIN_DUR kept for completeness (affects K-of-N only; OR+NMS ignores it)\n",
    "    MIN_DUR_SET  : tuple = (0.0, 0.5, 1.0)\n",
    "\n",
    "    # ---------------- Fusion (set to OR+NMS) ----------------\n",
    "    FUSE_MODE    : str  = \"or_nms\"\n",
    "    NMS_SEC      : int  = 30   # tighter NMS to admit more weak picks under budget\n",
    "    K_FOR_KOFN   : int  = 2    # unused when FUSE_MODE=\"or_nms\"\n",
    "\n",
    "    OUT_DIR      : str  = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- NPZ helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    \"\"\"Return the first existing key in NPZ among candidates.\"\"\"\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    \"\"\"Load positives from NPZ and return waveforms, labels, ids, window_starts (UTC-naive).\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    \"\"\"Return train/test scopes as sets of sample_id strings.\"\"\"\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))  # <-- FIXED here\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"\n",
    "    Test events = positives within TEST scope.\n",
    "    Event time (catalog-origin) = window_start + 20s (classification dataset convention).\n",
    "    \"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "# -------------------- CFT build --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    \"\"\"\n",
    "    Read one MiniSEED file; resample & detrend; then for each band:\n",
    "    - bandpass\n",
    "    - compute STA/LTA characteristic function (CFT)\n",
    "    Returns: dict(fs, t0, hours, cft_list=[cft_band1, ...])\n",
    "    \"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime) / 3600.0)\n",
    "\n",
    "    cft_list = []\n",
    "    for (fmin, fmax) in bands:\n",
    "        tr = tr0.copy()\n",
    "        tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "        cft_list.append(cft)\n",
    "\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "# -------------------- Normalizers & fusion helpers --------------------\n",
    "def to_onoff_array(x):\n",
    "    \"\"\"\n",
    "    Normalize trigger_onset output to a (N,2) int ndarray.\n",
    "    ObsPy may return a list on some versions; this makes it consistent.\n",
    "    \"\"\"\n",
    "    arr = np.asarray(x, dtype=int)\n",
    "    if arr.size == 0:\n",
    "        return np.empty((0, 2), dtype=int)\n",
    "    if arr.ndim == 1:\n",
    "        if arr.shape[0] % 2 != 0:\n",
    "            raise ValueError(\"Trigger on/off array length must be even.\")\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    elif arr.shape[1] != 2:\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    return arr\n",
    "\n",
    "def picks_or_then_nms(onoffs, fs, t0, nms_sec=30, refract=60):\n",
    "    \"\"\"\n",
    "    OR fusion (union of all band ON starts) followed by:\n",
    "      - NMS-like suppression with min spacing `nms_sec`\n",
    "      - final refractory thinning with `refract`\n",
    "    Returns a list of pandas.Timestamp.\n",
    "    \"\"\"\n",
    "    picks = []\n",
    "    for arr in onoffs:\n",
    "        if arr is None or len(arr) == 0:\n",
    "            continue\n",
    "        for a, b in arr:\n",
    "            picks.append(t0 + a / fs)  # UTCDateTime\n",
    "    if not picks:\n",
    "        return []\n",
    "    picks.sort()\n",
    "    # NMS\n",
    "    fused, last = [], None\n",
    "    for t in picks:\n",
    "        if (last is None) or ((t - last) > nms_sec):\n",
    "            fused.append(t); last = t\n",
    "    # Refractory\n",
    "    out, last = [], None\n",
    "    for t in fused:\n",
    "        if (last is None) or ((t - last) > refract):\n",
    "            out.append(pd.Timestamp(UTCDateTime(t).datetime)); last = t\n",
    "    return out\n",
    "\n",
    "def filter_min_dur(onoff, fs, min_dur_s):\n",
    "    \"\"\"Only used in K-of-N mode; kept for completeness.\"\"\"\n",
    "    if onoff.size == 0 or min_dur_s <= 0:\n",
    "        return onoff\n",
    "    keep = ((onoff[:,1] - onoff[:,0]) / fs) >= float(min_dur_s)\n",
    "    return onoff[keep]\n",
    "\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    \"\"\"Return boolean hits for each event window; a hit if any trigger ‚àà [start, end].\"\"\"\n",
    "    if trigs_ns.size == 0:\n",
    "        return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Main detection build & scan --------------------\n",
    "# 1) Test events and available days\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for dt in event_days:\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    cftd = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    cft_cache[dt] = cftd\n",
    "    total_hours += cftd[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between df_te and waveform files.\")\n",
    "\n",
    "# Detection-eval windows\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "print(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "\n",
    "def run_adaptive_multiband(q, alpha, min_dur_s=0.0, refract=None, adapt_scope=\"hour\"):\n",
    "    \"\"\"\n",
    "    For each day:\n",
    "      - per-band ON threshold via quantile(cft, q) * alpha\n",
    "        * adapt_scope = 'hour' -> per-hour quantiles (robust to drift)\n",
    "        * adapt_scope = 'day'  -> whole-day quantile (fallback)\n",
    "      - per-band on/off = trigger_onset(cft, ON, OFF)  (normalized to ndarray)\n",
    "      - OR fusion of ON starts -> NMS -> refractory to get picks\n",
    "    Aggregate picks across days, then compute recall/FPH.\n",
    "    \"\"\"\n",
    "    refract = CFG.REFRACT if refract is None else int(refract)\n",
    "    all_picks = []\n",
    "    for dt, entry in cft_cache.items():\n",
    "        onoffs = []\n",
    "        for cft in entry[\"cft_list\"]:\n",
    "            if adapt_scope == \"hour\":\n",
    "                fs = entry[\"fs\"]; H = int(3600 * fs)\n",
    "                segs = []\n",
    "                for s in range(0, len(cft), H):\n",
    "                    e = min(s + H, len(cft))\n",
    "                    seg = cft[s:e]\n",
    "                    if seg.size == 0:\n",
    "                        _onoff = np.empty((0, 2), dtype=int)\n",
    "                    else:\n",
    "                        on_val = float(np.quantile(seg, q) * alpha)\n",
    "                        raw = trigger_onset(seg, on_val, CFG.OFF)\n",
    "                        _onoff = to_onoff_array(raw)\n",
    "                        if _onoff.size:\n",
    "                            _onoff[:,0] += s; _onoff[:,1] += s\n",
    "                    segs.append(_onoff)\n",
    "                onoff = np.vstack(segs) if len(segs) else np.empty((0,2), dtype=int)\n",
    "            elif adapt_scope == \"day\":\n",
    "                if cft.size == 0:\n",
    "                    onoff = np.empty((0,2), dtype=int)\n",
    "                else:\n",
    "                    on_val = float(np.quantile(cft, q) * alpha)\n",
    "                    raw = trigger_onset(cft, on_val, CFG.OFF)\n",
    "                    onoff = to_onoff_array(raw)\n",
    "            else:\n",
    "                raise ValueError(\"adapt_scope must be 'hour' or 'day'\")\n",
    "            onoffs.append(onoff)\n",
    "\n",
    "        # OR+NMS fusion (recall-friendly)\n",
    "        picks = picks_or_then_nms(onoffs, entry[\"fs\"], entry[\"t0\"], nms_sec=CFG.NMS_SEC, refract=refract)\n",
    "        all_picks.extend(picks)\n",
    "\n",
    "    trigs_ns = np.array(sorted(all_picks), dtype=\"datetime64[ns]\")\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    return dict(q=q, alpha=alpha, min_dur=min_dur_s, recall=recall, fph=fph,\n",
    "                triggers=int(trigs_ns.size), trigs=trigs_ns, refract=refract, adapt=adapt_scope)\n",
    "\n",
    "# 2) Grid search (q, alpha, refractory, adapt_scope) under FPH budget\n",
    "rows = []\n",
    "for scope in CFG.ADAPT_SCOPES:\n",
    "    for q in CFG.GRID_Q:\n",
    "        for a in CFG.GRID_ALPHA:\n",
    "            for rf in CFG.GRID_REFRACT:\n",
    "                rows.append(run_adaptive_multiband(q, a, 0.0, refract=rf, adapt_scope=scope))\n",
    "\n",
    "scan = pd.DataFrame([{k:v for k,v in r.items() if k!='trigs'} for r in rows]) \\\n",
    "          .sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nTop candidates (pre MIN_DUR):\")\n",
    "print(scan.head(12))\n",
    "\n",
    "# pick best recall under budget (tie-breaker: lower FPH)\n",
    "cands = scan[scan[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "chosen = (cands if len(cands) else scan).sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "Q_CHOSEN, A_CHOSEN = float(chosen[\"q\"]), float(chosen[\"alpha\"])\n",
    "RF_CHOSEN, SCOPE_CHOSEN = int(chosen[\"refract\"]), str(chosen[\"adapt\"])\n",
    "print(f\"\\n[Chosen] q={Q_CHOSEN} alpha={A_CHOSEN} refract={RF_CHOSEN} scope={SCOPE_CHOSEN} \"\n",
    "      f\"| recall={chosen['recall']:.3f} FPH={chosen['fph']:.2f} triggers={int(chosen['triggers'])}\")\n",
    "\n",
    "# 3) MIN_DUR sweep (no effect in OR mode; keep for API parity)\n",
    "rows_md, res_cache = [], {}\n",
    "for md in CFG.MIN_DUR_SET:\n",
    "    r = run_adaptive_multiband(Q_CHOSEN, A_CHOSEN, md, refract=RF_CHOSEN, adapt_scope=SCOPE_CHOSEN)\n",
    "    rows_md.append({k:v for k,v in r.items() if k!='trigs'})\n",
    "    res_cache[md] = r\n",
    "\n",
    "scan_md = pd.DataFrame(rows_md).sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "print(\"\\nAfter MIN_DUR sweep:\")\n",
    "print(scan_md)\n",
    "\n",
    "# choose highest recall under budget (tie-breaker: lower FPH)\n",
    "cands2 = scan_md[scan_md[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "final = (cands2 if len(cands2) else scan_md).sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "MD_CHOSEN = float(final[\"min_dur\"])\n",
    "trigs_final = res_cache[MD_CHOSEN][\"trigs\"]\n",
    "\n",
    "# 4) Save triggers CSV for cascade (encode key params in filename)\n",
    "mode_tag = \"ornms\"\n",
    "csv_name = f\"triggers_MB_{mode_tag}_q{Q_CHOSEN}_a{A_CHOSEN}_rf{RF_CHOSEN}_sc{SCOPE_CHOSEN}_md{MD_CHOSEN}.csv\"\n",
    "csv_out = os.path.join(CFG.OUT_DIR, csv_name)\n",
    "\n",
    "if trigs_final.size == 0:\n",
    "    pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved EMPTY] {csv_out}\")\n",
    "else:\n",
    "    trig_ts = pd.to_datetime(trigs_final)\n",
    "    date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "    trig_df = pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "    trig_df.to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved] {csv_out} (rows={len(trig_df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6f0e34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Auto-picked latest trigger CSV] runs/cascade_eval/triggers_MB_ornms_q0.992_a0.98_rf60_scday_md0.0.csv\n",
      "[Info] events=1392 | hits=1386 | hit_rate=0.996\n",
      "[Cut] windows=1386 (of 1386 hits) | shape=(1386, 1800)\n",
      "\n",
      "== CNN (cascade on detected events) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9620    0.9552    0.9586      1272\n",
      "           M     0.4530    0.5196    0.4840       102\n",
      "           L     0.5000    0.2500    0.3333        12\n",
      "\n",
      "    accuracy                         0.9170      1386\n",
      "   macro avg     0.6383    0.5749    0.5920      1386\n",
      "weighted avg     0.9205    0.9170    0.9182      1386\n",
      "\n",
      "Confusion matrix:\n",
      " [[1215   55    2]\n",
      " [  48   53    1]\n",
      " [   0    9    3]]\n",
      "[End-to-End] S recall = 0.951 (total 1277)\n",
      "[End-to-End] M recall = 0.515 (total 103)\n",
      "[End-to-End] L recall = 0.250 (total 12)\n",
      "[Saved] runs/cascade_eval/cascade_v2_report.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cascade v2: Use multiband triggers -> re-center -> cut -> z-score -> strict CNN\n",
    "# - Wider re-centering window to stabilize alignment\n",
    "# - Global z-score from TRAIN positives (strict comparability)\n",
    "# - Optional post-softmax remapping (disabled by default)\n",
    "# - Robust trigger CSV resolver (auto-pick latest triggers_MB_*.csv)\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, json, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.filter import envelope\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Leave as-is; resolver will pick the newest 'triggers_MB_*.csv' in OUT_DIR\n",
    "    TRIG_CSV   : str = \"runs/cascade_eval/triggers_MB_ornms_q0.995_a1.02_rf90_schour_md0.0.csv\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "    BAND_CUT   : tuple = (0.5, 8.0)  # band for cutting windows\n",
    "\n",
    "    # CNN window (must match training)\n",
    "    PRE        : int = 20\n",
    "    POST       : int = 70\n",
    "\n",
    "    # Detection-eval window (should match detection eval)\n",
    "    PRE_DET    : int = 20\n",
    "    POST_DET   : int = 300\n",
    "\n",
    "    # Wider re-centering window\n",
    "    RC_PRE     : int = 20\n",
    "    RC_POST    : int = 40\n",
    "\n",
    "    # Strict CNN checkpoint\n",
    "    BEST_PT    : str = \"runs/cnn_strict/best.pt\"\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "    # Optional: post-softmax remapping (disabled by default)\n",
    "    REMAP_ENABLE  : bool  = False\n",
    "    REMAP_M_THRES : float = 0.33\n",
    "    REMAP_L_THRES : float = 0.20\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "LABEL_MAP = {\"S\":0, \"M\":1, \"L\":2}\n",
    "\n",
    "# ---------- Robust trigger CSV resolver ----------\n",
    "def resolve_trig_csv(cfg):\n",
    "    \"\"\"\n",
    "    Resolve the trigger CSV to use:\n",
    "      1) If cfg.TRIG_CSV exists, use it.\n",
    "      2) Else, pick the newest 'triggers_MB_*.csv' under cfg.OUT_DIR.\n",
    "      3) Else, try legacy filename from earlier v1.\n",
    "      4) Else, raise a clear error.\n",
    "    \"\"\"\n",
    "    if cfg.TRIG_CSV and os.path.exists(cfg.TRIG_CSV):\n",
    "        print(f\"[Using provided trigger CSV] {cfg.TRIG_CSV}\")\n",
    "        return cfg.TRIG_CSV\n",
    "\n",
    "    pattern = os.path.join(cfg.OUT_DIR, \"triggers_MB_*.csv\")\n",
    "    cands = sorted(glob.glob(pattern), key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    if cands:\n",
    "        print(f\"[Auto-picked latest trigger CSV] {cands[0]}\")\n",
    "        return cands[0]\n",
    "\n",
    "    legacy = os.path.join(cfg.OUT_DIR, \"triggers_MB_2of3_q0.997_a1.02_md0.0.csv\")\n",
    "    if os.path.exists(legacy):\n",
    "        print(f\"[Fallback to legacy triggers] {legacy}\")\n",
    "        return legacy\n",
    "\n",
    "    print(f\"[ERROR] No trigger CSV found.\\n\"\n",
    "          f\"- Working dir: {os.getcwd()}\\n\"\n",
    "          f\"- Looked for: {pattern}\\n\"\n",
    "          f\"- cfg.TRIG_CSV was: {cfg.TRIG_CSV}\\n\"\n",
    "          f\"Next steps:\\n\"\n",
    "          f\"  a) Run the detector (detect_multiband_v2.py) and note the '[Saved] ...csv' path,\\n\"\n",
    "          f\"  b) Or place a triggers CSV into {cfg.OUT_DIR}.\\n\")\n",
    "    raise FileNotFoundError(\"No triggers_MB_*.csv could be found.\")\n",
    "\n",
    "# -------------------- Data helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path); df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(cfg.PRE, \"s\"))\n",
    "    y_te = pd.Series(y[mte]).reset_index(drop=True)\n",
    "    order = np.argsort(evt_time.values)\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time.values[order]})\n",
    "    y_te_sorted = y_te.iloc[order].to_numpy()\n",
    "    return df_te.reset_index(drop=True), y_te_sorted\n",
    "\n",
    "def train_mean_std(cfg: Cfg):\n",
    "    X, y, sid, _ = load_npz_pos(cfg.NPZ_PATH)\n",
    "    tr_scope, _ = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mtr = np.isin(sid, list(tr_scope))\n",
    "    flat = X[mtr].reshape(np.sum(mtr), -1)\n",
    "    mean, std = float(flat.mean()), float(flat.std() + 1e-8)\n",
    "    with open(os.path.join(cfg.OUT_DIR, \"mean_std_from_train.json\"), \"w\") as f:\n",
    "        json.dump({\"mean\": mean, \"std\": std}, f, indent=2)\n",
    "    return mean, std\n",
    "\n",
    "# -------------------- Waveform IO  --------------------\n",
    "_trace_cache = {}\n",
    "def get_trace_for_day(cfg: Cfg, day_str: str):\n",
    "    if day_str not in _trace_cache:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=day_str))\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        if abs(tr.stats.sampling_rate - cfg.FS) > 1e-6:\n",
    "            tr.resample(cfg.FS)\n",
    "        tr.detrend(\"demean\")\n",
    "        tr.filter(\"bandpass\", freqmin=cfg.BAND_CUT[0], freqmax=cfg.BAND_CUT[1])\n",
    "        _trace_cache[day_str] = tr\n",
    "    return _trace_cache[day_str]\n",
    "\n",
    "def recenter_trigger(tr, t_ts, fs, pre, post):\n",
    "    \"\"\"\n",
    "    Find an envelope peak near the trigger to center the CNN window.\n",
    "    Wider coarse search (¬±pre/post), plus a fine re-peak in ¬±5 s window.\n",
    "    \"\"\"\n",
    "    t0 = UTCDateTime(t_ts.to_pydatetime()) - pre\n",
    "    t1 = UTCDateTime(t_ts.to_pydatetime()) + post\n",
    "    x = tr.slice(t0, t1).data.astype(np.float32, copy=False)\n",
    "    need = int((pre + post) * fs)\n",
    "    if len(x) < need:\n",
    "        return t_ts\n",
    "    env = envelope(x)\n",
    "    i = int(np.argmax(env))\n",
    "    fine = int(5 * fs)\n",
    "    a = max(0, i - fine); b = min(len(env), i + fine + 1)\n",
    "    j = a + int(np.argmax(env[a:b]))\n",
    "    t_pk = t0 + j / fs\n",
    "    return pd.Timestamp(UTCDateTime(t_pk).datetime)\n",
    "\n",
    "# -------------------- Strict CNN --------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k // 2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn   = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x); x = self.bn(x); x = F.gelu(x); x = self.pool(x); return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d): \n",
    "        super().__init__(); self.W = nn.Linear(d, d); self.v = nn.Linear(d, 1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U = torch.tanh(self.W(H)); a = self.v(U).squeeze(-1); a = torch.softmax(a, dim=1)\n",
    "        Z = torch.bmm(a.unsqueeze(1), H).squeeze(1); return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(ConvBlock(in_ch, 32), ConvBlock(32, 64), ConvBlock(64, 128))\n",
    "        self.lstm = nn.LSTM(128, hidden, num_layers=layers, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2 * hidden)\n",
    "        self.head = nn.Sequential(nn.Linear(2 * hidden, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, n_classes))\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)          # [B,C,L]\n",
    "        z = z.transpose(1, 2)    # [B,L,C]\n",
    "        H, _ = self.lstm(z)      # [B,L,2H]\n",
    "        Z, _ = self.attn(H)      # [B,2H]\n",
    "        return self.head(Z)\n",
    "\n",
    "def softmax_remap(logits, enable=False, m_th=0.33, l_th=0.20):\n",
    "    preds = logits.argmax(1).cpu().numpy()\n",
    "    if not enable:\n",
    "        return preds\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    pM, pL = probs[:,1], probs[:,2]\n",
    "    for n in range(len(preds)):\n",
    "        if pL[n] >= l_th:\n",
    "            preds[n] = 2\n",
    "        elif (pM[n] >= m_th) and (preds[n] == 0):\n",
    "            preds[n] = 1\n",
    "    return preds\n",
    "\n",
    "# -------------------- 1) Build test events & load triggers --------------------\n",
    "df_te, y_te_sorted = build_test_events(CFG)\n",
    "\n",
    "trig_csv_path = resolve_trig_csv(CFG)  # auto-pick if provided path missing\n",
    "trig_df = pd.read_csv(trig_csv_path)\n",
    "if \"trigger_time\" not in trig_df.columns:\n",
    "    raise ValueError(f\"'trigger_time' column not found in {trig_csv_path}\")\n",
    "trig_df[\"trigger_time\"] = pd.to_datetime(trig_df[\"trigger_time\"], errors=\"coerce\", utc=False)\n",
    "trig_df = trig_df.sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "\n",
    "# detection-eval windows (match POST_DET=300)\n",
    "ev = df_te.copy()\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "\n",
    "# safe dtype conversion\n",
    "tr       = trig_df[\"trigger_time\"].values.astype(\"datetime64[ns]\")\n",
    "ev_start = ev[\"start\"].values.astype(\"datetime64[ns]\")\n",
    "ev_end   = ev[\"end\"].values.astype(\"datetime64[ns]\")\n",
    "\n",
    "# Vectorized hit test\n",
    "i = np.searchsorted(tr, ev_start, side=\"left\")\n",
    "j = np.searchsorted(tr, ev_end,   side=\"right\")\n",
    "hit = (j - i) > 0\n",
    "first_idx = np.where(hit, i, -1)\n",
    "\n",
    "print(f\"[Info] events={len(ev)} | hits={int(hit.sum())} | hit_rate={hit.mean():.3f}\")\n",
    "\n",
    "# -------------------- 2) Re-center & cut CNN windows for hits --------------------\n",
    "win_len = CFG.FS * (CFG.PRE + CFG.POST)\n",
    "X_cut, y_hit, kept = [], [], []\n",
    "for idx in np.where(hit)[0]:\n",
    "    t_first_ns = tr[first_idx[idx]]\n",
    "    t_first = pd.Timestamp(t_first_ns)\n",
    "    day_str = str(t_first.date())\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=day_str))\n",
    "    if not os.path.exists(fp): \n",
    "        continue\n",
    "    tr_day = get_trace_for_day(CFG, day_str)\n",
    "    t_center = recenter_trigger(tr_day, t_first, fs=CFG.FS, pre=CFG.RC_PRE, post=CFG.RC_POST)\n",
    "    t0 = UTCDateTime(t_center.to_pydatetime()) - CFG.PRE\n",
    "    t1 = UTCDateTime(t_center.to_pydatetime()) + CFG.POST\n",
    "    x = tr_day.slice(t0, t1).data\n",
    "    if len(x) >= win_len:\n",
    "        X_cut.append(x[:win_len].astype(np.float32))\n",
    "        y_hit.append(y_te_sorted[idx])\n",
    "        kept.append(idx)\n",
    "\n",
    "if len(X_cut) == 0:\n",
    "    raise SystemExit(\"No windows cut; check filenames or time bounds.\")\n",
    "\n",
    "X_cut = np.stack(X_cut, axis=0)\n",
    "y_hit = np.array(y_hit, dtype=int)\n",
    "print(f\"[Cut] windows={len(X_cut)} (of {int(hit.sum())} hits) | shape={X_cut.shape}\")\n",
    "\n",
    "# -------------------- 3) Global z-score (TRAIN-only mean/std) --------------------\n",
    "mean, std = train_mean_std(CFG)\n",
    "Xn = (X_cut - mean) / (std if std > 0 else 1.0)\n",
    "Xn = Xn[:, None, :]  # [N,1,T]\n",
    "\n",
    "# -------------------- 4) Load strict CNN and infer --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "model = CNNBiLSTMAttn(in_ch=1, n_classes=3).to(device)\n",
    "state = torch.load(CFG.BEST_PT, map_location=device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i0 in range(0, len(Xn), 512):\n",
    "        xb = torch.from_numpy(Xn[i0:i0+512]).to(device)\n",
    "        logits = model(xb)\n",
    "        preds = softmax_remap(logits, enable=CFG.REMAP_ENABLE, m_th=CFG.REMAP_M_THRES, l_th=CFG.REMAP_L_THRES)\n",
    "        y_pred.extend(preds)\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# -------------------- 5) Reports --------------------\n",
    "rep = classification_report(y_hit, y_pred, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0)\n",
    "cm = confusion_matrix(y_hit, y_pred, labels=[0,1,2])\n",
    "macro_f1 = f1_score(y_hit, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n== CNN (cascade on detected events) ==\")\n",
    "print(rep)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# End-to-end per-class recall\n",
    "lab = y_te_sorted\n",
    "correct_mask = np.zeros_like(lab, dtype=bool)\n",
    "kept_idx = np.array(kept, dtype=int)\n",
    "for k, idx in enumerate(kept_idx):\n",
    "    if y_hit[k] == y_pred[k]:\n",
    "        correct_mask[idx] = True\n",
    "\n",
    "for c, name in enumerate([\"S\",\"M\",\"L\"]):\n",
    "    total_c = (lab == c).sum()\n",
    "    e2e_c = correct_mask[lab == c].sum() / max(1, total_c)\n",
    "    print(f\"[End-to-End] {name} recall = {e2e_c:.3f} (total {total_c})\")\n",
    "\n",
    "# Save artifacts\n",
    "with open(os.path.join(CFG.OUT_DIR, \"cascade_v2_report.txt\"), \"w\") as f:\n",
    "    f.write(rep + \"\\n\")\n",
    "    f.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "\n",
    "np.savez(os.path.join(CFG.OUT_DIR, \"cascade_v2_preds.npz\"),\n",
    "         y_true=y_hit, y_pred=y_pred, kept_event_indices=np.array(kept, dtype=int))\n",
    "\n",
    "print(f\"[Saved] {os.path.join(CFG.OUT_DIR, 'cascade_v2_report.txt')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62235c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Auto-picked latest trigger CSV] runs/cascade_eval/triggers_MB_ornms_q0.992_a0.98_rf60_scday_md0.0.csv\n",
      "\n",
      "================= FPH SUMMARY =================\n",
      "Triggers CSV: runs/cascade_eval/triggers_MB_ornms_q0.992_a0.98_rf60_scday_md0.0.csv\n",
      "Days considered: 17  |  Total hours: 408.000\n",
      "Total triggers (used): 13128  |  Overall FPH: 32.176 per hour\n",
      "===============================================\n",
      "\n",
      "Top-10 days by FPH:\n",
      "      date  triggers     hours       fph\n",
      "2011-03-01       939 23.999986 39.125023\n",
      "2011-03-03       938 23.999986 39.083356\n",
      "2011-03-04       936 23.999986 39.000023\n",
      "2011-03-07       914 23.999986 38.083355\n",
      "2011-03-05       911 23.999986 37.958355\n",
      "2011-03-10       887 23.999986 36.958355\n",
      "2011-03-09       828 23.999986 34.500020\n",
      "2011-03-19       738 23.999986 30.750018\n",
      "2011-03-17       733 23.999986 30.541684\n",
      "2011-03-18       714 23.999986 29.750017\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FPH Checker for Multiband STA/LTA Triggers\n",
    "# - Rebuild the exact day set used by detection:\n",
    "#   days = (unique test event dates) ‚à© (dates with available mseed files)\n",
    "# - Read triggers CSV (auto-pick latest if not provided), filter to those days\n",
    "# - Compute total_hours from mseed coverage and FPH = triggers / total_hours\n",
    "# - Print overall summary and per-day breakdown\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, json\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts (same as detector)\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"   # e.g., MAJO_2011-03-05.mseed\n",
    "\n",
    "    # Optional: specify a concrete triggers CSV; if missing, auto-pick the latest triggers_MB_*.csv in OUT_DIR\n",
    "    TRIG_CSV   : str = \"\"   # e.g., \"runs/cascade_eval/triggers_MB_ornms_q0.992_a0.98_rf60_scday_md0.0.csv\"\n",
    "\n",
    "    # Where trigger CSVs live\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "\n",
    "# -------------------- NPZ helpers (same logic as your detector) --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]  # not used here, but keep consistent\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_event_days(cfg: Cfg):\n",
    "    \"\"\"Return sorted unique dates of TEST positives (window_start + 20s by convention).\"\"\"\n",
    "    _, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    return sorted(set(pd.to_datetime(evt_time).dt.date))\n",
    "\n",
    "# -------------------- Trigger CSV resolver --------------------\n",
    "def resolve_trig_csv(cfg: Cfg):\n",
    "    if cfg.TRIG_CSV and os.path.exists(cfg.TRIG_CSV):\n",
    "        print(f\"[Using provided trigger CSV] {cfg.TRIG_CSV}\")\n",
    "        return cfg.TRIG_CSV\n",
    "    pattern = os.path.join(cfg.OUT_DIR, \"triggers_MB_*.csv\")\n",
    "    cands = sorted(glob.glob(pattern), key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trigger CSV found in {cfg.OUT_DIR} matching 'triggers_MB_*.csv'.\")\n",
    "    print(f\"[Auto-picked latest trigger CSV] {cands[0]}\")\n",
    "    return cands[0]\n",
    "\n",
    "# -------------------- Hours calculator --------------------\n",
    "def compute_hours_for_days(cfg: Cfg, days):\n",
    "    \"\"\"\n",
    "    For each day in `days`, if the corresponding mseed exists,\n",
    "    read it and compute coverage hours as (endtime - starttime)/3600.\n",
    "    Return: (per_day_hours: dict[date->hours], total_hours: float, days_ok: list[date])\n",
    "    \"\"\"\n",
    "    per_day_hours = {}\n",
    "    days_ok = []\n",
    "    total_hours = 0.0\n",
    "    for dt in days:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=dt))\n",
    "        if not os.path.exists(fp):\n",
    "            continue\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        hours = float((tr.stats.endtime - tr.stats.starttime) / 3600.0)\n",
    "        per_day_hours[dt] = hours\n",
    "        total_hours += hours\n",
    "        days_ok.append(dt)\n",
    "    return per_day_hours, total_hours, days_ok\n",
    "\n",
    "# -------------------- FPH checker --------------------\n",
    "def check_fph(cfg: Cfg):\n",
    "    # 1) Day set used in detection (events ‚à© available mseed days)\n",
    "    event_days = build_test_event_days(cfg)\n",
    "    per_day_hours, total_hours, days_ok = compute_hours_for_days(cfg, event_days)\n",
    "    if not days_ok:\n",
    "        raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "\n",
    "    # 2) Load triggers and keep only those on days_ok (consistent denominator)\n",
    "    trig_csv = resolve_trig_csv(cfg)\n",
    "    df = pd.read_csv(trig_csv)\n",
    "    if \"trigger_time\" not in df.columns:\n",
    "        raise ValueError(f\"'trigger_time' column not found in {trig_csv}\")\n",
    "    df[\"trigger_time\"] = pd.to_datetime(df[\"trigger_time\"], errors=\"coerce\", utc=False)\n",
    "    df = df.dropna(subset=[\"trigger_time\"]).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "    df[\"date\"] = df[\"trigger_time\"].dt.date\n",
    "\n",
    "    n_total_trigs = len(df)\n",
    "    df = df[df[\"date\"].isin(days_ok)].copy()\n",
    "    n_used_trigs = len(df)\n",
    "    n_dropped = n_total_trigs - n_used_trigs\n",
    "    if n_dropped > 0:\n",
    "        print(f\"[Note] Dropped {n_dropped} triggers outside the detection day set (kept {n_used_trigs}).\")\n",
    "\n",
    "    # 3) Overall FPH\n",
    "    if total_hours <= 0:\n",
    "        raise ValueError(\"Total hours computed as zero; check your mseed files.\")\n",
    "    fph = n_used_trigs / total_hours\n",
    "\n",
    "    # 4) Per-day breakdown\n",
    "    per_day = df.groupby(\"date\")[\"trigger_time\"].count().rename(\"triggers\").reset_index()\n",
    "    per_day[\"hours\"] = per_day[\"date\"].map(per_day_hours).astype(float)\n",
    "    per_day[\"fph\"] = per_day[\"triggers\"] / per_day[\"hours\"].replace(0, np.nan)\n",
    "\n",
    "    # 5) Print summary\n",
    "    print(\"\\n================= FPH SUMMARY =================\")\n",
    "    print(f\"Triggers CSV: {trig_csv}\")\n",
    "    print(f\"Days considered: {len(days_ok)}  |  Total hours: {total_hours:.3f}\")\n",
    "    print(f\"Total triggers (used): {n_used_trigs}  |  Overall FPH: {fph:.3f} per hour\")\n",
    "    if n_dropped > 0:\n",
    "        print(f\"(* {n_dropped} triggers were outside considered days and excluded from FPH.)\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    # 6) Show top-10 highest-FPH days (if you want to spot outliers)\n",
    "    if len(per_day) > 0:\n",
    "        per_day_sorted = per_day.sort_values(\"fph\", ascending=False)\n",
    "        print(\"\\nTop-10 days by FPH:\")\n",
    "        print(per_day_sorted.head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No triggers within the considered day set; FPH = 0.\")\n",
    "\n",
    "    return dict(fph=fph, total_hours=total_hours, n_trigs=n_used_trigs, per_day=per_day)\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    _ = check_fph(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e19e42e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] events considered: 1392 on 17 days | hours=408.0\n",
      "\n",
      "Top candidates (by recall then lower FPH):\n",
      "        q  alpha  refract  nms_sec adapt    recall        fph  triggers\n",
      "0   0.993   1.00       90       90  hour  0.985632  25.151975     10262\n",
      "1   0.993   1.00       90       90   day  0.984914  24.649524     10057\n",
      "2   0.993   1.01       90       90  hour  0.984914  24.730406     10090\n",
      "3   0.993   1.01       90       90   day  0.981322  24.252465      9895\n",
      "4   0.993   1.02       90       90  hour  0.981322  24.296583      9913\n",
      "5   0.993   1.00       90       60   day  0.980603  21.681385      8846\n",
      "6   0.993   1.00       90      120   day  0.979885  20.490208      8360\n",
      "7   0.993   1.00      120      120   day  0.979885  20.490208      8360\n",
      "8   0.993   1.00       90       60  hour  0.979885  21.953444      8957\n",
      "9   0.993   1.02       90       90   day  0.979885  23.840700      9727\n",
      "10  0.993   1.01       90       60  hour  0.978448  21.654424      8835\n",
      "11  0.993   1.00      120       60   day  0.977011  18.813736      7676\n",
      "\n",
      "[WARNING] No config met FPH budget; choosing lowest-FPH candidate q=0.996 alpha=1.02 refract=180 nms=120 scope=day | recall=0.815 FPH=11.054 triggers=4510\n",
      "[Saved] runs/cascade_eval/triggers_MB_ornms_q0.996_a1.02_rf180_nms120_scday.csv (rows=4510)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multiband STA/LTA Detection v2 ‚Äî OR+NMS with FPH-aware search\n",
    "# - Adds GRID_NMS_SEC (NMS window) to the search\n",
    "# - Picks best recall UNDER the FPH budget; if none meet it,\n",
    "#   falls back to the lowest-FPH candidate and prints a warning\n",
    "# - Keeps hour-level quantiles and loose low band (0.1‚Äì1.0 Hz)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "\n",
    "    # Bands (loose low band helps far/small events)\n",
    "    BANDS      : tuple = ((0.1, 1.0), (0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA windows (seconds)\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "\n",
    "    # Hysteresis OFF threshold\n",
    "    OFF        : float = 1.0\n",
    "\n",
    "    # Detection-eval window (NOT used for cutting CNN windows)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300\n",
    "\n",
    "    # ---------------- Recall vs budget search space ----------------\n",
    "    # Slightly stricter than your 0.992/0.98/60s combo\n",
    "    GRID_Q        : tuple = (0.993, 0.994, 0.995, 0.996)\n",
    "    GRID_ALPHA    : tuple = (1.00, 1.01, 1.02)\n",
    "    GRID_REFRACT  : tuple = (90, 120, 180)\n",
    "    GRID_NMS_SEC  : tuple = (60, 90, 120, 150, 180, 210)  # NEW: search NMS window\n",
    "    ADAPT_SCOPES  : tuple = (\"hour\", \"day\")               # prefer hour-level\n",
    "    FPH_BUDGET    : float = 6.0\n",
    "\n",
    "    # Fusion fixed to OR+NMS for recall\n",
    "    FUSE_MODE     : str  = \"or_nms\"\n",
    "\n",
    "    OUT_DIR       : str  = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- NPZ helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))  # FIXED: map(str, ...)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"\n",
    "    Test events = positives within TEST scope.\n",
    "    Event time (catalog-origin) = window_start + 20s (classification dataset convention).\n",
    "    \"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "# -------------------- CFT build --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    \"\"\"\n",
    "    Read one MiniSEED; resample & detrend; for each band:\n",
    "      bandpass -> classic_sta_lta -> CFT\n",
    "    Returns: dict(fs, t0, hours, cft_list=[...])\n",
    "    \"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime) / 3600.0)\n",
    "\n",
    "    cft_list = []\n",
    "    for (fmin, fmax) in bands:\n",
    "        tr = tr0.copy()\n",
    "        tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "        cft_list.append(cft)\n",
    "\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def to_onoff_array(x):\n",
    "    \"\"\"Normalize trigger_onset output to a (N,2) int ndarray (handles list/array).\"\"\"\n",
    "    arr = np.asarray(x, dtype=int)\n",
    "    if arr.size == 0:\n",
    "        return np.empty((0, 2), dtype=int)\n",
    "    if arr.ndim == 1:\n",
    "        if arr.shape[0] % 2 != 0:\n",
    "            raise ValueError(\"Trigger on/off array length must be even.\")\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    elif arr.shape[1] != 2:\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    return arr\n",
    "\n",
    "def picks_or_then_nms(onoffs, fs, t0, nms_sec=30, refract=60):\n",
    "    \"\"\"\n",
    "    OR fusion (union of all band ON starts) -> NMS (min spacing) -> refractory.\n",
    "    Returns a list of pandas.Timestamp.\n",
    "    \"\"\"\n",
    "    picks = []\n",
    "    for arr in onoffs:\n",
    "        if arr is None or len(arr) == 0:\n",
    "            continue\n",
    "        for a, b in arr:\n",
    "            picks.append(t0 + a / fs)  # UTCDateTime\n",
    "    if not picks:\n",
    "        return []\n",
    "    picks.sort()\n",
    "    # NMS\n",
    "    fused, last = [], None\n",
    "    for t in picks:\n",
    "        if (last is None) or ((t - last) > nms_sec):\n",
    "            fused.append(t); last = t\n",
    "    # Refractory\n",
    "    out, last = [], None\n",
    "    for t in fused:\n",
    "        if (last is None) or ((t - last) > refract):\n",
    "            out.append(pd.Timestamp(UTCDateTime(t).datetime)); last = t\n",
    "    return out\n",
    "\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    \"\"\"Return boolean hits for each event window; hit if any trigger ‚àà [start, end].\"\"\"\n",
    "    if trigs_ns.size == 0:\n",
    "        return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Build day cache & eval windows --------------------\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for dt in event_days:\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    cftd = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    cft_cache[dt] = cftd\n",
    "    total_hours += cftd[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "print(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "\n",
    "# -------------------- Core run (OR+NMS) --------------------\n",
    "def run_adaptive_or_nms(q, alpha, refract, nms_sec, adapt_scope):\n",
    "    \"\"\"\n",
    "    For each day:\n",
    "      - Threshold per band via quantile * alpha\n",
    "      - If adapt_scope == 'hour', compute per-hour thresholds; else per-day\n",
    "      - trigger_onset -> normalize -> collect ON intervals\n",
    "      - OR+NMS fusion to picks with (nms_sec, refract)\n",
    "    Return metrics dict.\n",
    "    \"\"\"\n",
    "    all_picks = []\n",
    "    for dt, entry in cft_cache.items():\n",
    "        onoffs = []\n",
    "        for cft in entry[\"cft_list\"]:\n",
    "            if adapt_scope == \"hour\":\n",
    "                fs = entry[\"fs\"]; H = int(3600 * fs)\n",
    "                segs = []\n",
    "                for s in range(0, len(cft), H):\n",
    "                    e = min(s + H, len(cft))\n",
    "                    seg = cft[s:e]\n",
    "                    if seg.size == 0:\n",
    "                        _onoff = np.empty((0, 2), dtype=int)\n",
    "                    else:\n",
    "                        on_val = float(np.quantile(seg, q) * alpha)\n",
    "                        raw = trigger_onset(seg, on_val, CFG.OFF)\n",
    "                        _onoff = to_onoff_array(raw)\n",
    "                        if _onoff.size:\n",
    "                            _onoff[:, 0] += s; _onoff[:, 1] += s\n",
    "                    segs.append(_onoff)\n",
    "                onoff = np.vstack(segs) if len(segs) else np.empty((0,2), dtype=int)\n",
    "            elif adapt_scope == \"day\":\n",
    "                if cft.size == 0:\n",
    "                    onoff = np.empty((0,2), dtype=int)\n",
    "                else:\n",
    "                    on_val = float(np.quantile(cft, q) * alpha)\n",
    "                    raw = trigger_onset(cft, on_val, CFG.OFF)\n",
    "                    onoff = to_onoff_array(raw)\n",
    "            else:\n",
    "                raise ValueError(\"adapt_scope must be 'hour' or 'day'\")\n",
    "            onoffs.append(onoff)\n",
    "\n",
    "        picks = picks_or_then_nms(onoffs, entry[\"fs\"], entry[\"t0\"], nms_sec=nms_sec, refract=refract)\n",
    "        all_picks.extend(picks)\n",
    "\n",
    "    trigs_ns = np.array(sorted(all_picks), dtype=\"datetime64[ns]\")\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    return dict(q=q, alpha=alpha, refract=refract, nms_sec=nms_sec,\n",
    "                adapt=adapt_scope, recall=recall, fph=fph,\n",
    "                triggers=int(trigs_ns.size), trigs=trigs_ns)\n",
    "\n",
    "# -------------------- Search --------------------\n",
    "rows = []\n",
    "for scope in CFG.ADAPT_SCOPES:\n",
    "    for q in CFG.GRID_Q:\n",
    "        for a in CFG.GRID_ALPHA:\n",
    "            for rf in CFG.GRID_REFRACT:\n",
    "                for nms in CFG.GRID_NMS_SEC:\n",
    "                    rows.append(run_adaptive_or_nms(q, a, rf, nms, scope))\n",
    "\n",
    "scan = pd.DataFrame([{k:v for k,v in r.items() if k!='trigs'} for r in rows])\n",
    "scan = scan.sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTop candidates (by recall then lower FPH):\")\n",
    "print(scan.head(12))\n",
    "\n",
    "# Choose under budget if possible\n",
    "cands = scan[scan[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "if len(cands) > 0:\n",
    "    chosen = cands.sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "    note = \"[Chosen UNDER budget]\"\n",
    "else:\n",
    "    # Fall back: choose the LOWEST-FPH candidate overall (closest to budget)\n",
    "    chosen = scan.sort_values([\"fph\",\"recall\"], ascending=[True, False]).iloc[0]\n",
    "    note = \"[WARNING] No config met FPH budget; choosing lowest-FPH candidate\"\n",
    "\n",
    "print(f\"\\n{note} q={chosen['q']} alpha={chosen['alpha']} \"\n",
    "      f\"refract={int(chosen['refract'])} nms={int(chosen['nms_sec'])} scope={chosen['adapt']} \"\n",
    "      f\"| recall={chosen['recall']:.3f} FPH={chosen['fph']:.3f} triggers={int(chosen['triggers'])}\")\n",
    "\n",
    "Q_CHOSEN     = float(chosen[\"q\"])\n",
    "A_CHOSEN     = float(chosen[\"alpha\"])\n",
    "RF_CHOSEN    = int(chosen[\"refract\"])\n",
    "NMS_CHOSEN   = int(chosen[\"nms_sec\"])\n",
    "SCOPE_CHOSEN = str(chosen[\"adapt\"])\n",
    "\n",
    "# -------------------- Save triggers CSV --------------------\n",
    "trigs_final = rows[scan.index.get_loc(chosen.name)][\"trigs\"] if \"trigs\" in rows[scan.index.get_loc(chosen.name)] else None\n",
    "# safer: recompute once for the chosen params\n",
    "res = run_adaptive_or_nms(Q_CHOSEN, A_CHOSEN, RF_CHOSEN, NMS_CHOSEN, SCOPE_CHOSEN)\n",
    "trigs_final = res[\"trigs\"]\n",
    "\n",
    "mode_tag = \"ornms\"\n",
    "csv_name = f\"triggers_MB_{mode_tag}_q{Q_CHOSEN}_a{A_CHOSEN}_rf{RF_CHOSEN}_nms{NMS_CHOSEN}_sc{SCOPE_CHOSEN}.csv\"\n",
    "csv_out = os.path.join(CFG.OUT_DIR, csv_name)\n",
    "\n",
    "if trigs_final.size == 0:\n",
    "    pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved EMPTY] {csv_out}\")\n",
    "else:\n",
    "    trig_ts = pd.to_datetime(trigs_final)\n",
    "    date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "    trig_df = pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "    trig_df.to_csv(csv_out, index=False)\n",
    "    print(f\"[Saved] {csv_out} (rows={len(trig_df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1d82096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:12:27] Building test events...\n",
      "[13:12:27] Building CFTs & quantiles per day...\n",
      "[13:12:28]   2 day(s) processed...\n",
      "[13:12:28]   4 day(s) processed...\n",
      "[13:12:28]   6 day(s) processed...\n",
      "[13:12:28]   8 day(s) processed...\n",
      "[13:12:29]   10 day(s) processed...\n",
      "[13:12:29]   12 day(s) processed...\n",
      "[13:12:29]   14 day(s) processed...\n",
      "[13:12:30]   16 day(s) processed...\n",
      "[13:12:30] [Info] events considered: 1392 on 17 days | hours=408.0\n",
      "[13:12:30] [Params] q=0.997 alpha=1.03 refract=240s nms=240s scope=hour bands=((0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
      "[13:12:30]   processed 1/17 day(s)\n",
      "[13:12:30]   processed 2/17 day(s)\n",
      "[13:12:30]   processed 3/17 day(s)\n",
      "[13:12:30]   processed 4/17 day(s)\n",
      "[13:12:30]   processed 5/17 day(s)\n",
      "[13:12:30]   processed 6/17 day(s)\n",
      "[13:12:30]   processed 7/17 day(s)\n",
      "[13:12:30]   processed 8/17 day(s)\n",
      "[13:12:30]   processed 9/17 day(s)\n",
      "[13:12:30]   processed 10/17 day(s)\n",
      "[13:12:30]   processed 11/17 day(s)\n",
      "[13:12:30]   processed 12/17 day(s)\n",
      "[13:12:30]   processed 13/17 day(s)\n",
      "[13:12:30]   processed 14/17 day(s)\n",
      "[13:12:30]   processed 15/17 day(s)\n",
      "[13:12:30]   processed 16/17 day(s)\n",
      "[13:12:30]   processed 17/17 day(s)\n",
      "[13:12:30] [Final] recall=0.643 | FPH=8.105 | triggers=3307\n",
      "[13:12:30] [Saved] runs/cascade_eval/triggers_MB_ornms_q0.997_a1.03_rf240_nms240_schour.csv (rows=3307)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multiband STA/LTA Detection v2 ‚Äî Budgeted (FPH <= 6/h)\n",
    "# Single-point, conservative config to keep false positives down:\n",
    "#   - Fusion: OR + NMS\n",
    "#   - Hour-level quantile thresholds (robust to drift)\n",
    "#   - q=0.997, alpha=1.03 (tighter than your previous)\n",
    "#   - Refractory = 240 s, NMS = 240 s (aggressive thinning)\n",
    "#   - Bands: 0.5‚Äì8 Hz (3 bands); you can re-enable 0.1‚Äì1.0 Hz if needed\n",
    "# Output: triggers_MB_ornms_q..._a..._rf..._nms..._sc...csv\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config (single-point, conservative) --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "\n",
    "    # Bands:\n",
    "    #   Conservative default (3 bands): reduces low-frequency false alarms\n",
    "    BANDS      : tuple = ((0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "    # If you want a bit more recall, uncomment the low band and use 4 bands:\n",
    "    # BANDS      : tuple = ((0.1, 1.0), (0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA windows (seconds)\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "\n",
    "    # Hysteresis OFF threshold\n",
    "    OFF        : float = 1.0\n",
    "\n",
    "    # Detection-eval window (for hit-rate; not for CNN cutting)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300\n",
    "\n",
    "    # ---------- Fixed parameters (tuned for <=6/h) ----------\n",
    "    Q          : float = 0.997   # tighter quantile\n",
    "    ALPHA      : float = 1.03    # slightly above 1.0\n",
    "    REFRACT    : int   = 240     # refractory thinning (seconds)\n",
    "    NMS_SEC    : int   = 240     # NMS spacing (seconds)\n",
    "    ADAPT_SCOPE: str   = \"hour\"  # \"hour\" or \"day\"\n",
    "\n",
    "    # Budget (used for reporting only)\n",
    "    FPH_BUDGET : float = 6.0\n",
    "\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "def log(msg): print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "# -------------------- NPZ helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    \"\"\"Load positives and return (X, y, sid, window_start) filtered to detect_label==1 if available.\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))  # FIXED: map(str, ...)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"\n",
    "    Test events = positives within TEST scope.\n",
    "    Event time = window_start + 20s (classification dataset convention).\n",
    "    \"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "# -------------------- Build daily CFTs + precompute quantiles --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    \"\"\"\n",
    "    Read one MiniSEED; resample & detrend; for each band: bandpass -> classic_sta_lta.\n",
    "    Returns dict(fs, t0, hours, cft_list=[...])\n",
    "    \"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime) / 3600.0)\n",
    "\n",
    "    cft_list = []\n",
    "    for (fmin, fmax) in bands:\n",
    "        tr = tr0.copy()\n",
    "        tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "        cft_list.append(cft)\n",
    "\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "def precompute_quantiles(entry, q_values):\n",
    "    \"\"\"\n",
    "    Precompute per-day & per-hour quantiles for all bands and given q set.\n",
    "    Adds keys:\n",
    "      entry[\"q_day\"][band_index][q] = value\n",
    "      entry[\"q_hour\"][band_index] = list of dict per hour: [{q:val,...}, ...]\n",
    "      entry[\"hour_segments\"] = list of (s,e) indices per hour\n",
    "    \"\"\"\n",
    "    fs = entry[\"fs\"]\n",
    "    T  = len(entry[\"cft_list\"][0])\n",
    "    H  = int(3600 * fs)\n",
    "    segs = [(s, min(s+H, T)) for s in range(0, T, H)]\n",
    "    entry[\"hour_segments\"] = segs\n",
    "    entry[\"q_day\"]  = []\n",
    "    entry[\"q_hour\"] = []\n",
    "    for cft in entry[\"cft_list\"]:\n",
    "        qd = {float(q): float(np.quantile(cft, q)) for q in q_values}\n",
    "        entry[\"q_day\"].append(qd)\n",
    "        qh = []\n",
    "        for (s,e) in segs:\n",
    "            seg = cft[s:e]\n",
    "            if seg.size == 0:\n",
    "                qh.append({float(q): 0.0 for q in q_values})\n",
    "            else:\n",
    "                qh.append({float(q): float(np.quantile(seg, q)) for q in q_values})\n",
    "        entry[\"q_hour\"].append(qh)\n",
    "\n",
    "# -------------------- Fusion & metrics helpers --------------------\n",
    "def to_onoff_array(x):\n",
    "    \"\"\"Normalize trigger_onset output to a (N,2) int ndarray.\"\"\"\n",
    "    arr = np.asarray(x, dtype=int)\n",
    "    if arr.size == 0:\n",
    "        return np.empty((0, 2), dtype=int)\n",
    "    if arr.ndim == 1:\n",
    "        if arr.shape[0] % 2 != 0:\n",
    "            raise ValueError(\"Trigger on/off array length must be even.\")\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    elif arr.shape[1] != 2:\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    return arr\n",
    "\n",
    "def picks_or_then_nms(onoffs, fs, t0, nms_sec=30, refract=60):\n",
    "    \"\"\"\n",
    "    OR fusion (union of ON starts across bands) -> NMS (min spacing) -> refractory.\n",
    "    Returns list of pandas.Timestamp in ascending time order.\n",
    "    \"\"\"\n",
    "    picks = []\n",
    "    for arr in onoffs:\n",
    "        if arr is None or len(arr) == 0:\n",
    "            continue\n",
    "        for a, b in arr:\n",
    "            picks.append(t0 + a / fs)  # UTCDateTime (float seconds internally)\n",
    "    if not picks:\n",
    "        return []\n",
    "    picks.sort()\n",
    "    # NMS\n",
    "    fused, last = [], None\n",
    "    for t in picks:\n",
    "        if (last is None) or ((t - last) > nms_sec):\n",
    "            fused.append(t); last = t\n",
    "    # Refractory\n",
    "    out, last = [], None\n",
    "    for t in fused:\n",
    "        if (last is None) or ((t - last) > refract):\n",
    "            out.append(pd.Timestamp(UTCDateTime(t).datetime)); last = t\n",
    "    return out\n",
    "\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    \"\"\"Boolean hits for each event window; hit if any trigger ‚àà [start, end].\"\"\"\n",
    "    if trigs_ns.size == 0:\n",
    "        return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Build caches & eval windows --------------------\n",
    "log(\"Building test events...\")\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "log(\"Building CFTs & quantiles per day...\")\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for di, dt in enumerate(event_days, 1):\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    entry = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    precompute_quantiles(entry, [CFG.Q])  # single q ‚Üí fastest\n",
    "    cft_cache[dt] = entry\n",
    "    total_hours += entry[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "    if di % 2 == 0:\n",
    "        log(f\"  {di} day(s) processed...\")\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "log(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "log(f\"[Params] q={CFG.Q} alpha={CFG.ALPHA} refract={CFG.REFRACT}s nms={CFG.NMS_SEC}s scope={CFG.ADAPT_SCOPE} bands={CFG.BANDS}\")\n",
    "\n",
    "# -------------------- Run once with fixed params --------------------\n",
    "def run_fixed_params():\n",
    "    all_picks = []\n",
    "    for di, dt in enumerate(dates_ok, 1):\n",
    "        entry = cft_cache[dt]\n",
    "        fs = entry[\"fs\"]\n",
    "        onoffs = []\n",
    "        for b, cft in enumerate(entry[\"cft_list\"]):\n",
    "            if CFG.ADAPT_SCOPE == \"hour\":\n",
    "                arrs = []\n",
    "                for (seg_idx, (s,e)) in enumerate(entry[\"hour_segments\"]):\n",
    "                    base = entry[\"q_hour\"][b][seg_idx].get(float(CFG.Q), 0.0)\n",
    "                    on_val = base * CFG.ALPHA\n",
    "                    raw = trigger_onset(cft[s:e], on_val, CFG.OFF)\n",
    "                    _onoff = to_onoff_array(raw)\n",
    "                    if _onoff.size:\n",
    "                        _onoff[:,0] += s; _onoff[:,1] += s\n",
    "                    arrs.append(_onoff)\n",
    "                onoff = np.vstack(arrs) if len(arrs) else np.empty((0,2), dtype=int)\n",
    "            elif CFG.ADAPT_SCOPE == \"day\":\n",
    "                base = entry[\"q_day\"][b].get(float(CFG.Q), 0.0)\n",
    "                on_val = base * CFG.ALPHA\n",
    "                raw = trigger_onset(cft, on_val, CFG.OFF)\n",
    "                onoff = to_onoff_array(raw)\n",
    "            else:\n",
    "                raise ValueError(\"ADAPT_SCOPE must be 'hour' or 'day'\")\n",
    "            onoffs.append(onoff)\n",
    "\n",
    "        picks = picks_or_then_nms(onoffs, fs, entry[\"t0\"], nms_sec=CFG.NMS_SEC, refract=CFG.REFRACT)\n",
    "        all_picks.extend(picks)\n",
    "\n",
    "        if (di % 1) == 0:\n",
    "            log(f\"  processed {di}/{len(dates_ok)} day(s)\")\n",
    "\n",
    "    # Metrics\n",
    "    trigs_ns = np.array(sorted(all_picks), dtype=\"datetime64[ns]\")\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "\n",
    "    # Save CSV\n",
    "    mode_tag = \"ornms\"\n",
    "    csv_name = f\"triggers_MB_{mode_tag}_q{CFG.Q}_a{CFG.ALPHA}_rf{CFG.REFRACT}_nms{CFG.NMS_SEC}_sc{CFG.ADAPT_SCOPE}.csv\"\n",
    "    csv_out = os.path.join(CFG.OUT_DIR, csv_name)\n",
    "    if trigs_ns.size == 0:\n",
    "        pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    else:\n",
    "        trig_ts = pd.to_datetime(trigs_ns)\n",
    "        date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "        trig_df = pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "        trig_df.to_csv(csv_out, index=False)\n",
    "\n",
    "    log(f\"[Final] recall={recall:.3f} | FPH={fph:.3f} | triggers={int(trigs_ns.size)}\")\n",
    "    log(f\"[Saved] {csv_out} (rows={trigs_ns.size})\")\n",
    "\n",
    "run_fixed_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80e33344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:16:20] Building test events...\n",
      "[13:16:20] Building CFTs & quantiles per day...\n",
      "[13:16:21]   2 day(s) processed...\n",
      "[13:16:21]   4 day(s) processed...\n",
      "[13:16:22]   6 day(s) processed...\n",
      "[13:16:23]   8 day(s) processed...\n",
      "[13:16:23]   10 day(s) processed...\n",
      "[13:16:24]   12 day(s) processed...\n",
      "[13:16:24]   14 day(s) processed...\n",
      "[13:16:25]   16 day(s) processed...\n",
      "[13:16:25] [Info] events considered: 1392 on 17 days | hours=408.0\n",
      "[13:16:25] Scanning 72 combos...\n",
      "[13:16:29]   scanned 10/72 combos\n",
      "[13:16:31]   scanned 20/72 combos\n",
      "[13:16:34]   scanned 30/72 combos\n",
      "[13:16:37]   scanned 40/72 combos\n",
      "[13:16:40]   scanned 50/72 combos\n",
      "[13:16:43]   scanned 60/72 combos\n",
      "[13:16:46]   scanned 70/72 combos\n",
      "[13:16:46] Top candidates (by recall then lower FPH):\n",
      "[13:16:46]     q  alpha  refract  nms_sec adapt   recall      fph  triggers\n",
      "0.997   1.03      240      240  hour 0.642960 8.105397      3307\n",
      "0.997   1.05      240      240  hour 0.624282 7.740201      3158\n",
      "0.997   1.03      240      240   day 0.620690 7.607848      3104\n",
      "0.997   1.05      240      240   day 0.607759 7.372553      3008\n",
      "0.997   1.03      240      300  hour 0.591954 7.132357      2910\n",
      "0.997   1.03      300      300  hour 0.591954 7.132357      2910\n",
      "0.997   1.05      240      300  hour 0.573276 6.808827      2778\n",
      "0.997   1.05      300      300  hour 0.573276 6.808827      2778\n",
      "0.997   1.03      240      300   day 0.568966 6.754906      2756\n",
      "0.997   1.03      300      300   day 0.568966 6.754906      2756\n",
      "0.997   1.03      300      240  hour 0.566092 6.426474      2622\n",
      "0.997   1.05      240      300   day 0.563218 6.536768      2667\n",
      "[13:16:46] [Chosen UNDER budget] q=0.997 a=1.05 rf=300 nms=240 sc=day | recall=0.534 FPH=5.963 triggers‚âà2433\n",
      "[13:16:47] [FINAL] q=0.997 a=1.05 rf=300 nms=240 sc=day | recall=0.534 FPH=5.963 triggers=2433\n",
      "[13:16:47] [Saved] runs/cascade_eval/triggers_MB_ornms_q0.997_a1.05_rf300_nms240_scday.csv (rows=2433)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multiband STA/LTA Detection v2 ‚Äî Auto-budget (FPH <= 6/h)\n",
    "# Strategy:\n",
    "#   - Fusion: OR + NMS  (recall-friendly, then thin)\n",
    "#   - Hour-level thresholds (robust to drift) + day-level fallback\n",
    "#   - Small, conservative search over (q, alpha, NMS, refractory, scope)\n",
    "#   - Pick the HIGHEST recall that satisfies FPH <= budget\n",
    "#   - If none satisfy, pick the LOWEST-FPH combo and warn\n",
    "# Logs are verbose so Jupyter won't look \"stuck\".\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "\n",
    "    # Bands: conservative 3-band set to curb low-freq false alarms\n",
    "    BANDS      : tuple = ((0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "    # If you want more recall later, add the low band back:\n",
    "    # BANDS      : tuple = ((0.1, 1.0), (0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA windows (seconds) and OFF hysteresis\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "    OFF        : float = 1.0\n",
    "\n",
    "    # Evaluation window for detection hit-rate only\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300\n",
    "\n",
    "    # -------- Small conservative search space (fast, budget-oriented) -------\n",
    "    GRID_Q        : tuple = (0.997, 0.998, 0.999)\n",
    "    GRID_ALPHA    : tuple = (1.03, 1.05)\n",
    "    GRID_REFRACT  : tuple = (240, 300)\n",
    "    GRID_NMS_SEC  : tuple = (240, 300, 360)\n",
    "    ADAPT_SCOPES  : tuple = (\"hour\", \"day\")  # try hour first; day can reduce picks\n",
    "\n",
    "    # Budget\n",
    "    FPH_BUDGET    : float = 6.0\n",
    "\n",
    "    # Output\n",
    "    OUT_DIR       : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "def log(msg): print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "# -------------------- NPZ & split helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))  # FIXED: map(str, ...)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"\n",
    "    Test events = positives in TEST scope; event_time = window_start + 20s.\n",
    "    \"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "# -------------------- CFT build & quantile precompute --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    \"\"\"\n",
    "    Read MiniSEED; resample & detrend; per band: bandpass -> classic_sta_lta.\n",
    "    \"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime) / 3600.0)\n",
    "    cft_list = []\n",
    "    for (fmin, fmax) in bands:\n",
    "        tr = tr0.copy()\n",
    "        tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "        cft_list.append(cft)\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "def precompute_quantiles(entry, q_values):\n",
    "    \"\"\"\n",
    "    Precompute per-day & per-hour quantiles for all bands and given q values.\n",
    "    Adds:\n",
    "      entry[\"q_day\"][band_idx][q]  and\n",
    "      entry[\"q_hour\"][band_idx][hour_idx][q]\n",
    "      entry[\"hour_segments\"] = [(s,e), ...]\n",
    "    \"\"\"\n",
    "    fs = entry[\"fs\"]\n",
    "    T  = len(entry[\"cft_list\"][0])\n",
    "    H  = int(3600 * fs)\n",
    "    segs = [(s, min(s+H, T)) for s in range(0, T, H)]\n",
    "    entry[\"hour_segments\"] = segs\n",
    "    entry[\"q_day\"]  = []\n",
    "    entry[\"q_hour\"] = []\n",
    "    for cft in entry[\"cft_list\"]:\n",
    "        qd = {float(q): float(np.quantile(cft, q)) for q in q_values}\n",
    "        entry[\"q_day\"].append(qd)\n",
    "        qh = []\n",
    "        for (s,e) in segs:\n",
    "            seg = cft[s:e]\n",
    "            if seg.size == 0:\n",
    "                qh.append({float(q): 0.0 for q in q_values})\n",
    "            else:\n",
    "                qh.append({float(q): float(np.quantile(seg, q)) for q in q_values})\n",
    "        entry[\"q_hour\"].append(qh)\n",
    "\n",
    "# -------------------- Fusion & metric helpers --------------------\n",
    "def to_onoff_array(x):\n",
    "    \"\"\"Normalize trigger_onset output to a (N,2) int ndarray (handles list/array).\"\"\"\n",
    "    arr = np.asarray(x, dtype=int)\n",
    "    if arr.size == 0:\n",
    "        return np.empty((0, 2), dtype=int)\n",
    "    if arr.ndim == 1:\n",
    "        if arr.shape[0] % 2 != 0:\n",
    "            raise ValueError(\"Trigger on/off array length must be even.\")\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    elif arr.shape[1] != 2:\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    return arr\n",
    "\n",
    "def picks_or_then_nms(onoffs, fs, t0, nms_sec=30, refract=60):\n",
    "    \"\"\"\n",
    "    OR fusion of ON starts across bands -> NMS (min spacing) -> refractory thinning.\n",
    "    Returns list of pandas.Timestamp (ascending).\n",
    "    \"\"\"\n",
    "    picks = []\n",
    "    for arr in onoffs:\n",
    "        if arr is None or len(arr) == 0:\n",
    "            continue\n",
    "        for a, b in arr:\n",
    "            picks.append(t0 + a / fs)  # UTCDateTime (float seconds internally)\n",
    "    if not picks: return []\n",
    "    picks.sort()\n",
    "    fused, last = [], None\n",
    "    for t in picks:\n",
    "        if (last is None) or ((t - last) > nms_sec):\n",
    "            fused.append(t); last = t\n",
    "    out, last = [], None\n",
    "    for t in fused:\n",
    "        if (last is None) or ((t - last) > refract):\n",
    "            out.append(pd.Timestamp(UTCDateTime(t).datetime)); last = t\n",
    "    return out\n",
    "\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    \"\"\"Hit if any trigger ‚àà [start, end] for each event (vectorized).\"\"\"\n",
    "    if trigs_ns.size == 0:\n",
    "        return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Build caches --------------------\n",
    "log(\"Building test events...\")\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "log(\"Building CFTs & quantiles per day...\")\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for di, dt in enumerate(event_days, 1):\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp):\n",
    "        continue\n",
    "    entry = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    precompute_quantiles(entry, CFG.GRID_Q)  # precompute for all candidate q\n",
    "    cft_cache[dt] = entry\n",
    "    total_hours += entry[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "    if di % 2 == 0:\n",
    "        log(f\"  {di} day(s) processed...\")\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "log(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "\n",
    "# -------------------- Core evaluator (uses quantile cache) --------------------\n",
    "def run_or_nms(q, alpha, refract, nms_sec, adapt_scope):\n",
    "    picks_all = []\n",
    "    for di, dt in enumerate(dates_ok, 1):\n",
    "        entry = cft_cache[dt]\n",
    "        fs = entry[\"fs\"]\n",
    "        onoffs = []\n",
    "        for b, cft in enumerate(entry[\"cft_list\"]):\n",
    "            if adapt_scope == \"hour\":\n",
    "                arrs = []\n",
    "                for (seg_idx, (s,e)) in enumerate(entry[\"hour_segments\"]):\n",
    "                    base = entry[\"q_hour\"][b][seg_idx].get(float(q), 0.0)\n",
    "                    on_val = base * alpha\n",
    "                    raw = trigger_onset(cft[s:e], on_val, CFG.OFF)\n",
    "                    _onoff = to_onoff_array(raw)\n",
    "                    if _onoff.size:\n",
    "                        _onoff[:,0] += s; _onoff[:,1] += s\n",
    "                    arrs.append(_onoff)\n",
    "                onoff = np.vstack(arrs) if len(arrs) else np.empty((0,2), dtype=int)\n",
    "            elif adapt_scope == \"day\":\n",
    "                base = entry[\"q_day\"][b].get(float(q), 0.0)\n",
    "                on_val = base * alpha\n",
    "                raw = trigger_onset(cft, on_val, CFG.OFF)\n",
    "                onoff = to_onoff_array(raw)\n",
    "            else:\n",
    "                raise ValueError(\"adapt_scope must be 'hour' or 'day'\")\n",
    "            onoffs.append(onoff)\n",
    "        picks = picks_or_then_nms(onoffs, fs, entry[\"t0\"], nms_sec=nms_sec, refract=refract)\n",
    "        picks_all.extend(picks)\n",
    "\n",
    "    trigs_ns = np.array(sorted(picks_all), dtype=\"datetime64[ns]\")\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    return dict(q=q, alpha=alpha, refract=int(refract), nms_sec=int(nms_sec),\n",
    "                adapt=adapt_scope, recall=recall, fph=fph, triggers=int(trigs_ns.size), trigs=trigs_ns)\n",
    "\n",
    "# -------------------- Search small grid (fast) --------------------\n",
    "rows = []\n",
    "total_combos = len(CFG.ADAPT_SCOPES)*len(CFG.GRID_Q)*len(CFG.GRID_ALPHA)*len(CFG.GRID_REFRACT)*len(CFG.GRID_NMS_SEC)\n",
    "log(f\"Scanning {total_combos} combos...\")\n",
    "done = 0\n",
    "for scope in CFG.ADAPT_SCOPES:\n",
    "    for q in CFG.GRID_Q:\n",
    "        for a in CFG.GRID_ALPHA:\n",
    "            for rf in CFG.GRID_REFRACT:\n",
    "                for nms in CFG.GRID_NMS_SEC:\n",
    "                    r = run_or_nms(q, a, rf, nms, scope)\n",
    "                    rows.append({k:v for k,v in r.items() if k!='trigs'})\n",
    "                    done += 1\n",
    "                    if done % 10 == 0:\n",
    "                        log(f\"  scanned {done}/{total_combos} combos\")\n",
    "\n",
    "scan = pd.DataFrame(rows).sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "log(\"Top candidates (by recall then lower FPH):\")\n",
    "log(scan.head(12).to_string(index=False))\n",
    "\n",
    "# Choose best under budget; else lowest-FPH overall\n",
    "cands = scan[scan[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "if len(cands) > 0:\n",
    "    chosen = cands.sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "    note = \"[Chosen UNDER budget]\"\n",
    "else:\n",
    "    chosen = scan.sort_values([\"fph\",\"recall\"], ascending=[True, False]).iloc[0]\n",
    "    note = \"[WARN] No combo meets budget; picking lowest-FPH overall\"\n",
    "\n",
    "log(f\"{note} q={chosen['q']} a={chosen['alpha']} rf={int(chosen['refract'])} nms={int(chosen['nms_sec'])} sc={chosen['adapt']} \"\n",
    "    f\"| recall={chosen['recall']:.3f} FPH={chosen['fph']:.3f} triggers‚âà{int(chosen['triggers'])}\")\n",
    "\n",
    "# -------------------- Finalize (re-run chosen & save CSV) --------------------\n",
    "def finalize_and_save(q, a, rf, nms, scope):\n",
    "    res = run_or_nms(q, a, rf, nms, scope)\n",
    "    trigs_ns = res[\"trigs\"]\n",
    "    mode_tag = \"ornms\"\n",
    "    csv_name = f\"triggers_MB_{mode_tag}_q{q}_a{a}_rf{rf}_nms{nms}_sc{scope}.csv\"\n",
    "    csv_out = os.path.join(CFG.OUT_DIR, csv_name)\n",
    "    if trigs_ns.size == 0:\n",
    "        pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    else:\n",
    "        trig_ts = pd.to_datetime(trigs_ns)\n",
    "        date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "        trig_df = pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "        trig_df.to_csv(csv_out, index=False)\n",
    "    log(f\"[FINAL] q={q} a={a} rf={rf} nms={nms} sc={scope} | recall={res['recall']:.3f} FPH={res['fph']:.3f} triggers={int(res['triggers'])}\")\n",
    "    log(f\"[Saved] {csv_out} (rows={int(res['triggers'])})\")\n",
    "\n",
    "finalize_and_save(float(chosen[\"q\"]), float(chosen[\"alpha\"]), int(chosen[\"refract\"]), int(chosen[\"nms_sec\"]), str(chosen[\"adapt\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de1bb3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:20] Building test events...\n",
      "[13:24:20] Building CFTs & quantiles per day...\n",
      "[13:24:21]   2 day(s) processed...\n",
      "[13:24:22]   4 day(s) processed...\n",
      "[13:24:22]   6 day(s) processed...\n",
      "[13:24:23]   8 day(s) processed...\n",
      "[13:24:24]   10 day(s) processed...\n",
      "[13:24:24]   12 day(s) processed...\n",
      "[13:24:25]   14 day(s) processed...\n",
      "[13:24:26]   16 day(s) processed...\n",
      "[13:24:26] [Info] events considered: 1392 on 17 days | hours=408.0\n",
      "[13:24:26] Scanning 72 combos...\n",
      "[13:24:29]   scanned 10/72 combos\n",
      "[13:24:32]   scanned 20/72 combos\n",
      "[13:24:35]   scanned 30/72 combos\n",
      "[13:24:38]   scanned 40/72 combos\n",
      "[13:24:41]   scanned 50/72 combos\n",
      "[13:24:44]   scanned 60/72 combos\n",
      "[13:24:47]   scanned 70/72 combos\n",
      "[13:24:48] Top candidates (by recall then lower FPH):\n",
      "[13:24:48]     q  alpha  refract  nms_sec adapt   recall      fph  triggers\n",
      "0.997   1.03      240      240  hour 0.642960 8.105397      3307\n",
      "0.997   1.05      240      240  hour 0.624282 7.740201      3158\n",
      "0.997   1.03      240      240   day 0.620690 7.607848      3104\n",
      "0.997   1.05      240      240   day 0.607759 7.372553      3008\n",
      "0.997   1.03      240      300  hour 0.591954 7.132357      2910\n",
      "0.997   1.03      300      300  hour 0.591954 7.132357      2910\n",
      "0.997   1.05      240      300  hour 0.573276 6.808827      2778\n",
      "0.997   1.05      300      300  hour 0.573276 6.808827      2778\n",
      "0.997   1.03      240      300   day 0.568966 6.754906      2756\n",
      "0.997   1.03      300      300   day 0.568966 6.754906      2756\n",
      "0.997   1.03      300      240  hour 0.566092 6.426474      2622\n",
      "0.997   1.05      240      300   day 0.563218 6.536768      2667\n",
      "[13:24:48] [Chosen UNDER budget] q=0.997 a=1.05 rf=300 nms=240 sc=day | recall=0.534 FPH=5.963 triggers‚âà2433\n",
      "[13:24:48] [FINAL] q=0.997 a=1.05 rf=300 nms=240 sc=day | recall=0.534 FPH=5.963 triggers=2433\n",
      "[13:24:48] [Saved] runs/cascade_eval/triggers_MB_ornms_q0.997_a1.05_rf300_nms240_scday.csv (rows=2433)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multiband STA/LTA Detection v2 ‚Äî Auto-budget (FPH <= 6/h)\n",
    "# Strategy:\n",
    "#   - Fusion: OR + NMS (recall-friendly, then thin)\n",
    "#   - Hour-level thresholds (robust to drift) + day-level fallback\n",
    "#   - Small conservative search over (q, alpha, NMS, refractory, scope)\n",
    "#   - Pick the HIGHEST recall that satisfies FPH <= budget\n",
    "#   - If none satisfy, pick the LOWEST-FPH combo and warn\n",
    "#   - Verbose logs so notebooks won't look \"stuck\"\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "\n",
    "    # Bands: conservative 3-band set to curb low-freq false alarms\n",
    "    BANDS      : tuple = ((0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "    # If you want more recall later, add the low band back:\n",
    "    # BANDS      : tuple = ((0.1, 1.0), (0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA windows and OFF hysteresis\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "    OFF        : float = 1.0\n",
    "\n",
    "    # Detection-eval window (for hit-rate; not for CNN cutting)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300\n",
    "\n",
    "    # -------- Small conservative search (fast, budget-oriented) -------\n",
    "    GRID_Q        : tuple = (0.997, 0.998, 0.999)\n",
    "    GRID_ALPHA    : tuple = (1.03, 1.05)\n",
    "    GRID_REFRACT  : tuple = (240, 300)\n",
    "    GRID_NMS_SEC  : tuple = (240, 300, 360)\n",
    "    ADAPT_SCOPES  : tuple = (\"hour\", \"day\")  # try hour first; day may reduce picks\n",
    "\n",
    "    # Budget\n",
    "    FPH_BUDGET    : float = 6.0\n",
    "\n",
    "    # Output\n",
    "    OUT_DIR       : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "def log(msg): print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "# -------------------- NPZ & split helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files: return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    \"\"\"Load positives and return (X, y, sid, window_start) filtered to detect_label==1 if available.\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))  # FIXED: map(str, ...)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"\n",
    "    Test events = positives in TEST scope; event_time = window_start + 20s.\n",
    "    \"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "# -------------------- CFT build & quantile precompute --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    \"\"\"Read MiniSEED; resample & detrend; per band: bandpass -> classic_sta_lta.\"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6: tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime) / 3600.0)\n",
    "    cft_list = []\n",
    "    for (fmin, fmax) in bands:\n",
    "        tr = tr0.copy()\n",
    "        tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "        cft_list.append(cft)\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "def precompute_quantiles(entry, q_values):\n",
    "    \"\"\"\n",
    "    Precompute per-day & per-hour quantiles for all bands and given q values.\n",
    "    Adds:\n",
    "      entry[\"q_day\"][band_idx][q]  and\n",
    "      entry[\"q_hour\"][band_idx][hour_idx][q]\n",
    "      entry[\"hour_segments\"] = [(s,e), ...]\n",
    "    \"\"\"\n",
    "    fs = entry[\"fs\"]\n",
    "    T  = len(entry[\"cft_list\"][0])\n",
    "    H  = int(3600 * fs)\n",
    "    segs = [(s, min(s+H, T)) for s in range(0, T, H)]\n",
    "    entry[\"hour_segments\"] = segs\n",
    "    entry[\"q_day\"]  = []\n",
    "    entry[\"q_hour\"] = []\n",
    "    for cft in entry[\"cft_list\"]:\n",
    "        qd = {float(q): float(np.quantile(cft, q)) for q in q_values}\n",
    "        entry[\"q_day\"].append(qd)\n",
    "        qh = []\n",
    "        for (s,e) in segs:\n",
    "            seg = cft[s:e]\n",
    "            if seg.size == 0:\n",
    "                qh.append({float(q): 0.0 for q in q_values})\n",
    "            else:\n",
    "                qh.append({float(q): float(np.quantile(seg, q)) for q in q_values})\n",
    "        entry[\"q_hour\"].append(qh)\n",
    "\n",
    "# -------------------- Fusion & metric helpers --------------------\n",
    "def to_onoff_array(x):\n",
    "    \"\"\"Normalize trigger_onset output to a (N,2) int ndarray (handles list/array).\"\"\"\n",
    "    arr = np.asarray(x, dtype=int)\n",
    "    if arr.size == 0: return np.empty((0, 2), dtype=int)\n",
    "    if arr.ndim == 1:\n",
    "        if arr.shape[0] % 2 != 0: raise ValueError(\"Trigger on/off array length must be even.\")\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    elif arr.shape[1] != 2:\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    return arr\n",
    "\n",
    "def picks_or_then_nms(onoffs, fs, t0, nms_sec=30, refract=60):\n",
    "    \"\"\"\n",
    "    OR fusion of ON starts across bands -> NMS (min spacing) -> refractory thinning.\n",
    "    Returns list of pandas.Timestamp (ascending).\n",
    "    \"\"\"\n",
    "    picks = []\n",
    "    for arr in onoffs:\n",
    "        if arr is None or len(arr) == 0: continue\n",
    "        for a, b in arr:\n",
    "            picks.append(t0 + a / fs)  # UTCDateTime (float seconds internally)\n",
    "    if not picks: return []\n",
    "    picks.sort()\n",
    "    fused, last = [], None\n",
    "    for t in picks:\n",
    "        if (last is None) or ((t - last) > nms_sec):\n",
    "            fused.append(t); last = t\n",
    "    out, last = [], None\n",
    "    for t in fused:\n",
    "        if (last is None) or ((t - last) > refract):\n",
    "            out.append(pd.Timestamp(UTCDateTime(t).datetime)); last = t\n",
    "    return out\n",
    "\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    \"\"\"Hit if any trigger ‚àà [start, end] for each event (vectorized).\"\"\"\n",
    "    if trigs_ns.size == 0: return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Build caches --------------------\n",
    "log(\"Building test events...\")\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "log(\"Building CFTs & quantiles per day...\")\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for di, dt in enumerate(event_days, 1):\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp): continue\n",
    "    entry = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    precompute_quantiles(entry, CFG.GRID_Q)  # precompute for all candidate q\n",
    "    cft_cache[dt] = entry\n",
    "    total_hours += entry[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "    if di % 2 == 0: log(f\"  {di} day(s) processed...\")\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "log(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "\n",
    "# -------------------- Core evaluator (uses quantile cache) --------------------\n",
    "def run_or_nms(q, alpha, refract, nms_sec, adapt_scope):\n",
    "    picks_all = []\n",
    "    for dt in dates_ok:\n",
    "        entry = cft_cache[dt]; fs = entry[\"fs\"]\n",
    "        onoffs = []\n",
    "        for b, cft in enumerate(entry[\"cft_list\"]):\n",
    "            if adapt_scope == \"hour\":\n",
    "                arrs = []\n",
    "                for (seg_idx, (s,e)) in enumerate(entry[\"hour_segments\"]):\n",
    "                    base = entry[\"q_hour\"][b][seg_idx].get(float(q), 0.0)\n",
    "                    on_val = base * alpha\n",
    "                    raw = trigger_onset(cft[s:e], on_val, CFG.OFF)\n",
    "                    _onoff = to_onoff_array(raw)\n",
    "                    if _onoff.size:\n",
    "                        _onoff[:,0] += s; _onoff[:,1] += s\n",
    "                    arrs.append(_onoff)\n",
    "                onoff = np.vstack(arrs) if len(arrs) else np.empty((0,2), dtype=int)\n",
    "            elif adapt_scope == \"day\":\n",
    "                base = entry[\"q_day\"][b].get(float(q), 0.0)\n",
    "                on_val = base * alpha\n",
    "                raw = trigger_onset(cft, on_val, CFG.OFF)\n",
    "                onoff = to_onoff_array(raw)\n",
    "            else:\n",
    "                raise ValueError(\"adapt_scope must be 'hour' or 'day'\")\n",
    "            onoffs.append(onoff)\n",
    "        picks = picks_or_then_nms(onoffs, fs, entry[\"t0\"], nms_sec=int(nms_sec), refract=int(refract))\n",
    "        picks_all.extend(picks)\n",
    "\n",
    "    trigs_ns = np.array(sorted(picks_all), dtype=\"datetime64[ns]\")\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    return dict(q=q, alpha=alpha, refract=int(refract), nms_sec=int(nms_sec),\n",
    "                adapt=adapt_scope, recall=recall, fph=fph, triggers=int(trigs_ns.size), trigs=trigs_ns)\n",
    "\n",
    "# -------------------- Search small grid (fast) --------------------\n",
    "rows = []\n",
    "total_combos = (len(CFG.ADAPT_SCOPES)*len(CFG.GRID_Q)*\n",
    "                len(CFG.GRID_ALPHA)*len(CFG.GRID_REFRACT)*len(CFG.GRID_NMS_SEC))\n",
    "log(f\"Scanning {total_combos} combos...\")\n",
    "done = 0\n",
    "for scope in CFG.ADAPT_SCOPES:\n",
    "    for q in CFG.GRID_Q:\n",
    "        for a in CFG.GRID_ALPHA:\n",
    "            for rf in CFG.GRID_REFRACT:\n",
    "                for nms in CFG.GRID_NMS_SEC:\n",
    "                    r = run_or_nms(q, a, rf, nms, scope)\n",
    "                    rows.append({k:v for k,v in r.items() if k!='trigs'})\n",
    "                    done += 1\n",
    "                    if done % 10 == 0:\n",
    "                        log(f\"  scanned {done}/{total_combos} combos\")\n",
    "\n",
    "scan = pd.DataFrame(rows).sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "log(\"Top candidates (by recall then lower FPH):\")\n",
    "log(scan.head(12).to_string(index=False))\n",
    "\n",
    "# Choose best under budget; else lowest-FPH overall\n",
    "cands = scan[scan[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "if len(cands) > 0:\n",
    "    chosen = cands.sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "    note = \"[Chosen UNDER budget]\"\n",
    "else:\n",
    "    chosen = scan.sort_values([\"fph\",\"recall\"], ascending=[True, False]).iloc[0]\n",
    "    note = \"[WARN] No combo meets budget; picking lowest-FPH overall\"\n",
    "\n",
    "log(f\"{note} q={chosen['q']} a={chosen['alpha']} rf={int(chosen['refract'])} \"\n",
    "    f\"nms={int(chosen['nms_sec'])} sc={chosen['adapt']} | \"\n",
    "    f\"recall={chosen['recall']:.3f} FPH={chosen['fph']:.3f} triggers‚âà{int(chosen['triggers'])}\")\n",
    "\n",
    "# -------------------- Finalize (re-run chosen & save CSV) --------------------\n",
    "def finalize_and_save(q, a, rf, nms, scope):\n",
    "    res = run_or_nms(q, a, rf, nms, scope)\n",
    "    trigs_ns = res[\"trigs\"]\n",
    "    mode_tag = \"ornms\"\n",
    "    csv_name = f\"triggers_MB_{mode_tag}_q{q}_a{a}_rf{rf}_nms{nms}_sc{scope}.csv\"\n",
    "    csv_out = os.path.join(CFG.OUT_DIR, csv_name)\n",
    "    if trigs_ns.size == 0:\n",
    "        pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    else:\n",
    "        trig_ts = pd.to_datetime(trigs_ns)\n",
    "        date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "        trig_df = pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "        trig_df.to_csv(csv_out, index=False)\n",
    "    log(f\"[FINAL] q={q} a={a} rf={rf} nms={nms} sc={scope} | recall={res['recall']:.3f} FPH={res['fph']:.3f} triggers={int(res['triggers'])}\")\n",
    "    log(f\"[Saved] {csv_out} (rows={int(res['triggers'])})\")\n",
    "\n",
    "finalize_and_save(float(chosen[\"q\"]), float(chosen[\"alpha\"]), int(chosen[\"refract\"]), int(chosen[\"nms_sec\"]), str(chosen[\"adapt\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f2dbaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:57] Building test events...\n",
      "[13:24:57] Resolving trigger CSV...\n",
      "[13:24:57] [Auto-picked latest trigger CSV] runs/cascade_eval/triggers_MB_ornms_q0.997_a1.05_rf300_nms240_scday.csv\n",
      "[13:24:57] Triggers loaded: 2433 (showing first 3)\n",
      "                trigger_time        date\n",
      "0 2011-03-01 00:02:45.119500  2011-03-01\n",
      "1 2011-03-01 00:16:13.769500  2011-03-01\n",
      "2 2011-03-01 00:28:27.219500  2011-03-01\n",
      "[13:24:57] Preparing detection-eval windows...\n",
      "[13:24:57] [Info] events=1392 | hits=743 | hit_rate=0.534\n",
      "[13:24:57] Cutting windows for 743 hits (win_len=1800)...\n",
      "[13:24:57]  Cut 100/743 windows...\n",
      "[13:24:57]  Cut 200/743 windows...\n",
      "[13:24:57]  Cut 300/743 windows...\n",
      "[13:24:57]  Cut 400/743 windows...\n",
      "[13:24:57]  Cut 500/743 windows...\n",
      "[13:24:58]  Cut 600/743 windows...\n",
      "[13:24:58]  Cut 700/743 windows...\n",
      "[13:24:58] [Cut] windows=743 | shape=(743, 1800) | kept=743\n",
      "[13:24:58] Computing TRAIN-only mean/std for z-score...\n",
      "[13:24:58] Z-score done.\n",
      "[13:24:58] Using device: cpu\n",
      "[13:24:58] Loading model checkpoint...\n",
      "[13:24:58] Model ready.\n",
      "[13:24:58] Running inference...\n",
      "[13:25:04]  Inferred 256/743\n",
      "[13:25:10]  Inferred 512/743\n",
      "[13:25:14]  Inferred 743/743\n",
      "[13:25:14] Inference done.\n",
      "\n",
      "== CNN (cascade on detected events) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9571    0.9204    0.9383       678\n",
      "           M     0.3372    0.5000    0.4028        58\n",
      "           L     0.6000    0.4286    0.5000         7\n",
      "\n",
      "    accuracy                         0.8829       743\n",
      "   macro avg     0.6314    0.6163    0.6137       743\n",
      "weighted avg     0.9053    0.8829    0.8924       743\n",
      "\n",
      "Confusion matrix:\n",
      " [[624  53   1]\n",
      " [ 28  29   1]\n",
      " [  0   4   3]]\n",
      "[End-to-End] S recall = 0.489 (total 1277)\n",
      "[End-to-End] M recall = 0.282 (total 103)\n",
      "[End-to-End] L recall = 0.250 (total 12)\n",
      "[13:25:14] Saved report to runs/cascade_eval/cascade_v2_report.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cascade v2 (verbose): triggers -> re-center -> cut -> z-score -> strict CNN\n",
    "# - Verbose progress logs with timestamps\n",
    "# - FORCE_DEVICE option (cpu/cuda/auto; MPS skipped by default)\n",
    "# - DRY_RUN_LIMIT for quick smoke tests\n",
    "# - Robust trigger CSV resolver (or set CFG.TRIG_CSV explicitly)\n",
    "# - Wider re-centering; TRAIN-only z-score; strict CNN\n",
    "# - Optional softmax remap to lift M/L recall slightly\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, time, json, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.filter import envelope\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # If empty, resolver will auto-pick newest triggers_MB_*.csv in OUT_DIR\n",
    "    TRIG_CSV   : str = \"\"  # e.g., \"runs/cascade_eval/triggers_MB_ornms_q0.997_a1.05_rf300_nms240_scday.csv\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "    BAND_CUT   : tuple = (0.5, 8.0)  # band for cutting windows\n",
    "\n",
    "    # CNN window (must match training)\n",
    "    PRE        : int = 20\n",
    "    POST       : int = 70\n",
    "\n",
    "    # Detection-eval window (match detection eval)\n",
    "    PRE_DET    : int = 20\n",
    "    POST_DET   : int = 300\n",
    "\n",
    "    # Wider re-centering window (coarse + fine)\n",
    "    RC_PRE     : int = 20\n",
    "    RC_POST    : int = 40\n",
    "\n",
    "    # Strict CNN checkpoint (same arch as training)\n",
    "    BEST_PT    : str = \"runs/cnn_strict/best.pt\"\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "    # Post-softmax remapping (optional)\n",
    "    REMAP_ENABLE  : bool  = True\n",
    "    REMAP_M_THRES : float = 0.33\n",
    "    REMAP_L_THRES : float = 0.20\n",
    "\n",
    "    # ---------- Stability & speed ----------\n",
    "    FORCE_DEVICE  : str = \"auto\"  # \"cpu\" | \"cuda\" | \"auto\" (auto prefers CUDA; MPS skipped)\n",
    "    BATCH_SIZE    : int = 256\n",
    "    DRY_RUN_LIMIT : int = 0       # set to 50 for a quick smoke test\n",
    "    PRINT_EVERY   : int = 100     # progress print frequency\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "def log(msg): print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "LABEL_MAP = {\"S\":0, \"M\":1, \"L\":2}\n",
    "\n",
    "# ---------- Robust trigger CSV resolver ----------\n",
    "def resolve_trig_csv(cfg):\n",
    "    if cfg.TRIG_CSV and os.path.exists(cfg.TRIG_CSV):\n",
    "        log(f\"[Using provided trigger CSV] {cfg.TRIG_CSV}\")\n",
    "        return cfg.TRIG_CSV\n",
    "    pattern = os.path.join(cfg.OUT_DIR, \"triggers_MB_*.csv\")\n",
    "    cands = sorted(glob.glob(pattern), key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    if cands:\n",
    "        log(f\"[Auto-picked latest trigger CSV] {cands[0]}\")\n",
    "        return cands[0]\n",
    "    raise FileNotFoundError(f\"No trigger CSV found in {cfg.OUT_DIR} matching 'triggers_MB_*.csv'\")\n",
    "\n",
    "# -------------------- Data helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files: return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path); df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(cfg.PRE, \"s\"))\n",
    "    y_te = pd.Series(y[mte]).reset_index(drop=True)\n",
    "    order = np.argsort(evt_time.values)\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time.values[order]})\n",
    "    y_te_sorted = y_te.iloc[order].to_numpy()\n",
    "    return df_te.reset_index(drop=True), y_te_sorted\n",
    "\n",
    "def train_mean_std(cfg: Cfg):\n",
    "    X, y, sid, _ = load_npz_pos(cfg.NPZ_PATH)\n",
    "    tr_scope, _ = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mtr = np.isin(sid, list(tr_scope))\n",
    "    flat = X[mtr].reshape(np.sum(mtr), -1)\n",
    "    mean, std = float(flat.mean()), float(flat.std() + 1e-8)\n",
    "    with open(os.path.join(cfg.OUT_DIR, \"mean_std_from_train.json\"), \"w\") as f:\n",
    "        json.dump({\"mean\": mean, \"std\": std}, f, indent=2)\n",
    "    return mean, std\n",
    "\n",
    "# -------------------- Waveform IO --------------------\n",
    "_trace_cache = {}\n",
    "def get_trace_for_day(cfg: Cfg, day_str: str):\n",
    "    if day_str not in _trace_cache:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=day_str))\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        if abs(tr.stats.sampling_rate - cfg.FS) > 1e-6:\n",
    "            tr.resample(cfg.FS)\n",
    "        tr.detrend(\"demean\")\n",
    "        tr.filter(\"bandpass\", freqmin=cfg.BAND_CUT[0], freqmax=cfg.BAND_CUT[1])\n",
    "        _trace_cache[day_str] = tr\n",
    "    return _trace_cache[day_str]\n",
    "\n",
    "def recenter_trigger(tr, t_ts, fs, pre, post):\n",
    "    \"\"\"\n",
    "    Find an envelope peak near the trigger to center the CNN window.\n",
    "    Coarse search (¬±pre/post), then fine re-peak in ¬±5 s window.\n",
    "    \"\"\"\n",
    "    t0 = UTCDateTime(t_ts.to_pydatetime()) - pre\n",
    "    t1 = UTCDateTime(t_ts.to_pydatetime()) + post\n",
    "    x = tr.slice(t0, t1).data.astype(np.float32, copy=False)\n",
    "    need = int((pre + post) * fs)\n",
    "    if len(x) < need:\n",
    "        return t_ts\n",
    "    env = envelope(x)\n",
    "    i = int(np.argmax(env))\n",
    "    fine = int(5 * fs)\n",
    "    a = max(0, i - fine); b = min(len(env), i + fine + 1)\n",
    "    j = a + int(np.argmax(env[a:b]))\n",
    "    t_pk = t0 + j / fs\n",
    "    return pd.Timestamp(UTCDateTime(t_pk).datetime)\n",
    "\n",
    "# -------------------- Strict CNN --------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k // 2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn   = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x); x = self.bn(x); x = F.gelu(x); x = self.pool(x); return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__(); self.W = nn.Linear(d, d); self.v = nn.Linear(d, 1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U = torch.tanh(self.W(H)); a = self.v(U).squeeze(-1); a = torch.softmax(a, dim=1)\n",
    "        Z = torch.bmm(a.unsqueeze(1), H).squeeze(1); return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(ConvBlock(in_ch, 32), ConvBlock(32, 64), ConvBlock(64, 128))\n",
    "        self.lstm = nn.LSTM(128, hidden, num_layers=layers, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2 * hidden)\n",
    "        self.head = nn.Sequential(nn.Linear(2 * hidden, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, n_classes))\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x)          # [B,C,L]\n",
    "        z = z.transpose(1, 2)    # [B,L,C]\n",
    "        H, _ = self.lstm(z)      # [B,L,2H]\n",
    "        Z, _ = self.attn(H)      # [B,2H]\n",
    "        return self.head(Z)\n",
    "\n",
    "def softmax_remap(logits, enable=False, m_th=0.33, l_th=0.20):\n",
    "    preds = logits.argmax(1).cpu().numpy()\n",
    "    if not enable: return preds\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    pM, pL = probs[:,1], probs[:,2]\n",
    "    for n in range(len(preds)):\n",
    "        if pL[n] >= l_th: preds[n] = 2\n",
    "        elif (pM[n] >= m_th) and (preds[n] == 0): preds[n] = 1\n",
    "    return preds\n",
    "\n",
    "# -------------------- Main --------------------\n",
    "log(\"Building test events...\")\n",
    "df_te, y_te_sorted = build_test_events(CFG)\n",
    "\n",
    "log(\"Resolving trigger CSV...\")\n",
    "trig_csv_path = resolve_trig_csv(CFG)\n",
    "trig_df = pd.read_csv(trig_csv_path)\n",
    "if \"trigger_time\" not in trig_df.columns:\n",
    "    raise ValueError(f\"'trigger_time' column not found in {trig_csv_path}\")\n",
    "trig_df[\"trigger_time\"] = pd.to_datetime(trig_df[\"trigger_time\"], errors=\"coerce\", utc=False)\n",
    "trig_df = trig_df.dropna(subset=[\"trigger_time\"]).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "log(f\"Triggers loaded: {len(trig_df)} (showing first 3)\\n{trig_df.head(3)}\")\n",
    "\n",
    "log(\"Preparing detection-eval windows...\")\n",
    "ev = df_te.copy()\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "\n",
    "tr       = trig_df[\"trigger_time\"].values.astype(\"datetime64[ns]\")\n",
    "ev_start = ev[\"start\"].values.astype(\"datetime64[ns]\")\n",
    "ev_end   = ev[\"end\"].values.astype(\"datetime64[ns]\")\n",
    "\n",
    "i = np.searchsorted(tr, ev_start, side=\"left\")\n",
    "j = np.searchsorted(tr, ev_end,   side=\"right\")\n",
    "hit = (j - i) > 0\n",
    "first_idx = np.where(hit, i, -1)\n",
    "log(f\"[Info] events={len(ev)} | hits={int(hit.sum())} | hit_rate={hit.mean():.3f}\")\n",
    "\n",
    "# -------------------- Cut windows --------------------\n",
    "win_len = CFG.FS * (CFG.PRE + CFG.POST)\n",
    "X_cut, y_hit, kept = [], [], []\n",
    "hit_indices = np.where(hit)[0]\n",
    "if CFG.DRY_RUN_LIMIT > 0:\n",
    "    hit_indices = hit_indices[:CFG.DRY_RUN_LIMIT]\n",
    "log(f\"Cutting windows for {len(hit_indices)} hits (win_len={win_len})...\")\n",
    "\n",
    "_trace_cache = {}\n",
    "def get_trace_for_day_cached(day_str):\n",
    "    if day_str not in _trace_cache:\n",
    "        _trace_cache[day_str] = get_trace_for_day(CFG, day_str)\n",
    "    return _trace_cache[day_str]\n",
    "\n",
    "for k, idx in enumerate(hit_indices, 1):\n",
    "    t_first_ns = tr[first_idx[idx]]\n",
    "    t_first = pd.Timestamp(t_first_ns)\n",
    "    day_str = str(t_first.date())\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=day_str))\n",
    "    if not os.path.exists(fp): \n",
    "        continue\n",
    "    tr_day = get_trace_for_day_cached(day_str)\n",
    "    t_center = recenter_trigger(tr_day, t_first, fs=CFG.FS, pre=CFG.RC_PRE, post=CFG.RC_POST)\n",
    "    t0 = UTCDateTime(t_center.to_pydatetime()) - CFG.PRE\n",
    "    t1 = UTCDateTime(t_center.to_pydatetime()) + CFG.POST\n",
    "    x = tr_day.slice(t0, t1).data\n",
    "    if len(x) >= win_len:\n",
    "        X_cut.append(x[:win_len].astype(np.float32))\n",
    "        y_hit.append(y_te_sorted[idx])\n",
    "        kept.append(idx)\n",
    "    if (k % CFG.PRINT_EVERY) == 0:\n",
    "        log(f\" Cut {k}/{len(hit_indices)} windows...\")\n",
    "\n",
    "if len(X_cut) == 0:\n",
    "    raise SystemExit(\"No windows cut; check waveform paths or time bounds.\")\n",
    "\n",
    "X_cut = np.stack(X_cut, axis=0)\n",
    "y_hit = np.array(y_hit, dtype=int)\n",
    "log(f\"[Cut] windows={len(X_cut)} | shape={X_cut.shape} | kept={len(kept)}\")\n",
    "\n",
    "# -------------------- Normalize --------------------\n",
    "log(\"Computing TRAIN-only mean/std for z-score...\")\n",
    "mean, std = train_mean_std(CFG)\n",
    "Xn = (X_cut - mean) / (std if std > 0 else 1.0)\n",
    "Xn = Xn[:, None, :]  # [N,1,T]\n",
    "log(\"Z-score done.\")\n",
    "\n",
    "# -------------------- Device & model --------------------\n",
    "def choose_device(pref: str):\n",
    "    if pref.lower() == \"cpu\": return torch.device(\"cpu\")\n",
    "    if pref.lower() == \"cuda\" and torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = choose_device(CFG.FORCE_DEVICE)\n",
    "log(f\"Using device: {device}\")\n",
    "if not os.path.exists(CFG.BEST_PT):\n",
    "    raise FileNotFoundError(f\"CNN checkpoint not found: {CFG.BEST_PT}\")\n",
    "\n",
    "log(\"Loading model checkpoint...\")\n",
    "model = CNNBiLSTMAttn(in_ch=1, n_classes=3).to(device)\n",
    "state = torch.load(CFG.BEST_PT, map_location=device)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "log(\"Model ready.\")\n",
    "\n",
    "# -------------------- Inference --------------------\n",
    "log(\"Running inference...\")\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i0 in range(0, len(Xn), CFG.BATCH_SIZE):\n",
    "        xb = torch.from_numpy(Xn[i0:i0+CFG.BATCH_SIZE]).to(device)\n",
    "        logits = model(xb)\n",
    "        preds = softmax_remap(logits, enable=CFG.REMAP_ENABLE,\n",
    "                              m_th=CFG.REMAP_M_THRES, l_th=CFG.REMAP_L_THRES)\n",
    "        y_pred.extend(preds)\n",
    "        if ((i0 // CFG.BATCH_SIZE + 1) % max(1, CFG.PRINT_EVERY // max(1, CFG.BATCH_SIZE))) == 0:\n",
    "            log(f\" Inferred {min(i0+CFG.BATCH_SIZE, len(Xn))}/{len(Xn)}\")\n",
    "\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "log(\"Inference done.\")\n",
    "\n",
    "# -------------------- Reports --------------------\n",
    "rep = classification_report(y_hit, y_pred, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0)\n",
    "cm = confusion_matrix(y_hit, y_pred, labels=[0,1,2])\n",
    "macro_f1 = f1_score(y_hit, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n== CNN (cascade on detected events) ==\")\n",
    "print(rep)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# End-to-end per-class recall (missed detections count as errors)\n",
    "lab = y_te_sorted\n",
    "correct_mask = np.zeros_like(lab, dtype=bool)\n",
    "kept_idx = np.array(kept, dtype=int)\n",
    "for k, idx in enumerate(kept_idx):\n",
    "    if y_hit[k] == y_pred[k]:\n",
    "        correct_mask[idx] = True\n",
    "\n",
    "for c, name in enumerate([\"S\",\"M\",\"L\"]):\n",
    "    total_c = (lab == c).sum()\n",
    "    e2e_c = correct_mask[lab == c].sum() / max(1, total_c)\n",
    "    print(f\"[End-to-End] {name} recall = {e2e_c:.3f} (total {total_c})\")\n",
    "\n",
    "# Save artifacts\n",
    "out_report = os.path.join(CFG.OUT_DIR, \"cascade_v2_report.txt\")\n",
    "with open(out_report, \"w\") as f:\n",
    "    f.write(rep + \"\\n\")\n",
    "    f.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "np.savez(os.path.join(CFG.OUT_DIR, \"cascade_v2_preds.npz\"),\n",
    "         y_true=y_hit, y_pred=y_pred, kept_event_indices=np.array(kept, dtype=int))\n",
    "log(f\"Saved report to {out_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68285656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Auto-picked latest trigger CSV] runs/cascade_eval/triggers_MB_ornms_q0.997_a1.05_rf300_nms240_scday.csv\n",
      "\n",
      "================= FPH SUMMARY =================\n",
      "Triggers CSV: runs/cascade_eval/triggers_MB_ornms_q0.997_a1.05_rf300_nms240_scday.csv\n",
      "Days considered: 17  |  Total hours: 408.000\n",
      "Total triggers (used): 2433  |  Overall FPH: 5.963 per hour\n",
      "===============================================\n",
      "\n",
      "Top-10 days by FPH:\n",
      "      date  triggers     hours      fph\n",
      "2011-03-04       171 23.999986 7.125004\n",
      "2011-03-03       167 23.999986 6.958337\n",
      "2011-03-01       165 23.999986 6.875004\n",
      "2011-03-07       164 23.999986 6.833337\n",
      "2011-03-05       154 23.999986 6.416670\n",
      "2011-03-10       153 23.999986 6.375004\n",
      "2011-03-13       143 23.999986 5.958337\n",
      "2011-03-12       140 23.999986 5.833337\n",
      "2011-03-09       140 23.999986 5.833337\n",
      "2011-03-14       135 23.999986 5.625003\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FPH Checker for Multiband STA/LTA Triggers\n",
    "# - Build the exact day set used by detection:\n",
    "#   days = (unique test event dates) ‚à© (dates with available mseed files)\n",
    "# - Load triggers CSV (auto-pick latest if not provided), filter to those days\n",
    "# - Compute total_hours from mseed coverage and FPH = triggers / total_hours\n",
    "# - Print overall summary and per-day breakdown\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, json\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "\n",
    "    TRIG_CSV   : str = \"\"   # if empty, auto-pick latest triggers_MB_*.csv\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files: return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_event_days(cfg: Cfg):\n",
    "    \"\"\"Return sorted unique dates of TEST positives (window_start + 20s by convention).\"\"\"\n",
    "    _, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    return sorted(set(pd.to_datetime(evt_time).dt.date))\n",
    "\n",
    "def resolve_trig_csv(cfg: Cfg):\n",
    "    if cfg.TRIG_CSV and os.path.exists(cfg.TRIG_CSV):\n",
    "        print(f\"[Using provided trigger CSV] {cfg.TRIG_CSV}\")\n",
    "        return cfg.TRIG_CSV\n",
    "    pattern = os.path.join(cfg.OUT_DIR, \"triggers_MB_*.csv\")\n",
    "    cands = sorted(glob.glob(pattern), key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trigger CSV found in {cfg.OUT_DIR} matching 'triggers_MB_*.csv'.\")\n",
    "    print(f\"[Auto-picked latest trigger CSV] {cands[0]}\")\n",
    "    return cands[0]\n",
    "\n",
    "def compute_hours_for_days(cfg: Cfg, days):\n",
    "    per_day_hours, days_ok, total_hours = {}, [], 0.0\n",
    "    for dt in days:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=dt))\n",
    "        if not os.path.exists(fp): continue\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        hours = float((tr.stats.endtime - tr.stats.starttime) / 3600.0)\n",
    "        per_day_hours[dt] = hours\n",
    "        total_hours += hours\n",
    "        days_ok.append(dt)\n",
    "    return per_day_hours, total_hours, days_ok\n",
    "\n",
    "def check_fph(cfg: Cfg):\n",
    "    event_days = build_test_event_days(cfg)\n",
    "    per_day_hours, total_hours, days_ok = compute_hours_for_days(cfg, event_days)\n",
    "    if not days_ok:\n",
    "        raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "\n",
    "    trig_csv = resolve_trig_csv(cfg)\n",
    "    df = pd.read_csv(trig_csv)\n",
    "    if \"trigger_time\" not in df.columns:\n",
    "        raise ValueError(f\"'trigger_time' column not found in {trig_csv}\")\n",
    "    df[\"trigger_time\"] = pd.to_datetime(df[\"trigger_time\"], errors=\"coerce\", utc=False)\n",
    "    df = df.dropna(subset=[\"trigger_time\"]).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "    df[\"date\"] = df[\"trigger_time\"].dt.date\n",
    "\n",
    "    n_total_trigs = len(df)\n",
    "    df = df[df[\"date\"].isin(days_ok)].copy()\n",
    "    n_used_trigs = len(df)\n",
    "    n_dropped = n_total_trigs - n_used_trigs\n",
    "    if n_dropped > 0:\n",
    "        print(f\"[Note] Dropped {n_dropped} triggers outside the detection day set (kept {n_used_trigs}).\")\n",
    "\n",
    "    if total_hours <= 0: raise ValueError(\"Total hours computed as zero; check your mseed files.\")\n",
    "    fph = n_used_trigs / total_hours\n",
    "\n",
    "    per_day = df.groupby(\"date\")[\"trigger_time\"].count().rename(\"triggers\").reset_index()\n",
    "    per_day[\"hours\"] = per_day[\"date\"].map(per_day_hours).astype(float)\n",
    "    per_day[\"fph\"] = per_day[\"triggers\"] / per_day[\"hours\"].replace(0, np.nan)\n",
    "\n",
    "    print(\"\\n================= FPH SUMMARY =================\")\n",
    "    print(f\"Triggers CSV: {trig_csv}\")\n",
    "    print(f\"Days considered: {len(days_ok)}  |  Total hours: {total_hours:.3f}\")\n",
    "    print(f\"Total triggers (used): {n_used_trigs}  |  Overall FPH: {fph:.3f} per hour\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    if len(per_day) > 0:\n",
    "        per_day_sorted = per_day.sort_values(\"fph\", ascending=False)\n",
    "        print(\"\\nTop-10 days by FPH:\")\n",
    "        print(per_day_sorted.head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No triggers within the considered day set; FPH = 0.\")\n",
    "\n",
    "    return dict(fph=fph, total_hours=total_hours, n_trigs=n_used_trigs, per_day=per_day)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _ = check_fph(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11ce1126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:36:31] Building test events...\n",
      "[13:36:31] Building CFTs & quantiles per day...\n",
      "[13:36:31]   2 day(s) processed...\n",
      "[13:36:32]   4 day(s) processed...\n",
      "[13:36:32]   6 day(s) processed...\n",
      "[13:36:32]   8 day(s) processed...\n",
      "[13:36:32]   10 day(s) processed...\n",
      "[13:36:33]   12 day(s) processed...\n",
      "[13:36:33]   14 day(s) processed...\n",
      "[13:36:33]   16 day(s) processed...\n",
      "[13:36:34] [Info] events considered: 1392 on 17 days | hours=408.0\n",
      "[13:36:34] Scanning 3 combos...\n",
      "[13:36:34]   scanned 1/3 combos\n",
      "[13:36:34]   scanned 2/3 combos\n",
      "[13:36:34]   scanned 3/3 combos\n",
      "[13:36:34] Top candidates (by recall then lower FPH):\n",
      "[13:36:34]     q  alpha  refract  nms_sec adapt   recall      fph  triggers\n",
      "0.997   1.03      300      360  hour 0.540230 6.367651      2598\n",
      "0.997   1.03      300      390  hour 0.515805 6.056376      2471\n",
      "0.997   1.03      300      420  hour 0.496408 5.747552      2345\n",
      "[13:36:34] [Chosen UNDER budget] q=0.997 a=1.03 rf=300 nms=420 sc=hour | recall=0.496 FPH=5.748 triggers‚âà2345\n",
      "[13:36:35] [FINAL] q=0.997 a=1.03 rf=300 nms=420 sc=hour | recall=0.496 FPH=5.748 triggers=2345\n",
      "[13:36:35] [Saved] runs/cascade_eval/triggers_MB_ornms_q0.997_a1.03_rf300_nms420_schour.csv (rows=2345)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multiband STA/LTA Detection v2 ‚Äî Scheme A (Auto-budget, hour-adapt)\n",
    "# Goal:\n",
    "#   - Push recall up by using hour-level adaptive thresholds,\n",
    "#     while keeping FPH <= 6/h by tightening NMS (330/360/390s)\n",
    "# Choices:\n",
    "#   - Fusion: OR + NMS (recall-friendly, then thin)\n",
    "#   - Hour-level quantiles (robust to drift)\n",
    "#   - Conservative 3-band front-end to curb low-freq false alarms\n",
    "# Output:\n",
    "#   runs/cascade_eval/triggers_MB_ornms_q{q}_a{a}_rf{rf}_nms{nms}_schour.csv\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "# -------------------- Config (Scheme A) --------------------\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "\n",
    "    # Bands: conservative 3-band set to reduce LF false alarms\n",
    "    BANDS      : tuple = ((0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "    # If you later want extra recall, add low band back:\n",
    "    # BANDS      : tuple = ((0.1, 1.0), (0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA windows and OFF hysteresis\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "    OFF        : float = 1.0\n",
    "\n",
    "    # Detection-eval window (for hit-rate; not for CNN cutting)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300\n",
    "\n",
    "    # -------- Tiny grid for Scheme A (hour-adapt only) --------\n",
    "    GRID_Q        : tuple = (0.997,)\n",
    "    GRID_ALPHA    : tuple = (1.03,)\n",
    "    GRID_REFRACT  : tuple = (300,)            # strong refractory\n",
    "    GRID_NMS_SEC  : tuple = (360, 390, 420)   # tighten NMS to curb FPH\n",
    "    ADAPT_SCOPES  : tuple = (\"hour\",)         # hour-level only\n",
    "\n",
    "    # Budget\n",
    "    FPH_BUDGET    : float = 6.0\n",
    "\n",
    "    # Output directory\n",
    "    OUT_DIR       : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "\n",
    "def log(msg): print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "# -------------------- NPZ & split helpers --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files:\n",
    "            return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    \"\"\"Load positives and return (X, y, sid, window_start) filtered to detect_label==1 if available.\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))  # fix: map(str, ...)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"Test events = TEST positives; event_time = window_start + 20s (dataset convention).\"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "    return df_te\n",
    "\n",
    "# -------------------- CFT build & quantile precompute --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    \"\"\"Read MiniSEED; resample & detrend; per band: bandpass -> classic_sta_lta.\"\"\"\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6:\n",
    "        tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime) / 3600.0)\n",
    "    cft_list = []\n",
    "    for (fmin, fmax) in bands:\n",
    "        tr = tr0.copy()\n",
    "        tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta * fs), int(lta * fs))\n",
    "        cft_list.append(cft)\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "def precompute_quantiles(entry, q_values):\n",
    "    \"\"\"\n",
    "    Precompute per-day & per-hour quantiles for all bands and given q values.\n",
    "    Adds:\n",
    "      entry[\"q_day\"][band_idx][q]  and\n",
    "      entry[\"q_hour\"][band_idx][hour_idx][q]\n",
    "      entry[\"hour_segments\"] = [(s,e), ...]\n",
    "    \"\"\"\n",
    "    fs = entry[\"fs\"]\n",
    "    T  = len(entry[\"cft_list\"][0])\n",
    "    H  = int(3600 * fs)\n",
    "    segs = [(s, min(s+H, T)) for s in range(0, T, H)]\n",
    "    entry[\"hour_segments\"] = segs\n",
    "    entry[\"q_day\"]  = []\n",
    "    entry[\"q_hour\"] = []\n",
    "    for cft in entry[\"cft_list\"]:\n",
    "        qd = {float(q): float(np.quantile(cft, q)) for q in q_values}\n",
    "        entry[\"q_day\"].append(qd)\n",
    "        qh = []\n",
    "        for (s,e) in segs:\n",
    "            seg = cft[s:e]\n",
    "            if seg.size == 0:\n",
    "                qh.append({float(q): 0.0 for q in q_values})\n",
    "            else:\n",
    "                qh.append({float(q): float(np.quantile(seg, q)) for q in q_values})\n",
    "        entry[\"q_hour\"].append(qh)\n",
    "\n",
    "# -------------------- Fusion & metric helpers --------------------\n",
    "def to_onoff_array(x):\n",
    "    \"\"\"Normalize trigger_onset output to a (N,2) int ndarray (handles list/array).\"\"\"\n",
    "    arr = np.asarray(x, dtype=int)\n",
    "    if arr.size == 0:\n",
    "        return np.empty((0, 2), dtype=int)\n",
    "    if arr.ndim == 1:\n",
    "        if arr.shape[0] % 2 != 0:\n",
    "            raise ValueError(\"Trigger on/off array length must be even.\")\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    elif arr.shape[1] != 2:\n",
    "        arr = arr.reshape(-1, 2)\n",
    "    return arr\n",
    "\n",
    "def picks_or_then_nms(onoffs, fs, t0, nms_sec=30, refract=60):\n",
    "    \"\"\"\n",
    "    OR fusion of ON starts across bands -> NMS (min spacing) -> refractory thinning.\n",
    "    Returns list of pandas.Timestamp (ascending).\n",
    "    \"\"\"\n",
    "    picks = []\n",
    "    for arr in onoffs:\n",
    "        if arr is None or len(arr) == 0: continue\n",
    "        for a, b in arr:\n",
    "            picks.append(t0 + a / fs)  # UTCDateTime (float seconds internally)\n",
    "    if not picks: return []\n",
    "    picks.sort()\n",
    "    fused, last = [], None\n",
    "    for t in picks:\n",
    "        if (last is None) or ((t - last) > nms_sec):\n",
    "            fused.append(t); last = t\n",
    "    out, last = [], None\n",
    "    for t in fused:\n",
    "        if (last is None) or ((t - last) > refract):\n",
    "            out.append(pd.Timestamp(UTCDateTime(t).datetime)); last = t\n",
    "    return out\n",
    "\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    \"\"\"Hit if any trigger ‚àà [start, end] for each event (vectorized).\"\"\"\n",
    "    if trigs_ns.size == 0:\n",
    "        return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Build caches --------------------\n",
    "log(\"Building test events...\")\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "log(\"Building CFTs & quantiles per day...\")\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for di, dt in enumerate(event_days, 1):\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp): continue\n",
    "    entry = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    precompute_quantiles(entry, CFG.GRID_Q)  # precompute only needed q's\n",
    "    cft_cache[dt] = entry\n",
    "    total_hours += entry[\"hours\"]\n",
    "    dates_ok.append(dt)\n",
    "    if di % 2 == 0:\n",
    "        log(f\"  {di} day(s) processed...\")\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "log(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "\n",
    "# -------------------- Core evaluator (hour-adapt only) --------------------\n",
    "def run_or_nms(q, alpha, refract, nms_sec):\n",
    "    picks_all = []\n",
    "    for dt in dates_ok:\n",
    "        entry = cft_cache[dt]\n",
    "        fs = entry[\"fs\"]\n",
    "        onoffs = []\n",
    "        for b, cft in enumerate(entry[\"cft_list\"]):\n",
    "            # hour-adapt thresholds using precomputed quantiles\n",
    "            arrs = []\n",
    "            for (seg_idx, (s,e)) in enumerate(entry[\"hour_segments\"]):\n",
    "                base = entry[\"q_hour\"][b][seg_idx].get(float(q), 0.0)\n",
    "                on_val = base * alpha\n",
    "                raw = trigger_onset(cft[s:e], on_val, CFG.OFF)\n",
    "                _onoff = to_onoff_array(raw)\n",
    "                if _onoff.size:\n",
    "                    _onoff[:,0] += s; _onoff[:,1] += s\n",
    "                arrs.append(_onoff)\n",
    "            onoff = np.vstack(arrs) if len(arrs) else np.empty((0,2), dtype=int)\n",
    "            onoffs.append(onoff)\n",
    "\n",
    "        # OR + NMS + refractory\n",
    "        picks = picks_or_then_nms(onoffs, fs, entry[\"t0\"], nms_sec=int(nms_sec), refract=int(refract))\n",
    "        picks_all.extend(picks)\n",
    "\n",
    "    trigs_ns = np.array(sorted(picks_all), dtype=\"datetime64[ns]\")\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "    return dict(q=q, alpha=alpha, refract=int(refract), nms_sec=int(nms_sec),\n",
    "                adapt=\"hour\", recall=recall, fph=fph, triggers=int(trigs_ns.size), trigs=trigs_ns)\n",
    "\n",
    "# -------------------- Scan tiny grid (3 combos) --------------------\n",
    "rows = []\n",
    "total_combos = len(CFG.GRID_Q)*len(CFG.GRID_ALPHA)*len(CFG.GRID_REFRACT)*len(CFG.GRID_NMS_SEC)\n",
    "log(f\"Scanning {total_combos} combos...\")\n",
    "done = 0\n",
    "for q in CFG.GRID_Q:\n",
    "    for a in CFG.GRID_ALPHA:\n",
    "        for rf in CFG.GRID_REFRACT:\n",
    "            for nms in CFG.GRID_NMS_SEC:\n",
    "                r = run_or_nms(q, a, rf, nms)\n",
    "                rows.append({k:v for k,v in r.items() if k!='trigs'})\n",
    "                done += 1\n",
    "                log(f\"  scanned {done}/{total_combos} combos\")\n",
    "\n",
    "scan = pd.DataFrame(rows).sort_values([\"recall\",\"fph\"], ascending=[False, True]).reset_index(drop=True)\n",
    "log(\"Top candidates (by recall then lower FPH):\")\n",
    "log(scan.to_string(index=False))\n",
    "\n",
    "# Choose best under budget; else lowest-FPH overall\n",
    "cands = scan[scan[\"fph\"] <= CFG.FPH_BUDGET]\n",
    "if len(cands) > 0:\n",
    "    chosen = cands.sort_values([\"recall\",\"fph\"], ascending=[False, True]).iloc[0]\n",
    "    note = \"[Chosen UNDER budget]\"\n",
    "else:\n",
    "    chosen = scan.sort_values([\"fph\",\"recall\"], ascending=[True, False]).iloc[0]\n",
    "    note = \"[WARN] No combo meets budget; picking lowest-FPH overall\"\n",
    "\n",
    "log(f\"{note} q={chosen['q']} a={chosen['alpha']} rf={int(chosen['refract'])} \"\n",
    "    f\"nms={int(chosen['nms_sec'])} sc=hour | \"\n",
    "    f\"recall={chosen['recall']:.3f} FPH={chosen['fph']:.3f} triggers‚âà{int(chosen['triggers'])}\")\n",
    "\n",
    "# -------------------- Finalize (re-run chosen & save CSV) --------------------\n",
    "def finalize_and_save(q, a, rf, nms):\n",
    "    res = run_or_nms(q, a, rf, nms)\n",
    "    trigs_ns = res[\"trigs\"]\n",
    "    mode_tag = \"ornms\"\n",
    "    csv_name = f\"triggers_MB_{mode_tag}_q{q}_a{a}_rf{rf}_nms{nms}_schour.csv\"\n",
    "    csv_out = os.path.join(CFG.OUT_DIR, csv_name)\n",
    "    if trigs_ns.size == 0:\n",
    "        pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    else:\n",
    "        trig_ts = pd.to_datetime(trigs_ns)\n",
    "        date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "        trig_df = pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "        trig_df.to_csv(csv_out, index=False)\n",
    "    log(f\"[FINAL] q={q} a={a} rf={rf} nms={nms} sc=hour | recall={res['recall']:.3f} FPH={res['fph']:.3f} triggers={int(res['triggers'])}\")\n",
    "    log(f\"[Saved] {csv_out} (rows={int(res['triggers'])})\")\n",
    "\n",
    "finalize_and_save(float(chosen[\"q\"]), float(chosen[\"alpha\"]), int(chosen[\"refract\"]), int(chosen[\"nms_sec\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47a331ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:40:18] Building test events...\n",
      "[13:40:18] Building CFTs & quantiles per day...\n",
      "[13:40:19]   2 day(s) processed...\n",
      "[13:40:19]   4 day(s) processed...\n",
      "[13:40:19]   6 day(s) processed...\n",
      "[13:40:20]   8 day(s) processed...\n",
      "[13:40:20]   10 day(s) processed...\n",
      "[13:40:20]   12 day(s) processed...\n",
      "[13:40:20]   14 day(s) processed...\n",
      "[13:40:21]   16 day(s) processed...\n",
      "[13:40:21] [Info] events considered: 1392 on 17 days | hours=408.0\n",
      "[13:40:21] [Params] q=0.997 alpha=1.03 rf=300s nms=420s scope=hour\n",
      "[13:40:21]   processed 1/17 day(s)\n",
      "[13:40:21]   processed 2/17 day(s)\n",
      "[13:40:21]   processed 3/17 day(s)\n",
      "[13:40:21]   processed 4/17 day(s)\n",
      "[13:40:21]   processed 5/17 day(s)\n",
      "[13:40:21]   processed 6/17 day(s)\n",
      "[13:40:21]   processed 7/17 day(s)\n",
      "[13:40:21]   processed 8/17 day(s)\n",
      "[13:40:21]   processed 9/17 day(s)\n",
      "[13:40:21]   processed 10/17 day(s)\n",
      "[13:40:21]   processed 11/17 day(s)\n",
      "[13:40:21]   processed 12/17 day(s)\n",
      "[13:40:21]   processed 13/17 day(s)\n",
      "[13:40:21]   processed 14/17 day(s)\n",
      "[13:40:21]   processed 15/17 day(s)\n",
      "[13:40:21]   processed 16/17 day(s)\n",
      "[13:40:21]   processed 17/17 day(s)\n",
      "[13:40:21] [Final] recall=0.496 | FPH=5.748 | triggers=2345\n",
      "[13:40:21] [Saved] runs/cascade_eval/triggers_MB_ornms_q0.997_a1.03_rf300_nms420_schour.csv (rows=2345)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multiband STA/LTA ‚Äî Scheme A (FIXED)  FPB <= ~6/h\n",
    "# Fixed params (from your best run under budget):\n",
    "#   q=0.997, alpha=1.03, refractory=300s, NMS=420s, adapt=hour\n",
    "#   Bands: (0.5‚Äì2), (1‚Äì5), (5‚Äì8) Hz  (conservative 3-band)\n",
    "# Output CSV: runs/cascade_eval/triggers_MB_ornms_q0.997_a1.03_rf300_nms420_schour.csv\n",
    "# ============================================================\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    # Project artifacts\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # Continuous waveforms\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "\n",
    "    # Bands (3-band conservative front-end)\n",
    "    BANDS      : tuple = ((0.5, 2.0), (1.0, 5.0), (5.0, 8.0))\n",
    "\n",
    "    # STA/LTA + hysteresis\n",
    "    STA        : float = 1.5\n",
    "    LTA        : float = 20.0\n",
    "    OFF        : float = 1.0\n",
    "\n",
    "    # Detection-eval window (for recall computation)\n",
    "    PRE_DET    : int   = 20\n",
    "    POST_DET   : int   = 300\n",
    "\n",
    "    # -------- FIXED params (Scheme A) --------\n",
    "    Q          : float = 0.997\n",
    "    ALPHA      : float = 1.03\n",
    "    REFRACT    : int   = 300\n",
    "    NMS_SEC    : int   = 420\n",
    "    ADAPT_SCOPE: str   = \"hour\"   # \"hour\" only\n",
    "\n",
    "    OUT_DIR    : str   = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "def log(msg): print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "# -------------------- NPZ + splits --------------------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files: return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\",\"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\",\"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos = (d[\"detect_label\"].astype(int)==1) if \"detect_label\" in d.files else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path); df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"Test positives in TEST scope; event_time = window_start + 20s.\"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    return pd.DataFrame({\"event_time\": evt_time}).sort_values(\"event_time\").reset_index(drop=True)\n",
    "\n",
    "# -------------------- CFT build + quantiles --------------------\n",
    "def build_cfts_for_day_multiband(fp, fs, bands, sta, lta):\n",
    "    st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "    tr0 = st[0]\n",
    "    if abs(tr0.stats.sampling_rate - fs) > 1e-6: tr0.resample(fs)\n",
    "    tr0.detrend(\"demean\")\n",
    "    hours = float((tr0.stats.endtime - tr0.stats.starttime)/3600.0)\n",
    "    cft_list = []\n",
    "    for (fmin,fmax) in bands:\n",
    "        tr = tr0.copy(); tr.filter(\"bandpass\", freqmin=fmin, freqmax=fmax)\n",
    "        x = tr.data.astype(np.float32, copy=False)\n",
    "        cft = classic_sta_lta(x, int(sta*fs), int(lta*fs))\n",
    "        cft_list.append(cft)\n",
    "    return dict(fs=fs, t0=tr0.stats.starttime, hours=hours, cft_list=cft_list)\n",
    "\n",
    "def precompute_quantiles(entry, q_values):\n",
    "    fs = entry[\"fs\"]; T = len(entry[\"cft_list\"][0]); H = int(3600*fs)\n",
    "    segs = [(s, min(s+H, T)) for s in range(0, T, H)]\n",
    "    entry[\"hour_segments\"] = segs; entry[\"q_day\"] = []; entry[\"q_hour\"] = []\n",
    "    for cft in entry[\"cft_list\"]:\n",
    "        entry[\"q_day\"].append({float(q): float(np.quantile(cft,q)) for q in q_values})\n",
    "        qh=[]\n",
    "        for (s,e) in segs:\n",
    "            seg = cft[s:e]\n",
    "            qh.append({float(q): float(np.quantile(seg,q)) if seg.size else 0.0 for q in q_values})\n",
    "        entry[\"q_hour\"].append(qh)\n",
    "\n",
    "# -------------------- Fusion + metrics --------------------\n",
    "def to_onoff_array(x):\n",
    "    arr = np.asarray(x, dtype=int)\n",
    "    if arr.size == 0: return np.empty((0,2), dtype=int)\n",
    "    if arr.ndim == 1:\n",
    "        if arr.shape[0]%2!=0: raise ValueError(\"Trigger on/off array length must be even.\")\n",
    "        arr = arr.reshape(-1,2)\n",
    "    elif arr.shape[1] != 2:\n",
    "        arr = arr.reshape(-1,2)\n",
    "    return arr\n",
    "\n",
    "def picks_or_then_nms(onoffs, fs, t0, nms_sec=30, refract=60):\n",
    "    picks=[]\n",
    "    for arr in onoffs:\n",
    "        if arr is None or len(arr)==0: continue\n",
    "        for a,b in arr: picks.append(t0 + a/fs)\n",
    "    if not picks: return []\n",
    "    picks.sort()\n",
    "    fused, last = [], None\n",
    "    for t in picks:\n",
    "        if (last is None) or ((t-last) > nms_sec):\n",
    "            fused.append(t); last = t\n",
    "    out, last = [], None\n",
    "    for t in fused:\n",
    "        if (last is None) or ((t-last) > refract):\n",
    "            out.append(pd.Timestamp(UTCDateTime(t).datetime)); last = t\n",
    "    return out\n",
    "\n",
    "def vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns):\n",
    "    if trigs_ns.size == 0: return np.zeros(len(ev_start_ns), dtype=bool)\n",
    "    i = np.searchsorted(trigs_ns, ev_start_ns, side=\"left\")\n",
    "    j = np.searchsorted(trigs_ns, ev_end_ns,   side=\"right\")\n",
    "    return (j - i) > 0\n",
    "\n",
    "# -------------------- Build caches + run --------------------\n",
    "log(\"Building test events...\")\n",
    "df_te = build_test_events(CFG)\n",
    "event_days = sorted(set(df_te[\"event_time\"].dt.date))\n",
    "\n",
    "log(\"Building CFTs & quantiles per day...\")\n",
    "cft_cache, total_hours, dates_ok = {}, 0.0, []\n",
    "for di, dt in enumerate(event_days, 1):\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=dt))\n",
    "    if not os.path.exists(fp): continue\n",
    "    entry = build_cfts_for_day_multiband(fp, CFG.FS, CFG.BANDS, CFG.STA, CFG.LTA)\n",
    "    precompute_quantiles(entry, [CFG.Q])\n",
    "    cft_cache[dt] = entry\n",
    "    total_hours += entry[\"hours\"]; dates_ok.append(dt)\n",
    "    if di % 2 == 0: log(f\"  {di} day(s) processed...\")\n",
    "\n",
    "if not dates_ok:\n",
    "    raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "\n",
    "ev = df_te[df_te[\"event_time\"].dt.date.isin(dates_ok)].copy().sort_values(\"event_time\").reset_index(drop=True)\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "ev_start_ns = ev[\"start\"].to_numpy(\"datetime64[ns]\")\n",
    "ev_end_ns   = ev[\"end\"].to_numpy(\"datetime64[ns]\")\n",
    "\n",
    "log(f\"[Info] events considered: {len(ev)} on {len(dates_ok)} days | hours={total_hours:.1f}\")\n",
    "log(f\"[Params] q={CFG.Q} alpha={CFG.ALPHA} rf={CFG.REFRACT}s nms={CFG.NMS_SEC}s scope={CFG.ADAPT_SCOPE}\")\n",
    "\n",
    "def run_fixed():\n",
    "    all_picks=[]\n",
    "    for di, dt in enumerate(dates_ok, 1):\n",
    "        entry = cft_cache[dt]; fs = entry[\"fs\"]; onoffs=[]\n",
    "        for b, cft in enumerate(entry[\"cft_list\"]):\n",
    "            # hour-adapt threshold using precomputed quantiles\n",
    "            arrs=[]\n",
    "            for (seg_idx,(s,e)) in enumerate(entry[\"hour_segments\"]):\n",
    "                base = entry[\"q_hour\"][b][seg_idx].get(float(CFG.Q), 0.0)\n",
    "                on_val = base * CFG.ALPHA\n",
    "                raw = trigger_onset(cft[s:e], on_val, CFG.OFF)\n",
    "                _onoff = to_onoff_array(raw)\n",
    "                if _onoff.size:\n",
    "                    _onoff[:,0]+=s; _onoff[:,1]+=s\n",
    "                arrs.append(_onoff)\n",
    "            onoff = np.vstack(arrs) if len(arrs) else np.empty((0,2), dtype=int)\n",
    "            onoffs.append(onoff)\n",
    "        picks = picks_or_then_nms(onoffs, fs, entry[\"t0\"], nms_sec=CFG.NMS_SEC, refract=CFG.REFRACT)\n",
    "        all_picks.extend(picks)\n",
    "        if di % 1 == 0: log(f\"  processed {di}/{len(dates_ok)} day(s)\")\n",
    "\n",
    "    trigs_ns = np.array(sorted(all_picks), dtype=\"datetime64[ns]\")\n",
    "    fph = trigs_ns.size / max(1e-6, total_hours)\n",
    "    hit = vectorized_any_hit(trigs_ns, ev_start_ns, ev_end_ns)\n",
    "    recall = float(hit.mean())\n",
    "\n",
    "    csv_out = os.path.join(CFG.OUT_DIR, f\"triggers_MB_ornms_q{CFG.Q}_a{CFG.ALPHA}_rf{CFG.REFRACT}_nms{CFG.NMS_SEC}_schour.csv\")\n",
    "    if trigs_ns.size == 0:\n",
    "        pd.DataFrame(columns=[\"trigger_time\",\"date\"]).to_csv(csv_out, index=False)\n",
    "    else:\n",
    "        trig_ts = pd.to_datetime(trigs_ns)\n",
    "        date_str = pd.Series(trig_ts).dt.strftime('%Y-%m-%d')\n",
    "        pd.DataFrame({\"trigger_time\": trig_ts, \"date\": date_str}).sort_values(\"trigger_time\").to_csv(csv_out, index=False)\n",
    "\n",
    "    log(f\"[Final] recall={recall:.3f} | FPH={fph:.3f} | triggers={int(trigs_ns.size)}\")\n",
    "    log(f\"[Saved] {csv_out} (rows={trigs_ns.size})\")\n",
    "\n",
    "run_fixed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84bfd3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:44:03] Building test events...\n",
      "[13:44:03] [Using fixed trigger CSV] runs/cascade_eval/triggers_MB_ornms_q0.997_a1.03_rf300_nms420_schour.csv\n",
      "[13:44:03] Triggers loaded: 2345 (first 3)\n",
      "              trigger_time       date\n",
      "2011-03-01 00:02:45.069500 2011-03-01\n",
      "2011-03-01 00:16:13.619500 2011-03-01\n",
      "2011-03-01 00:23:19.819500 2011-03-01\n",
      "[13:44:03] Preparing detection-eval windows...\n",
      "[13:44:03] [Info] events=1392 | hits=691 | hit_rate=0.496\n",
      "[13:44:03] Cutting windows for 691 hits (win_len=1800)...\n",
      "[13:44:03]   Cut 100/691\n",
      "[13:44:04]   Cut 200/691\n",
      "[13:44:04]   Cut 300/691\n",
      "[13:44:04]   Cut 400/691\n",
      "[13:44:04]   Cut 500/691\n",
      "[13:44:04]   Cut 600/691\n",
      "[13:44:04] [Cut] windows=691 | shape=(691, 1800) | kept=691\n",
      "[13:44:04] Computing TRAIN-only mean/std...\n",
      "[13:44:04] Using device: cpu\n",
      "[13:44:04] Loading model...\n",
      "[13:44:04] Model ready.\n",
      "[13:44:04] Running inference...\n",
      "[13:44:10]   Inferred 256/691\n",
      "[13:44:20]   Inferred 512/691\n",
      "[13:44:27]   Inferred 691/691\n",
      "[13:44:27] Inference done.\n",
      "\n",
      "== CNN (cascade on detected events) ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           S     0.9566    0.9110    0.9332       629\n",
      "           M     0.3452    0.5088    0.4113        57\n",
      "           L     0.2500    0.4000    0.3077         5\n",
      "\n",
      "    accuracy                         0.8741       691\n",
      "   macro avg     0.5173    0.6066    0.5508       691\n",
      "weighted avg     0.9011    0.8741    0.8856       691\n",
      "\n",
      "Confusion matrix:\n",
      " [[573  52   4]\n",
      " [ 26  29   2]\n",
      " [  0   3   2]]\n",
      "[End-to-End] S recall = 0.449 (total 1277)\n",
      "[End-to-End] M recall = 0.282 (total 103)\n",
      "[End-to-End] L recall = 0.167 (total 12)\n",
      "[13:44:27] Saved report to runs/cascade_eval/cascade_fixed_report.txt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cascade (FIXED): triggers -> re-center -> cut -> z-score -> strict CNN\n",
    "# Fixed to your chosen detection CSV:\n",
    "#   triggers_MB_ornms_q0.997_a1.03_rf300_nms420_schour.csv\n",
    "# Remap: keep defaults (M=0.33, L=0.20) to match your current version\n",
    "# ============================================================\n",
    "\n",
    "import os, time, json, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from obspy import read, UTCDateTime\n",
    "from obspy.signal.filter import envelope\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    # FIX to the CSV we just produced (edit if you rename)\n",
    "    TRIG_CSV   : str = \"runs/cascade_eval/triggers_MB_ornms_q0.997_a1.03_rf300_nms420_schour.csv\"\n",
    "\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "    FS         : int = 20\n",
    "    BAND_CUT   : tuple = (0.5, 8.0)\n",
    "\n",
    "    PRE        : int = 20\n",
    "    POST       : int = 70\n",
    "    PRE_DET    : int = 20\n",
    "    POST_DET   : int = 300\n",
    "\n",
    "    RC_PRE     : int = 20\n",
    "    RC_POST    : int = 40\n",
    "\n",
    "    BEST_PT    : str = \"runs/cnn_strict/best.pt\"\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "    # Keep the same remap as your previous version\n",
    "    REMAP_ENABLE  : bool  = True\n",
    "    REMAP_M_THRES : float = 0.33\n",
    "    REMAP_L_THRES : float = 0.20\n",
    "\n",
    "    FORCE_DEVICE  : str = \"auto\"  # \"cpu\" | \"cuda\" | \"auto\"\n",
    "    BATCH_SIZE    : int = 256\n",
    "    PRINT_EVERY   : int = 100\n",
    "\n",
    "CFG = Cfg()\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "def log(msg): print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "# ------ dataset helpers ------\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files: return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\",\"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\",\"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos = (d[\"detect_label\"].astype(int)==1) if \"detect_label\" in d.files else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path); df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_events(cfg: Cfg):\n",
    "    \"\"\"\n",
    "    FIXED: use pd.to_timedelta for seconds offset; do NOT wrap Timedelta with to_datetime.\n",
    "    \"\"\"\n",
    "    X, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = wst[mte].reset_index(drop=True) + pd.to_timedelta(cfg.PRE, unit=\"s\")\n",
    "    y_te = pd.Series(y[mte]).reset_index(drop=True)\n",
    "    order = np.argsort(evt_time.values)\n",
    "    df_te = pd.DataFrame({\"event_time\": evt_time.values[order]})\n",
    "    y_te_sorted = y_te.iloc[order].to_numpy()\n",
    "    return df_te.reset_index(drop=True), y_te_sorted\n",
    "\n",
    "# ------ waveform IO ------\n",
    "_trace_cache = {}\n",
    "def get_trace_for_day(cfg: Cfg, day_str: str):\n",
    "    if day_str not in _trace_cache:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=day_str))\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        if abs(tr.stats.sampling_rate - cfg.FS) > 1e-6: tr.resample(cfg.FS)\n",
    "        tr.detrend(\"demean\")\n",
    "        tr.filter(\"bandpass\", freqmin=cfg.BAND_CUT[0], freqmax=cfg.BAND_CUT[1])\n",
    "        _trace_cache[day_str] = tr\n",
    "    return _trace_cache[day_str]\n",
    "\n",
    "def recenter_trigger(tr, t_ts, fs, pre, post):\n",
    "    \"\"\"Envelope peak near trigger, with fine search ¬±5s.\"\"\"\n",
    "    t0 = UTCDateTime(t_ts.to_pydatetime()) - pre\n",
    "    t1 = UTCDateTime(t_ts.to_pydatetime()) + post\n",
    "    x = tr.slice(t0, t1).data.astype(np.float32, copy=False)\n",
    "    need = int((pre + post) * fs)\n",
    "    if len(x) < need: return t_ts\n",
    "    env = envelope(x)\n",
    "    i = int(np.argmax(env)); fine = int(5*fs); a = max(0, i-fine); b = min(len(env), i+fine+1)\n",
    "    j = a + int(np.argmax(env[a:b])); t_pk = t0 + j / fs\n",
    "    return pd.Timestamp(UTCDateTime(t_pk).datetime)\n",
    "\n",
    "# ------ strict CNN ------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=9, p=None, pool=2):\n",
    "        super().__init__()\n",
    "        if p is None: p = k//2\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=p)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x); x = self.bn(x); x = F.gelu(x); x = self.pool(x); return x\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__(); self.W = nn.Linear(d, d); self.v = nn.Linear(d, 1, bias=False)\n",
    "    def forward(self, H):\n",
    "        U = torch.tanh(self.W(H)); a = self.v(U).squeeze(-1); a = torch.softmax(a, dim=1)\n",
    "        Z = torch.bmm(a.unsqueeze(1), H).squeeze(1); return Z, a\n",
    "\n",
    "class CNNBiLSTMAttn(nn.Module):\n",
    "    def __init__(self, in_ch=1, hidden=96, layers=2, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(ConvBlock(in_ch,32), ConvBlock(32,64), ConvBlock(64,128))\n",
    "        self.lstm = nn.LSTM(128, hidden, num_layers=layers, batch_first=True, bidirectional=True, dropout=0.1)\n",
    "        self.attn = AdditiveAttention(2*hidden)\n",
    "        self.head = nn.Sequential(nn.Linear(2*hidden,128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128,n_classes))\n",
    "    def forward(self, x):\n",
    "        z = self.cnn(x); z = z.transpose(1,2); H,_ = self.lstm(z); Z,_ = self.attn(H); return self.head(Z)\n",
    "\n",
    "def softmax_remap(logits, enable=False, m_th=0.33, l_th=0.20):\n",
    "    preds = logits.argmax(1).cpu().numpy()\n",
    "    if not enable: return preds\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    pM, pL = probs[:,1], probs[:,2]\n",
    "    for n in range(len(preds)):\n",
    "        if pL[n] >= l_th: preds[n] = 2\n",
    "        elif (pM[n] >= m_th) and (preds[n] == 0): preds[n] = 1\n",
    "    return preds\n",
    "\n",
    "# ------ misc helpers ------\n",
    "def log_head(df, n=3): return df.head(n).to_string(index=False)\n",
    "\n",
    "def choose_device(pref: str):\n",
    "    if pref.lower()==\"cpu\": return torch.device(\"cpu\")\n",
    "    if pref.lower()==\"cuda\" and torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "log(\"Building test events...\")\n",
    "df_te, y_te_sorted = build_test_events(CFG)\n",
    "\n",
    "if not os.path.exists(CFG.TRIG_CSV):\n",
    "    raise FileNotFoundError(f\"Trigger CSV not found: {CFG.TRIG_CSV}\")\n",
    "log(f\"[Using fixed trigger CSV] {CFG.TRIG_CSV}\")\n",
    "trig_df = pd.read_csv(CFG.TRIG_CSV)\n",
    "if \"trigger_time\" not in trig_df.columns:\n",
    "    raise ValueError(\"'trigger_time' column missing in CSV.\")\n",
    "trig_df[\"trigger_time\"] = pd.to_datetime(trig_df[\"trigger_time\"], errors=\"coerce\", utc=False)\n",
    "trig_df = trig_df.dropna(subset=[\"trigger_time\"]).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "log(f\"Triggers loaded: {len(trig_df)} (first 3)\\n{log_head(trig_df[['trigger_time','date']])}\")\n",
    "\n",
    "log(\"Preparing detection-eval windows...\")\n",
    "ev = df_te.copy()\n",
    "ev[\"start\"] = ev[\"event_time\"] - pd.to_timedelta(CFG.PRE_DET, unit=\"s\")\n",
    "ev[\"end\"]   = ev[\"event_time\"] + pd.to_timedelta(CFG.POST_DET, unit=\"s\")\n",
    "\n",
    "tr = trig_df[\"trigger_time\"].values.astype(\"datetime64[ns]\")\n",
    "ev_start = ev[\"start\"].values.astype(\"datetime64[ns]\")\n",
    "ev_end   = ev[\"end\"].values.astype(\"datetime64[ns]\")\n",
    "\n",
    "i = np.searchsorted(tr, ev_start, side=\"left\")\n",
    "j = np.searchsorted(tr, ev_end,   side=\"right\")\n",
    "hit = (j - i) > 0\n",
    "first_idx = np.where(hit, i, -1)\n",
    "log(f\"[Info] events={len(ev)} | hits={int(hit.sum())} | hit_rate={hit.mean():.3f}\")\n",
    "\n",
    "# Cut windows\n",
    "win_len = CFG.FS * (CFG.PRE + CFG.POST)\n",
    "X_cut, y_hit, kept = [], [], []\n",
    "hit_indices = np.where(hit)[0]\n",
    "log(f\"Cutting windows for {len(hit_indices)} hits (win_len={win_len})...\")\n",
    "_cache = {}\n",
    "def get_tr_day(day_str):\n",
    "    if day_str not in _cache: _cache[day_str] = get_trace_for_day(CFG, day_str)\n",
    "    return _cache[day_str]\n",
    "\n",
    "for k, idx in enumerate(hit_indices, 1):\n",
    "    t_first = pd.Timestamp(tr[first_idx[idx]])\n",
    "    day_str = str(t_first.date())\n",
    "    fp = os.path.join(CFG.MSEED_DIR, CFG.MSEED_FMT.format(date=day_str))\n",
    "    if not os.path.exists(fp): continue\n",
    "    tr_day = get_tr_day(day_str)\n",
    "    t_center = recenter_trigger(tr_day, t_first, fs=CFG.FS, pre=CFG.RC_PRE, post=CFG.RC_POST)\n",
    "    t0 = UTCDateTime(t_center.to_pydatetime()) - CFG.PRE\n",
    "    t1 = UTCDateTime(t_center.to_pydatetime()) + CFG.POST\n",
    "    x = tr_day.slice(t0, t1).data\n",
    "    if len(x) >= win_len:\n",
    "        X_cut.append(x[:win_len].astype(np.float32))\n",
    "        y_hit.append(y_te_sorted[idx])\n",
    "        kept.append(idx)\n",
    "    if (k % CFG.PRINT_EVERY) == 0:\n",
    "        log(f\"  Cut {k}/{len(hit_indices)}\")\n",
    "\n",
    "if len(X_cut)==0: raise SystemExit(\"No windows cut; check files or bounds.\")\n",
    "X_cut = np.stack(X_cut, axis=0); y_hit = np.array(y_hit, dtype=int)\n",
    "log(f\"[Cut] windows={len(X_cut)} | shape={X_cut.shape} | kept={len(kept)}\")\n",
    "\n",
    "# z-score using TRAIN-only stats\n",
    "def train_mean_std(cfg: Cfg):\n",
    "    X, y, sid, _ = load_npz_pos(cfg.NPZ_PATH)\n",
    "    tr_scope, _ = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mtr = np.isin(sid, list(tr_scope))\n",
    "    flat = X[mtr].reshape(np.sum(mtr), -1)\n",
    "    mean, std = float(flat.mean()), float(flat.std() + 1e-8)\n",
    "    with open(os.path.join(cfg.OUT_DIR, \"mean_std_from_train.json\"), \"w\") as f:\n",
    "        json.dump({\"mean\":mean,\"std\":std}, f, indent=2)\n",
    "    return mean, std\n",
    "\n",
    "log(\"Computing TRAIN-only mean/std...\")\n",
    "mean, std = train_mean_std(CFG)\n",
    "Xn = (X_cut - mean) / (std if std>0 else 1.0)\n",
    "Xn = Xn[:, None, :]\n",
    "\n",
    "# device + model\n",
    "def choose_device(pref: str):\n",
    "    if pref.lower()==\"cpu\": return torch.device(\"cpu\")\n",
    "    if pref.lower()==\"cuda\" and torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available(): return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = choose_device(CFG.FORCE_DEVICE); log(f\"Using device: {device}\")\n",
    "if not os.path.exists(CFG.BEST_PT):\n",
    "    raise FileNotFoundError(f\"CNN checkpoint not found: {CFG.BEST_PT}\")\n",
    "log(\"Loading model...\")\n",
    "class Model(CNNBiLSTMAttn): pass\n",
    "model = Model(in_ch=1, n_classes=3).to(device)\n",
    "state = torch.load(CFG.BEST_PT, map_location=device)\n",
    "model.load_state_dict(state, strict=True); model.eval(); log(\"Model ready.\")\n",
    "\n",
    "# inference\n",
    "log(\"Running inference...\")\n",
    "y_pred=[]\n",
    "with torch.no_grad():\n",
    "    for i0 in range(0, len(Xn), CFG.BATCH_SIZE):\n",
    "        xb = torch.from_numpy(Xn[i0:i0+CFG.BATCH_SIZE]).to(device)\n",
    "        logits = model(xb)\n",
    "        preds = softmax_remap(logits, enable=CFG.REMAP_ENABLE,\n",
    "                              m_th=CFG.REMAP_M_THRES, l_th=CFG.REMAP_L_THRES)\n",
    "        y_pred.extend(preds)\n",
    "        if ((i0 // CFG.BATCH_SIZE + 1) % max(1, CFG.PRINT_EVERY // max(1, CFG.BATCH_SIZE))) == 0:\n",
    "            log(f\"  Inferred {min(i0+CFG.BATCH_SIZE, len(Xn))}/{len(Xn)}\")\n",
    "y_pred = np.array(y_pred, dtype=int); log(\"Inference done.\")\n",
    "\n",
    "# reports\n",
    "rep = classification_report(y_hit, y_pred, labels=[0,1,2], target_names=[\"S\",\"M\",\"L\"], digits=4, zero_division=0)\n",
    "cm = confusion_matrix(y_hit, y_pred, labels=[0,1,2])\n",
    "macro_f1 = f1_score(y_hit, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n== CNN (cascade on detected events) ==\")\n",
    "print(rep); print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# end-to-end per-class recall\n",
    "lab = y_te_sorted; correct_mask = np.zeros_like(lab, dtype=bool)\n",
    "kept_idx = np.array(kept, dtype=int)\n",
    "for k, idx in enumerate(kept_idx):\n",
    "    if y_hit[k] == y_pred[k]: correct_mask[idx] = True\n",
    "\n",
    "for c, name in enumerate([\"S\",\"M\",\"L\"]):\n",
    "    total_c = (lab==c).sum()\n",
    "    e2e_c = correct_mask[lab==c].sum() / max(1,total_c)\n",
    "    print(f\"[End-to-End] {name} recall = {e2e_c:.3f} (total {total_c})\")\n",
    "\n",
    "# save artifacts\n",
    "out_report = os.path.join(CFG.OUT_DIR, \"cascade_fixed_report.txt\")\n",
    "with open(out_report, \"w\") as f:\n",
    "    f.write(rep + \"\\n\"); f.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "np.savez(os.path.join(CFG.OUT_DIR, \"cascade_fixed_preds.npz\"),\n",
    "         y_true=y_hit, y_pred=y_pred, kept_event_indices=np.array(kept, dtype=int))\n",
    "log(f\"Saved report to {out_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "840d9860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= FPH SUMMARY =================\n",
      "Triggers CSV: runs/cascade_eval/triggers_MB_ornms_q0.997_a1.03_rf300_nms420_schour.csv\n",
      "Days considered: 17  |  Total hours: 408.000\n",
      "Total triggers (used): 2345  |  Overall FPH: 5.748 per hour\n",
      "===============================================\n",
      "\n",
      "Top-10 days by FPH:\n",
      "      date  triggers     hours      fph\n",
      "2011-03-03       166 23.999986 6.916671\n",
      "2011-03-04       165 23.999986 6.875004\n",
      "2011-03-07       165 23.999986 6.875004\n",
      "2011-03-05       162 23.999986 6.750004\n",
      "2011-03-01       159 23.999986 6.625004\n",
      "2011-03-10       143 23.999986 5.958337\n",
      "2011-03-09       131 23.999986 5.458336\n",
      "2011-03-11       131 23.999986 5.458336\n",
      "2011-03-13       128 23.999986 5.333336\n",
      "2011-03-15       128 23.999986 5.333336\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FPH Checker for Multiband STA/LTA Triggers (fixed pipeline)\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, json\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    NPZ_PATH   : str = \"data/wave_mag_dataset.npz\"\n",
    "    CSV_PATH   : str = \"data/features_from_npz_mag.csv\"\n",
    "    SPLIT_PATH : str = \"runs/frozen_splits.json\"\n",
    "\n",
    "    MSEED_DIR  : str = \"waveforms\"\n",
    "    MSEED_FMT  : str = \"MAJO_{date}.mseed\"\n",
    "\n",
    "    TRIG_CSV   : str = \"runs/cascade_eval/triggers_MB_ornms_q0.997_a1.03_rf300_nms420_schour.csv\"\n",
    "    OUT_DIR    : str = \"runs/cascade_eval\"\n",
    "\n",
    "CFG = Cfg()\n",
    "\n",
    "def _first_key(d, candidates):\n",
    "    for k in candidates:\n",
    "        if k in d.files: return k\n",
    "    raise KeyError(f\"None of {candidates} found in NPZ.\")\n",
    "\n",
    "def load_npz_pos(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True, mmap_mode=\"r\")\n",
    "    X = d[\"waveforms\"]\n",
    "    sid_key = _first_key(d, [\"sample_id\", \"sample_ids\"])\n",
    "    y_key   = _first_key(d, [\"mag_class\", \"labels\"])\n",
    "    sid = np.array([str(s) for s in d[sid_key]])\n",
    "    y   = d[y_key].astype(int)\n",
    "    wst = pd.Series(pd.to_datetime(d[\"window_start\"].astype(object), utc=True).tz_localize(None))\n",
    "    pos_key = \"detect_label\" if \"detect_label\" in d.files else None\n",
    "    pos = (d[pos_key].astype(int) == 1) if pos_key else np.ones(len(y), dtype=bool)\n",
    "    return X[pos], y[pos], sid[pos], wst[pos].reset_index(drop=True)\n",
    "\n",
    "def get_ids_split(csv_path, split_path):\n",
    "    with open(split_path, \"r\") as f:\n",
    "        splits = json.load(f)\n",
    "    tr_ids = set(map(str, splits[\"magcls\"][\"train_ids\"]))\n",
    "    te_ids = set(map(str, splits[\"magcls\"][\"test_ids\"]))\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"sample_id\"] = df[\"sample_id\"].astype(str)\n",
    "    tr_scope = set(df[df[\"sample_id\"].isin(tr_ids)][\"sample_id\"])\n",
    "    te_scope = set(df[df[\"sample_id\"].isin(te_ids)][\"sample_id\"])\n",
    "    return tr_scope, te_scope\n",
    "\n",
    "def build_test_event_days(cfg: Cfg):\n",
    "    _, y, sid, wst = load_npz_pos(cfg.NPZ_PATH)\n",
    "    _, te_scope = get_ids_split(cfg.CSV_PATH, cfg.SPLIT_PATH)\n",
    "    mte = np.isin(sid, list(te_scope))\n",
    "    evt_time = (wst[mte].reset_index(drop=True) + pd.to_timedelta(20, \"s\"))\n",
    "    return sorted(set(pd.to_datetime(evt_time).dt.date))\n",
    "\n",
    "def compute_hours_for_days(cfg: Cfg, days):\n",
    "    per_day_hours, days_ok, total_hours = {}, [], 0.0\n",
    "    for dt in days:\n",
    "        fp = os.path.join(cfg.MSEED_DIR, cfg.MSEED_FMT.format(date=dt))\n",
    "        if not os.path.exists(fp): continue\n",
    "        st = read(fp).merge(method=1, fill_value=\"interpolate\")\n",
    "        tr = st[0]\n",
    "        hours = float((tr.stats.endtime - tr.stats.starttime) / 3600.0)\n",
    "        per_day_hours[dt] = hours\n",
    "        total_hours += hours\n",
    "        days_ok.append(dt)\n",
    "    return per_day_hours, total_hours, days_ok\n",
    "\n",
    "def check_fph(cfg: Cfg):\n",
    "    event_days = build_test_event_days(cfg)\n",
    "    per_day_hours, total_hours, days_ok = compute_hours_for_days(cfg, event_days)\n",
    "    if not days_ok:\n",
    "        raise SystemExit(\"No overlapping days between TEST events and waveform files.\")\n",
    "    if not os.path.exists(cfg.TRIG_CSV):\n",
    "        raise FileNotFoundError(f\"Trigger CSV not found: {cfg.TRIG_CSV}\")\n",
    "\n",
    "    df = pd.read_csv(cfg.TRIG_CSV)\n",
    "    if \"trigger_time\" not in df.columns:\n",
    "        raise ValueError(f\"'trigger_time' column not found in {cfg.TRIG_CSV}\")\n",
    "    df[\"trigger_time\"] = pd.to_datetime(df[\"trigger_time\"], errors=\"coerce\", utc=False)\n",
    "    df = df.dropna(subset=[\"trigger_time\"]).sort_values(\"trigger_time\").reset_index(drop=True)\n",
    "    df[\"date\"] = df[\"trigger_time\"].dt.date\n",
    "\n",
    "    n_total_trigs = len(df)\n",
    "    df = df[df[\"date\"].isin(days_ok)].copy()\n",
    "    n_used_trigs = len(df)\n",
    "    if total_hours <= 0: raise ValueError(\"Total hours computed as zero; check your mseed files.\")\n",
    "    fph = n_used_trigs / total_hours\n",
    "\n",
    "    per_day = df.groupby(\"date\")[\"trigger_time\"].count().rename(\"triggers\").reset_index()\n",
    "    per_day[\"hours\"] = per_day[\"date\"].map(per_day_hours).astype(float)\n",
    "    per_day[\"fph\"]   = per_day[\"triggers\"] / per_day[\"hours\"].replace(0, np.nan)\n",
    "\n",
    "    print(\"\\n================= FPH SUMMARY =================\")\n",
    "    print(f\"Triggers CSV: {cfg.TRIG_CSV}\")\n",
    "    print(f\"Days considered: {len(days_ok)}  |  Total hours: {total_hours:.3f}\")\n",
    "    print(f\"Total triggers (used): {n_used_trigs}  |  Overall FPH: {fph:.3f} per hour\")\n",
    "    print(\"===============================================\")\n",
    "\n",
    "    if len(per_day) > 0:\n",
    "        print(\"\\nTop-10 days by FPH:\")\n",
    "        print(per_day.sort_values(\"fph\", ascending=False).head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No triggers within the considered day set; FPH = 0.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_fph(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749955b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
